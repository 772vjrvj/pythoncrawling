import json
import math
import os
import ssl
import threading
import time
from urllib.parse import urlparse, parse_qs, unquote

import pandas as pd
import pyautogui  # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
import requests
from PyQt5.QtCore import QThread, pyqtSignal
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

from src.utils.time_utils import get_current_yyyymmddhhmmss

ssl._create_default_https_context = ssl._create_unverified_context

image_main_directory = 'albamon_images'
company_name = 'ì•Œë°”ëª¬'
site_name = 'albamon'

excel_filename = ''
baseUrl = "https://www.albamon.com/jobs/area"
baseLoginUrl = "https://www.albamon.com/user-account/login"
baseAllUrl = "https://www.albamon.com/jobs/total"

# API
def parse_albamon_url():
    # page ì œì™¸í•˜ê³  ê¸°ë³¸ payload ì„¤ì •
    payload = {
        "pagination": {
            "page": 1,  # pageëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ìŒ
            "size": 50
        },
        "recruitListType": "NORMAL_ALL",
        "sortTabCondition": {
            "searchPeriodType": "ALL",
            "sortType": "DEFAULT"
        },

        "condition": {
            "age": 0,
            "areas": [],
            "educationType": "ALL",
            "employmentTypes": [],
            "endWorkTime": "",
            "excludeBar": False,
            "excludeKeywordList": [],
            "excludeKeywords": [],
            "excludeNegoAge": False,
            "excludeNegoGender": False,
            "excludeNegoWorkTime": False,
            "excludeNegoWorkWeek": False,
            "genderType": "NONE",
            "includeKeyword": "",
            "moreThanEducation": False,
            "parts": [],
            "similarDongJoin": False,
            "startWorkTime": "",
            "workDayTypes": [],
            "workPeriodTypes": [],
            "workTimeTypes": [],
            "workWeekTypes": [],
        }
    }
    return payload


class ApiAlbamonSetLoadWorker(QThread):
    log_signal = pyqtSignal(str)         # ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_signal = pyqtSignal(float, float)  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_end_signal = pyqtSignal()   # ì¢…ë£Œ ì‹œê·¸ë„
    msg_signal = pyqtSignal(str, str, object)

    # ì´ˆê¸°í™”
    def __init__(self, checked_list):
        super().__init__()
        self.baseUrl = baseUrl
        self.baseLoginUrl = baseLoginUrl
        self.baseAllUrl = baseAllUrl
        self.sess = requests.Session()
        self.checked_list = checked_list

        self.excludeKeywords = ""
        self.includeKeyword = ""

        self.running = True  # ì‹¤í–‰ ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€
        self.driver = None

        self.com_list = []
        self.main_model = None
        self.product_info_list = []

        self.total_cnt = 0
        self.total_pages = 0
        self.current_page = 0
        self.current_cnt = 0
        self.before_pro_value = 0


    # í”„ë¡œê·¸ë¨ ì‹¤í–‰
    def run(self):
        global image_main_directory, company_name, site_name, excel_filename, baseUrl

        self.log_signal.emit("í¬ë¡¤ë§ ì‹œì‘")
        result_list = []
        self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì„ ì‹œë„ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        self.login()
        self.wait_for_user_confirmation()

        self.wait_for_select_confirmation()

        self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì— ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.")
        self.log_signal.emit(f"ì „ì²´ íšŒì‚¬ìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        self.total_cnt_cal()
        self.log_signal.emit(f"ì „ì²´ íšŒì‚¬ìˆ˜ {self.total_cnt} ê°œ")
        self.log_signal.emit(f"ì „ì²´ í˜ì´ì§€ìˆ˜ {self.total_pages} ê°œ")

        csv_filename = os.path.join(os.getcwd(), f"ì•Œë°”ëª¬_{get_current_yyyymmddhhmmss()}.csv")
        # columns = ["NO", "ì‚¬ì—…ì²´ëª…", "ì±„ìš©ë‹´ë‹¹ìëª…", "íœ´ëŒ€í° ë²ˆí˜¸", "ê·¼ë¬´ì§€ ì£¼ì†Œ", "ì§€ì—­1", "ì§€ì—­2", "ì§€ì—­3", "ê¸‰ì—¬ ì •ë³´", "ê·¼ë¬´ ê¸°ê°„", "ë“±ë¡ì¼",
        #            "ê·¼ë¬´ ìš”ì¼", "ê·¼ë¬´ ì‹œê°„", "ê³ ìš© í˜•íƒœ", "ë³µë¦¬í›„ìƒ ì •ë³´", "ì—…ì§ì¢…", "ì—…ì¢…", "ëŒ€í‘œìëª…", "ê¸°ì—…ì£¼ì†Œ"]

        columns = ["NO", "ì‚¬ì—…ì²´ëª…", "ì±„ìš©ë‹´ë‹¹ìëª…", "íœ´ëŒ€í° ë²ˆí˜¸","í¬í•¨ í‚¤ì›Œë“œ", "ì œì™¸ í‚¤ì›Œë“œ"]

        df = pd.DataFrame(columns=columns)
        df.to_csv(csv_filename, index=False, encoding="utf-8-sig")

        for page in range(1, self.total_pages + 1):
            self.log_signal.emit(f"í˜„ì¬ í˜ì´ì§€ {page}")
            time.sleep(1)
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                self.log_signal.emit("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            collection, pagination = self.main_request(page)

            for index, data in enumerate(collection):

                if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                    self.log_signal.emit("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break

                time.sleep(1)

                # í°ë²ˆí˜¸ê°€ ì—†ëŠ”ê²½ìš°
                # if data.get('managerPhoneNumber', '') == '':
                #     self.log_signal.emit(f"ë²ˆí˜¸ ì—†ìŒ Skip")
                #     self.current_cnt = self.current_cnt + 1
                #     pro_value = (self.current_cnt / self.total_cnt) * 1000000
                #     self.progress_signal.emit(self.before_pro_value, pro_value)
                #     self.before_pro_value = pro_value
                #     self.log_signal.emit(f"í˜„ì¬ í˜ì´ì§€ {self.current_cnt}/{self.total_cnt}")
                #     continue

                scraped_date = data.get("scrapedDate", "")

                # ì„œìš¸ ê°•ì„œêµ¬ ë§ˆê³¡ë™
                workplace_area = data.get("workplaceArea", "").strip()
                area_parts = workplace_area.split() if workplace_area else []

                obj = {
                    "NO": data.get('recruitNo', ''),
                    # "ì‚¬ì—…ì²´ëª…": data.get('companyName', ''),
                    "ì±„ìš©ë‹´ë‹¹ìëª…": '',
                    "íœ´ëŒ€í° ë²ˆí˜¸": data.get('managerPhoneNumber', ''),
                    # "ê·¼ë¬´ì§€ ì£¼ì†Œ": data.get('workplaceAddress', ''),
                    # "ì§€ì—­": data.get('workplaceArea', ''),    # ì„œìš¸ ì¤‘êµ¬
                    # "ê¸‰ì—¬ ì •ë³´": data.get('pay', ''),
                    # "ë“±ë¡ì¼": scraped_date,
                    # "ê·¼ë¬´ ê¸°ê°„": data.get('workingPeriod', ''),
                    # "ê·¼ë¬´ ìš”ì¼": data.get('workingWeek', ''),
                    # "ê·¼ë¬´ ì‹œê°„": data.get('workingTime', ''),
                    # "ê³ ìš© í˜•íƒœ": data.get('recruitType', {}).get('description', ''),
                    # "ë³µë¦¬í›„ìƒ ì •ë³´": data.get('filterTotal', ''),
                    # "ì—…ì§ì¢…": data.get('parts', ''),
                    # "ì§€ì—­1": area_parts[0] if len(area_parts) > 0 else "",
                    # "ì§€ì—­2": area_parts[1] if len(area_parts) > 1 else "",
                    # "ì§€ì—­3": area_parts[2] if len(area_parts) > 2 else "",
                    # "ì‚¬ì—…ì²´ëª…": "",
                    # "ì—…ì¢…": "",
                    # "ëŒ€í‘œìëª…": "",
                    # "ê¸°ì—…ì£¼ì†Œ": "",
                    "í¬í•¨ í‚¤ì›Œë“œ": self.includeKeyword,
                    "ì œì™¸ í‚¤ì›Œë“œ": self.excludeKeywords,
                }

                detail_data = self.get_api_request(data.get('recruitNo', ''))

                if detail_data:
                    obj['ì±„ìš©ë‹´ë‹¹ìëª…'] = detail_data.get('viewData', {}).get('recruiter', '')
                    # obj['ë“±ë¡ì¼'] = detail_data.get('viewData', {}).get('pcSortDate', '')
                    # obj['ì‚¬ì—…ì²´ëª…'] = detail_data.get('viewData',{}).get('recruitCompanyName','')
                    obj['ì‚¬ì—…ì²´ëª…'] = detail_data.get('companyData', {}).get('companyName', '')
                    # obj['ì—…ì¢…'] = detail_data.get('companyData', {}).get('jobTypeName', '')
                    # obj['ëŒ€í‘œìëª…'] = detail_data.get('companyData', {}).get('representativeName', '')
                    # obj['ê¸°ì—…ì£¼ì†Œ'] = detail_data.get('companyData', {}).get('fullAddress', '')

                self.log_signal.emit(f"í˜„ì¬ ì±„ìš© ì •ë³´ : {obj}")

                result_list.append(obj)

                if (index + 1) % 5 == 0:
                    df = pd.DataFrame(result_list, columns=columns)
                    df.to_csv(csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")
                    result_list.clear()

                self.current_cnt = self.current_cnt + 1

                pro_value = (self.current_cnt / self.total_cnt) * 1000000
                self.progress_signal.emit(self.before_pro_value, pro_value)
                self.before_pro_value = pro_value
                self.log_signal.emit(f"í˜„ì¬ í˜ì´ì§€ {self.current_cnt}/{self.total_cnt}")

            if result_list:
                df = pd.DataFrame(result_list, columns=columns)
                df.to_csv(csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")

        self.progress_signal.emit(self.before_pro_value, 1000000)
        time.sleep(3)
        self.log_signal.emit("=============== í¬ë¡¤ë§ ì¢…ë£Œ")
        self.progress_end_signal.emit()

    # í”„ë¡œê·¸ë¨ ì¤‘ë‹¨
    def stop(self):
        """ìŠ¤ë ˆë“œ ì¤‘ì§€ë¥¼ ìš”ì²­í•˜ëŠ” ë©”ì„œë“œ"""

        self.running = False

    def wait_for_user_confirmation(self):
        """ì‚¬ìš©ìê°€ í™•ì¸(alert) ì°½ì—ì„œ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°"""
        event = threading.Event()  # OK ë²„íŠ¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°í•  ì´ë²¤íŠ¸ ê°ì²´

        # ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì°½ ìš”ì²­
        self.msg_signal.emit("ë¡œê·¸ì¸ í›„  í›„ OKë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”", "info", event)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.log_signal.emit("ğŸ“¢ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...")
        event.wait()  # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥´ë©´ í•´ì œë¨

        # ì¿ í‚¤ ì„¤ì •
        cookies = self.driver.get_cookies()
        for cookie in cookies:
            self.sess.cookies.set(cookie['name'], cookie['value'])

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆŒë €ì„ ê²½ìš° ì‹¤í–‰
        self.log_signal.emit("âœ… ì‚¬ìš©ìê°€ í™•ì¸ ë²„íŠ¼ì„ ëˆŒë €ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‘ì—… ì§„í–‰ ì¤‘...")

        self.driver.get(self.baseAllUrl)

        time.sleep(2)  # ì˜ˆì œìš©

        # "ìƒì„¸ì¡°ê±´" í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ spanì„ í¬í•¨í•˜ëŠ” ì™¸ë¶€ spanì„ ì°¾ê³  í´ë¦­
        WebDriverWait(self.driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, '//span[.//span[text()="ìƒì„¸ì¡°ê±´"]]'))
        ).click()

        self.log_signal.emit("ğŸš€ ì‘ì—… ì™„ë£Œ!")


    def wait_for_select_confirmation(self):
        """ì‚¬ìš©ìê°€ í™•ì¸(alert) ì°½ì—ì„œ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°"""
        event = threading.Event()  # OK ë²„íŠ¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°í•  ì´ë²¤íŠ¸ ê°ì²´

        # ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì°½ ìš”ì²­
        self.msg_signal.emit("í‚¤ì›Œë“œ(í¬í•¨/ì œì™¸) ì¶”ê°€ í›„ OKë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”(ì•„ë˜ ëª©ë¡ì´ ë‚˜ì˜¤ëŠ”ê±¸ í™•ì¸í•˜ì„¸ìš”)", "info", event)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.log_signal.emit("ğŸ“¢ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...")
        event.wait()  # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥´ë©´ í•´ì œë¨

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆŒë €ì„ ê²½ìš° ì‹¤í–‰
        self.log_signal.emit("âœ… í™•ì¸ ë²„íŠ¼ì„ ëˆŒë €ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‘ì—… ì§„í–‰ ì¤‘...")


        # í˜„ì¬ URL ê°€ì ¸ì˜¤ê¸°
        current_url = self.driver.current_url
        parsed_url = urlparse(current_url)
        query_params = parse_qs(parsed_url.query)

        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì €ì¥
        exclude = query_params.get("excludeKeywords", [""])[0]
        include = query_params.get("includeKeyword", [""])[0]

        self.excludeKeywords = unquote(exclude)
        self.includeKeyword = unquote(include)

        self.log_signal.emit(f"ğŸ” ì œì™¸ í‚¤ì›Œë“œ: {self.excludeKeywords}")
        self.log_signal.emit(f"ğŸ” í¬í•¨ í‚¤ì›Œë“œ: {self.includeKeyword}")

        time.sleep(2)  # ì˜ˆì œìš©
        self.log_signal.emit("ğŸš€ ì‘ì—… ì™„ë£Œ!")


    def login(self):
        webdriver_options = webdriver.ChromeOptions()

        ###### ìë™ ì œì–´ ê°ì§€ ë°©ì§€ #####
        webdriver_options.add_argument('--disable-blink-features=AutomationControlled')

        ##### í™”ë©´ ìµœëŒ€ #####
        # webdriver_options.add_argument("--start-maximized")  # ìµœëŒ€í™” ëŒ€ì‹  í¬ê¸° ì¡°ì ˆ ì‚¬ìš©

        ##### headless ëª¨ë“œ ë¹„í™œì„±í™” (ë¸Œë¼ìš°ì € UI ë³´ì´ê²Œ) #####
        # webdriver_options.add_argument("--headless")

        ##### ìë™ ê²½ê³  ì œê±° #####
        webdriver_options.add_experimental_option('useAutomationExtension', False)

        ##### ë¡œê¹… ë¹„í™œì„±í™” #####
        webdriver_options.add_experimental_option('excludeSwitches', ['enable-logging'])

        ##### ìë™í™” ë„êµ¬ ì‚¬ìš© ê°ì§€ ì œê±° #####
        webdriver_options.add_experimental_option("excludeSwitches", ["enable-automation"])

        # ë“œë¼ì´ë²„ ì‹¤í–‰
        self.driver = webdriver.Chrome(options=webdriver_options)
        self.driver.set_page_load_timeout(120)

        # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸°
        screen_width, screen_height = pyautogui.size()

        # ì°½ í¬ê¸°ë¥¼ ë„ˆë¹„ ì ˆë°˜, ë†’ì´ ì „ì²´ë¡œ ì„¤ì •
        self.driver.set_window_size(screen_width // 2, screen_height)

        # ì°½ ìœ„ì¹˜ë¥¼ ì™¼ìª½ ìƒë‹¨ì— ë°°ì¹˜
        self.driver.set_window_position(0, 0)

        # ë¡œê·¸ì¸ ì—´ê¸°
        self.driver.get(self.baseLoginUrl)


    def main_request(self, page=1):
        """í˜„ì¬ ë¸Œë¼ìš°ì € URLì„ ê¸°ë°˜ìœ¼ë¡œ API ìš”ì²­"""
        url = "https://bff-general.albamon.com/recruit/search"

        headers = {
            "authority": "bff-general.albamon.com",
            "method": "POST",
            "path": "/recruit/search",
            "scheme": "https",
            "accept": "application/json",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "albamon-domain-type": "pc",
            "content-type": "application/json",
            "origin": "https://www.albamon.com",
            "priority": "u=1, i",
            "sec-ch-ua": '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133")',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-site",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
        }

        # self.excludeKeywords: ì‰¼í‘œ êµ¬ë¶„ëœ ë¬¸ìì—´ì´ë¼ê³  ê°€ì •
        exclude_keywords_list = [kw.strip() for kw in self.excludeKeywords.split(',')] if self.excludeKeywords else []
        include_keyword = self.includeKeyword if self.includeKeyword else ""


        # í˜„ì¬ ë¸Œë¼ìš°ì € URLì„ ê¸°ë°˜ìœ¼ë¡œ payload ìƒì„±
        payload = {
            "pagination": {
                "page": page,  # pageëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ìŒ
                "size": 50
            },
            "recruitListType": "NORMAL_ALL",
            "sortTabCondition": {
                "searchPeriodType": "ALL",
                "sortType": "DEFAULT"
            },
            "condition": {
                "age": 0,
                "areas": [],
                "educationType": "ALL",
                "employmentTypes": [],
                "endWorkTime": "",
                "excludeBar": False,
                "excludeKeywordList": exclude_keywords_list,
                "excludeKeywords": exclude_keywords_list,
                "excludeNegoAge": False,
                "excludeNegoGender": False,
                "excludeNegoWorkTime": False,
                "excludeNegoWorkWeek": False,
                "genderType": "NONE",
                "includeKeyword": include_keyword,
                "moreThanEducation": False,
                "parts": [],
                "similarDongJoin": False,
                "startWorkTime": "",
                "workDayTypes": [],
                "workPeriodTypes": [],
                "workTimeTypes": [],
                "workWeekTypes": [],
            }
        }

        self.log_signal.emit(f"payload : {payload}")

        response = self.sess.post(url, headers=headers, json=payload, timeout=10)

        if response.status_code == 200:
            data = response.json()
            collection_list = data.get("base", {}).get("normal", {}).get("collection", [])
            pagination = data.get("base", {}).get("pagination", {})
            return collection_list, pagination
        else:
            print(f"Error: {response.status_code}")
            return [], {}



    # í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    def get_api_request(self, recruit_no):
        url = f"https://www.albamon.com/jobs/detail/{recruit_no}?logpath=7&productCount=1"

        headers = {
            "authority": "www.albamon.com",
            "method": "GET",
            "path": "/jobs/detail/107967948?logpath=7&productCount=1",
            "scheme": "https",
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "cookie": "_ga=GA1.1.1589651135.1739978839; ConditionId=1187621C-98D0-44C9-AE4E-D1D3869438EF; ab.storage.deviceId.7a5f1472-069a-4372-8631-2f711442ee40=%7B%22g%22%3A%22c3f5d6c8-3939-dca6-cfae-3a6ade1a2651%22%2C%22c%22%3A1739978837484%2C%22l%22%3A1740054277046%7D; AM_USER_UUID=b0949b94-81f4-40d0-9821-b06f97df5dfa",
            "sec-ch-ua": '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133")',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "none",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
        }


        try:
            res = self.sess.get(url, headers=headers, timeout=10)

            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, "html.parser")
                script_tag = soup.find("script", {"id": "__NEXT_DATA__", "type": "application/json"})
                if script_tag:
                    json_data = json.loads(script_tag.string)
                    data = json_data.get("props", {}).get("pageProps", {}).get("data", {})
                    self.log_signal.emit(f"íšŒì‚¬ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
                    return data
                else:
                    print("JSON ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # ìƒíƒœ ì½”ë“œê°€ 200ì´ ì•„ë‹Œ ê²½ìš°
                self.log_signal.emit(f"HTTP ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {res.status_code}, ë‚´ìš©: {res.text}")
                return None

        except Exception as e:
            print(f'error : {e}')
            # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë˜ëŠ” ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            self.log_signal.emit(f"ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None

    # ì „ì²´ ê°¯ìˆ˜ ì¡°íšŒ
    def total_cnt_cal(self):
        try:
            collection, pagination = self.main_request(1)

            total_count = pagination.get('totalCount', 0)
            page_size = pagination.get('size', 1)  # 0 ë°©ì§€

            total_page_cnt = math.ceil(total_count / page_size)
            total_product_cnt = total_count

            self.total_cnt = total_product_cnt
            self.total_pages = total_page_cnt

        except Exception as e:
            print(f"Error calculating total count: {e}")
            return 0, 0
