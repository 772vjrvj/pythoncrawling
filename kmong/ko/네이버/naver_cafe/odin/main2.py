import requests
import json
import time
from bs4 import BeautifulSoup
import openpyxl
from datetime import datetime
import re
import os
import csv

BASE_API = "https://m.cafe.daum.net/api/v1/common-articles"
BASE_VIEW = "https://cafe.daum.net/odin"
GRPID = "1YvZ5"
PAGE_SIZE = 50
STOP_DATE = "23.08.11"  # ë°œê²¬ ì‹œ ì¢…ë£Œ
HEADERS = {
    "Accept": "application/json",
    "User-Agent": "Mozilla/5.0"
}
TODAY_STR = datetime.today().strftime("%y.%m.%d")

# CSV ì €ì¥ ë‹¨ìœ„(ê°œ)
CSV_CHUNK_SIZE = 100

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Excel ë¶ˆë²• ì œì–´ë¬¸ì ì œê±° (openpyxl IllegalCharacterError ëŒ€ì‘)
# í—ˆìš©: \t, \n, \r / ì œê±°: ê·¸ ì™¸ 0x00-0x1F
ILLEGAL_CTRL_RE = re.compile(r'[\x00-\x08\x0B\x0C\x0E-\x1F]')

def clean_text(value) -> str:
    """ì—‘ì…€ì´ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ì œì–´ë¬¸ì ì œê±°"""
    if value is None:
        return ""
    if not isinstance(value, str):
        value = str(value)
    return ILLEGAL_CTRL_RE.sub("", value)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ê³µí†µ í—¤ë”
HEADERS_ROW = ["ê²Œì‹œíŒ", "ì‘ì„± ë‚ ì§œ", "ê²Œì‹œê¸€ ì œëª©", "ê²Œì‹œê¸€ ë‚´ìš©", "url", "id"]

# ----------- API ìš”ì²­ -----------
def fetch_page(fldid, page_num, after=None):
    params = {
        "grpid": GRPID,
        "fldid": fldid,
        "pageSize": PAGE_SIZE,
        "targetPage": page_num
    }
    if after:
        params["afterBbsDepth"] = after

    r = requests.get(BASE_API, params=params, headers=HEADERS, timeout=10)
    r.raise_for_status()
    return r.json()

# ----------- ìˆ˜ì§‘ -----------
def collect_articles(fldid):
    all_data = []
    after_cursor = None
    page_num = 1

    while True:
        data = fetch_page(fldid, page_num, after_cursor)
        articles = data.get("articles", [])

        if not articles:
            print(f"ğŸ“Œ [{fldid}] ë” ì´ìƒ ë°ì´í„° ì—†ìŒ. ì¢…ë£Œ")
            break

        last_date = articles[-1].get("articleElapsedTime", "N/A")

        for article in articles:
            if article.get("articleElapsedTime") == STOP_DATE:
                print(f"ğŸ“Œ [{fldid}] {STOP_DATE} ë°œê²¬ â†’ ìˆ˜ì§‘ ì¢…ë£Œ")
                return all_data
            all_data.append(article)

        print(f"[{fldid} | Page {page_num}] {len(articles)}ê±´ ìˆ˜ì§‘ / ëˆ„ì  {len(all_data)}ê±´ / ë§ˆì§€ë§‰ ë‚ ì§œ {last_date}")

        after_cursor = articles[-1].get("bbsDepth")
        if not after_cursor:
            print(f"ğŸ“Œ [{fldid}] ë‹¤ìŒ ì»¤ì„œ ì—†ìŒ. ì¢…ë£Œ")
            break

        page_num += 1
        time.sleep(0.3)

    return all_data

# ----------- HTML íŒŒì‹± -----------
def parse_article_content(fldid, dataid):
    url = f"https://m.cafe.daum.net/odin/{fldid}/{dataid}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Linux; Android 10; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0 Mobile Safari/537.36",
        "Referer": f"https://m.cafe.daum.net/odin/{fldid}"
    }
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ [{fldid}] {dataid} ìš”ì²­ ì‹¤íŒ¨: {e}")
        return ""

    soup = BeautifulSoup(res.text, "html.parser")

    # 1) ê¸°ë³¸: id="article" ì•ˆì˜ <p> íƒœê·¸ë“¤
    article_div = soup.find("div", id="article", class_="tx-content-container")
    paragraphs = []
    if article_div:
        for p in article_div.find_all("p"):
            for br in p.find_all("br"):
                br.replace_with("\n")
            text = p.get_text(strip=True)
            if text:
                paragraphs.append(text)

    # 2) <p>ê°€ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë¹„ì—ˆì„ ë•Œ â†’ id="protectTable" ì•ˆì˜ text ì‚¬ìš©
    if not paragraphs:
        protect_div = soup.find(id="protectTable")
        if protect_div:
            text = protect_div.get_text(strip=True)
            if text:
                paragraphs.append(text)

    return "\n".join(paragraphs)

# ----------- CSV ìœ í‹¸ -----------
def get_out_paths(board_name: str):
    """í˜„ì¬ ì‘ì—… í´ë” ê¸°ì¤€ìœ¼ë¡œ JSON/CSV/XLSX íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    json_path = os.path.abspath(f"odin_{board_name}.json")
    base_dir = os.path.dirname(json_path)
    csv_path  = os.path.join(base_dir, f"odin_{board_name}.csv")
    xlsx_path = os.path.join(base_dir, f"odin_{board_name}.xlsx")
    return json_path, csv_path, xlsx_path

def ensure_csv_header(csv_path: str):
    """CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±"""
    if not os.path.exists(csv_path):
        with open(csv_path, "w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            w.writerow(HEADERS_ROW)
        print(f"ğŸ§¾ CSV í—¤ë” ìƒì„±: {csv_path}")

def append_rows(csv_path: str, rows: list):
    """rowsë¥¼ CSVì— append (ì´ë¯¸ sanitizeëœ ê°’ ì‚¬ìš© ê¶Œì¥)"""
    if not rows:
        return
    with open(csv_path, "a", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerows(rows)
    print(f"ğŸ’¾ CSV append ì €ì¥: +{len(rows)} rows â†’ {csv_path}")

def row_from_item(board_name: str, fldid: str, item: dict) -> list:
    """API ì•„ì´í…œì„ CSV 1í–‰ìœ¼ë¡œ ë³€í™˜ + sanitize"""
    date_str = item.get("articleElapsedTime", "") or ""
    if ("ë¶„ ì „" in date_str) or ("ì‹œê°„ ì „" in date_str) or ("ì´ˆ ì „" in date_str):
        date_str = TODAY_STR

    title   = item.get("title", "") or ""
    content = item.get("content", "") or ""
    dataid  = str(item.get("dataid", "") or "")
    url     = f"{BASE_VIEW}/{fldid}/{dataid}" if dataid else ""

    row = [
        board_name,
        date_str,
        title,
        content,
        url,
        dataid
    ]
    # sanitize
    return [clean_text(v) for v in row]

# ----------- CSV â†’ XLSX ë³€í™˜ -----------
def csv_to_xlsx(csv_path: str, xlsx_path: str):
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "Articles"

    with open(csv_path, "r", encoding="utf-8-sig", newline="") as f:
        reader = csv.reader(f)
        for row in reader:
            # XLSX ì“°ê¸° ì „ì— ì¶”ê°€ sanitize
            safe_row = [clean_text(c) for c in row]
            ws.append(safe_row)

    wb.save(xlsx_path)
    print(f"âœ… ìµœì¢… ì—‘ì…€ ì €ì¥ ì™„ë£Œ â†’ {xlsx_path}")

# ----------- ë©”ì¸ ì‹¤í–‰ -----------
def run_for_board(board_name, fldid):
    print(f"\n===== [{board_name}] ìˆ˜ì§‘ ì‹œì‘ =====")
    json_path, csv_path, xlsx_path = get_out_paths(board_name)

    # 1. JSON ìˆ˜ì§‘ ì €ì¥
    articles = collect_articles(fldid)
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ â†’ {json_path}")

    # 2. CSV ì¤€ë¹„(í—¤ë”)
    ensure_csv_header(csv_path)

    # 3. ìƒì„¸ í˜ì´ì§€ íŒŒì‹± + 100ê°œë§ˆë‹¤ CSV append
    buffer = []
    total = len(articles)
    for idx, art in enumerate(articles, 1):
        dataid = art.get("dataid")
        if not dataid:
            continue

        content = parse_article_content(fldid, dataid)
        art["content"] = content

        buffer.append(row_from_item(board_name, fldid, art))

        # 100ê°œ ë‹¨ìœ„ë¡œ ì €ì¥
        if len(buffer) == CSV_CHUNK_SIZE:
            append_rows(csv_path, buffer)
            buffer = []

        print(f"[{board_name}] detail {idx}/{total} dataid={dataid} âœ…")
        time.sleep(0.15)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    # ë‚¨ì€ ë°ì´í„° ì €ì¥
    if buffer:
        append_rows(csv_path, buffer)

    # 4. CSV â†’ XLSX ë³€í™˜
    csv_to_xlsx(csv_path, xlsx_path)

    print(f"ğŸ‰ [{board_name}] ì „ì²´ ì™„ë£Œ")

if __name__ == "__main__":
    # ììœ ê²Œì‹œíŒ
    # run_for_board("ììœ ê²Œì‹œíŒ", "D034")

    # ì˜¤ë”˜ ê´‘ì¥
    run_for_board("ì˜¤ë”˜ê´‘ì¥", "DjO0")
