import requests
import json
import time

BASE_URL = "https://m.cafe.daum.net/api/v1/common-articles"
GRPID = "1YvZ5"
FLDID = "D034"
PAGE_SIZE = 50
OUTPUT_FILE = "odin.json"
STOP_DATE = "23.08.11"  # ë°œê²¬ ì‹œ ì¢…ë£Œ

HEADERS = {
    "Accept": "application/json",
    "User-Agent": "Mozilla/5.0"
}

def fetch_page(page_num, after=None):
    params = {
        "grpid": GRPID,
        "fldid": FLDID,
        "pageSize": PAGE_SIZE,
        "targetPage": page_num
    }
    if after:
        params["afterBbsDepth"] = after

    r = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=10)
    r.raise_for_status()
    return r.json()

def main():
    all_data = []
    after_cursor = None
    page_num = 1

    while True:
        data = fetch_page(page_num, after_cursor)
        articles = data.get("articles", [])

        if not articles:
            print("ğŸ“Œ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ. ì¢…ë£Œ")
            break

        last_date = articles[-1].get("articleElapsedTime", "N/A")

        for article in articles:
            if article.get("articleElapsedTime") == STOP_DATE:
                print(f"ğŸ“Œ {STOP_DATE} ë°œê²¬ â†’ ìˆ˜ì§‘ ì¢…ë£Œ")
                with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
                    json.dump(all_data, f, ensure_ascii=False, indent=2)
                print(f"âœ… {len(all_data)}ê±´ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}")
                return
            all_data.append(article)

        print(f"[Page {page_num}] {len(articles)}ê±´ ìˆ˜ì§‘ / ëˆ„ì  {len(all_data)}ê±´ / ë§ˆì§€ë§‰ ë‚ ì§œ {last_date}")

        after_cursor = articles[-1].get("bbsDepth")
        if not after_cursor:
            print("ğŸ“Œ ë‹¤ìŒ ì»¤ì„œ ì—†ìŒ. ì¢…ë£Œ")
            break

        page_num += 1  # targetPage ì¦ê°€
        time.sleep(0.3)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… {len(all_data)}ê±´ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
