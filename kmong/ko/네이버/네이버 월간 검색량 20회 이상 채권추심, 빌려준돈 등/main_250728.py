import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# ì£¼ìš” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
main_keywords = [
    'ë¹Œë ¤ì¤€ ëˆ', 'ì–‘ìœ¡ë¹„', 'ì„ëŒ€ë£Œ', 'ê³µì‚¬ëŒ€ê¸ˆ', 'ë¬¼í’ˆëŒ€ê¸ˆ',
    'ê³„ì•½ê¸ˆë°˜í™˜', 'í‡´ì§ê¸ˆ', 'ì„ê¸ˆ', 'ë³´ì¦ê¸ˆ',
    'ìƒê°„ë…€ ìƒê°„ë‚¨ ìœ„ìë£Œ', 'íŒê²° í›„ ëª» ë°›ì€ ëˆ', 'ì†Œì†¡ë¹„ìš©', 'ëŒ€ì—¬ê¸ˆ'
]

max_keywords = 50  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜

# ê³µí†µ headers
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}


def crawl_related_keywords(keyword):
    """
    ë‹¨ì¼ í‚¤ì›Œë“œë¡œ ì—°ê´€ í‚¤ì›Œë“œ í…Œì´ë¸” í¬ë¡¤ë§
    """
    related = []
    try:
        url = f'https://www.cardveryview.com/ë„¤ì´ë²„-í‚¤ì›Œë“œ-ê²€ìƒ‰ëŸ‰-ì¡°íšŒ-í™•ì¸/?keyword={keyword}'
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        table = soup.find('table', id='keywordTableth')

        if table:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) == 5:
                    kw = cols[0].text.strip()
                    if kw:
                        related.append(kw)
    except Exception as e:
        print(f"âŒ [{keyword}] ìš”ì²­ ì‹¤íŒ¨: {e}")
    return related


def get_full_related_keywords(seed_keyword, max_count):
    """
    í•˜ë‚˜ì˜ ë©”ì¸ í‚¤ì›Œë“œì—ì„œ ì—°ê´€ í‚¤ì›Œë“œë¥¼ ìµœëŒ€ max_countê¹Œì§€ ìˆ˜ì§‘
    """
    collected = []
    visited = set()
    queue = [seed_keyword]

    while queue and len(collected) < max_count:
        current = queue.pop(0)
        if current in visited:
            continue
        visited.add(current)

        print(f"ğŸ” '{seed_keyword}' ê´€ë ¨ â†’ '{current}' ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜ì§‘ ì¤‘... ({len(collected)}/{max_count})")
        new_keywords = crawl_related_keywords(current)

        for kw in new_keywords:
            if kw not in collected and len(collected) < max_count:
                collected.append(kw)
                queue.append(kw)  # ìˆ˜ì§‘í•œ í‚¤ì›Œë“œë„ ë‹¤ìŒ íƒìƒ‰ ëŒ€ìƒìœ¼ë¡œ ì¶”ê°€

        time.sleep(1)  # ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€

    return collected


if __name__ == '__main__':
    start_time = time.time()
    result_dict = {}

    for main_kw in main_keywords:
        result = get_full_related_keywords(main_kw, max_keywords)
        # ë¶€ì¡±í•œ ê²½ìš° ë¹ˆì¹¸ ì±„ìš°ê¸°
        result += [''] * (max_keywords - len(result))
        result_dict[main_kw] = result

    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(result_dict)
    df.to_excel('keyword_search_volume.xlsx', index=False)

    end_time = time.time()
    print(f"âœ… ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘ ì™„ë£Œ. ì—‘ì…€ ì €ì¥ë¨. ì´ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
