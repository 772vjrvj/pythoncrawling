import os
import ssl
import time
import json

import pandas as pd
import requests
from PyQt5.QtCore import QThread, pyqtSignal
from bs4 import BeautifulSoup
from selenium import webdriver
import math
from datetime import datetime  # ìˆ˜ì • ì¶”ê°€
import threading
import pyautogui  # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
import urllib.parse

from src.utils.time_utils import get_current_yyyymmddhhmmss, get_current_formatted_datetime
from selenium.webdriver.common.alert import Alert

ssl._create_default_https_context = ssl._create_unverified_context

image_main_directory = 'albamon_images'
company_name = 'ì•Œë°”ëª¬'
site_name = 'albamon'
excel_filename = ''
baseUrl = "https://www.albamon.com/jobs/area"
select_baseUrl = ""


# API
class ApiAlbamonSetLoadWorker(QThread):
    log_signal = pyqtSignal(str)         # ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_signal = pyqtSignal(float, float)  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_end_signal = pyqtSignal()   # ì¢…ë£Œ ì‹œê·¸ë„
    msg_signal = pyqtSignal(str, str, object)

    # ì´ˆê¸°í™”
    def __init__(self, checked_list):
        super().__init__()
        self.baseUrl = baseUrl
        self.sess = requests.Session()
        self.checked_list = checked_list

        self.running = True  # ì‹¤í–‰ ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€
        self.driver = None

        self.com_list = []
        self.main_model = None
        self.product_info_list = []

        self.total_cnt = 0
        self.total_pages = 0
        self.current_page = 0
        self.current_cnt = 0
        self.before_pro_value = 0

    # í”„ë¡œê·¸ë¨ ì‹¤í–‰
    def run(self):
        global image_main_directory, company_name, site_name, excel_filename, baseUrl

        self.log_signal.emit("í¬ë¡¤ë§ ì‹œì‘")
        result_list = []
        self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì„ ì‹œë„ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        self.login()
        self.wait_for_user_confirmation()

        self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì— ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.")
        self.log_signal.emit(f"ì „ì²´ íšŒì‚¬ìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        self.total_cnt_cal()
        self.log_signal.emit(f"ì „ì²´ íšŒì‚¬ìˆ˜ {self.total_cnt} ê°œ")
        self.log_signal.emit(f"ì „ì²´ í˜ì´ì§€ìˆ˜ {self.total_pages} ê°œ")

        csv_filename = os.path.join(os.getcwd(), f"ì•Œë°”ëª¬_{get_current_yyyymmddhhmmss()}.csv")
        columns = ["NO", "ì‚¬ì—…ì²´ëª…", "ì±„ìš©ë‹´ë‹¹ìëª…", "íœ´ëŒ€í° ë²ˆí˜¸", "ê·¼ë¬´ì§€ ì£¼ì†Œ", "ì§€ì—­1", "ì§€ì—­2", "ì§€ì—­3", "ê¸‰ì—¬ ì •ë³´", "ê·¼ë¬´ ê¸°ê°„", "ë“±ë¡ì¼",
                   "ê·¼ë¬´ ìš”ì¼", "ê·¼ë¬´ ì‹œê°„", "ê³ ìš© í˜•íƒœ", "ë³µë¦¬í›„ìƒ ì •ë³´", "ì—…ì§ì¢…", "ì—…ì¢…", "ëŒ€í‘œìëª…", "ê¸°ì—…ì£¼ì†Œ"]
        df = pd.DataFrame(columns=columns)
        df.to_csv(csv_filename, index=False, encoding="utf-8-sig")

        for page in range(1, self.total_pages + 1):
            self.log_signal.emit(f"í˜„ì¬ í˜ì´ì§€ {page}")
            time.sleep(1)
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                self.log_signal.emit("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            collection, pagination = self.main_request(page)

            for index, data in enumerate(collection):

                if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                    self.log_signal.emit("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break

                time.sleep(1)

                # í°ë²ˆí˜¸ê°€ ì—†ëŠ”ê²½ìš°
                if data.get('managerPhoneNumber', '') == '':
                    self.log_signal.emit(f"ë²ˆí˜¸ ì—†ìŒ Skip")
                    self.current_cnt = self.current_cnt + 1
                    pro_value = (self.current_cnt / self.total_cnt) * 1000000
                    self.progress_signal.emit(self.before_pro_value, pro_value)
                    self.before_pro_value = pro_value
                    self.log_signal.emit(f"í˜„ì¬ í˜ì´ì§€ {self.current_cnt}/{self.total_cnt}")
                    continue

                scraped_date = data.get("scrapedDate", "")

                # ì„œìš¸ ê°•ì„œêµ¬ ë§ˆê³¡ë™
                workplace_area = data.get("workplaceArea", "").strip()
                area_parts = workplace_area.split() if workplace_area else []

                obj = {
                    "NO": data.get('recruitNo', ''),
                    # "ì‚¬ì—…ì²´ëª…": data.get('companyName', ''),
                    "ì±„ìš©ë‹´ë‹¹ìëª…": '',
                    "íœ´ëŒ€í° ë²ˆí˜¸": data.get('managerPhoneNumber', ''),
                    "ê·¼ë¬´ì§€ ì£¼ì†Œ": data.get('workplaceAddress', ''),
                    "ì§€ì—­": data.get('workplaceArea', ''),    # ì„œìš¸ ì¤‘êµ¬
                    "ê¸‰ì—¬ ì •ë³´": data.get('pay', ''),
                    "ë“±ë¡ì¼": scraped_date,
                    "ê·¼ë¬´ ê¸°ê°„": data.get('workingPeriod', ''),
                    "ê·¼ë¬´ ìš”ì¼": data.get('workingWeek', ''),
                    "ê·¼ë¬´ ì‹œê°„": data.get('workingTime', ''),
                    "ê³ ìš© í˜•íƒœ": data.get('recruitType', {}).get('description', ''),
                    "ë³µë¦¬í›„ìƒ ì •ë³´": data.get('filterTotal', ''),
                    "ì—…ì§ì¢…": data.get('parts', ''),
                    "ì§€ì—­1": area_parts[0] if len(area_parts) > 0 else "",
                    "ì§€ì—­2": area_parts[1] if len(area_parts) > 1 else "",
                    "ì§€ì—­3": area_parts[2] if len(area_parts) > 2 else "",
                    "ì‚¬ì—…ì²´ëª…": "",
                    "ì—…ì¢…": "",
                    "ëŒ€í‘œìëª…": "",
                    "ê¸°ì—…ì£¼ì†Œ": "",
                }

                detail_data = self.get_api_request(data.get('recruitNo', ''))

                if detail_data:
                    obj['ì±„ìš©ë‹´ë‹¹ìëª…'] = detail_data.get('viewData', {}).get('recruiter', '')
                    obj['ë“±ë¡ì¼'] = detail_data.get('viewData', {}).get('pcSortDate', '')
                    # obj['ì‚¬ì—…ì²´ëª…'] = detail_data.get('viewData',{}).get('recruitCompanyName','')
                    obj['ì‚¬ì—…ì²´ëª…'] = detail_data.get('companyData', {}).get('companyName', '')
                    obj['ì—…ì¢…'] = detail_data.get('companyData', {}).get('jobTypeName', '')
                    obj['ëŒ€í‘œìëª…'] = detail_data.get('companyData', {}).get('representativeName', '')
                    obj['ê¸°ì—…ì£¼ì†Œ'] = detail_data.get('companyData', {}).get('fullAddress', '')

                self.log_signal.emit(f"í˜„ì¬ ì±„ìš© ì •ë³´ : {obj}")

                result_list.append(obj)

                if (index + 1) % 5 == 0:
                    df = pd.DataFrame(result_list, columns=columns)
                    df.to_csv(csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")
                    result_list.clear()

                self.current_cnt = self.current_cnt + 1

                pro_value = (self.current_cnt / self.total_cnt) * 1000000
                self.progress_signal.emit(self.before_pro_value, pro_value)
                self.before_pro_value = pro_value
                self.log_signal.emit(f"í˜„ì¬ í˜ì´ì§€ {self.current_cnt}/{self.total_cnt}")

            if result_list:
                df = pd.DataFrame(result_list, columns=columns)
                df.to_csv(csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")

        self.progress_signal.emit(self.before_pro_value, 1000000)
        time.sleep(3)
        self.log_signal.emit("=============== í¬ë¡¤ë§ ì¢…ë£Œ")
        self.progress_end_signal.emit()

    # í”„ë¡œê·¸ë¨ ì¤‘ë‹¨
    def stop(self):
        """ìŠ¤ë ˆë“œ ì¤‘ì§€ë¥¼ ìš”ì²­í•˜ëŠ” ë©”ì„œë“œ"""

        self.running = False

    def wait_for_user_confirmation(self):
        """ì‚¬ìš©ìê°€ í™•ì¸(alert) ì°½ì—ì„œ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°"""
        event = threading.Event()  # OK ë²„íŠ¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°í•  ì´ë²¤íŠ¸ ê°ì²´

        # ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì°½ ìš”ì²­
        self.msg_signal.emit("ì˜µì…˜ì„ ì„ íƒ í›„ OKë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”", "info", event)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.log_signal.emit("ğŸ“¢ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...")
        event.wait()  # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥´ë©´ í•´ì œë¨

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆŒë €ì„ ê²½ìš° ì‹¤í–‰
        self.log_signal.emit("âœ… ì‚¬ìš©ìê°€ í™•ì¸ ë²„íŠ¼ì„ ëˆŒë €ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‘ì—… ì§„í–‰ ì¤‘...")
        time.sleep(2)  # ì˜ˆì œìš©
        self.log_signal.emit("ğŸš€ ì‘ì—… ì™„ë£Œ!")

        # í˜„ì¬ ë¸Œë¼ìš°ì €ì˜ URLì„ ê°€ì ¸ì™€ ì €ì¥
        self.select_baseUrl = self.driver.current_url
        self.log_signal.emit(f"ğŸŒ í˜„ì¬ ë¸Œë¼ìš°ì € URL ì €ì¥ë¨: {self.select_baseUrl}")


    def login(self):
        webdriver_options = webdriver.ChromeOptions()

        ###### ìë™ ì œì–´ ê°ì§€ ë°©ì§€ #####
        webdriver_options.add_argument('--disable-blink-features=AutomationControlled')

        ##### í™”ë©´ ìµœëŒ€ #####
        # webdriver_options.add_argument("--start-maximized")  # ìµœëŒ€í™” ëŒ€ì‹  í¬ê¸° ì¡°ì ˆ ì‚¬ìš©

        ##### headless ëª¨ë“œ ë¹„í™œì„±í™” (ë¸Œë¼ìš°ì € UI ë³´ì´ê²Œ) #####
        # webdriver_options.add_argument("--headless")

        ##### ìë™ ê²½ê³  ì œê±° #####
        webdriver_options.add_experimental_option('useAutomationExtension', False)

        ##### ë¡œê¹… ë¹„í™œì„±í™” #####
        webdriver_options.add_experimental_option('excludeSwitches', ['enable-logging'])

        ##### ìë™í™” ë„êµ¬ ì‚¬ìš© ê°ì§€ ì œê±° #####
        webdriver_options.add_experimental_option("excludeSwitches", ["enable-automation"])

        # ë“œë¼ì´ë²„ ì‹¤í–‰
        self.driver = webdriver.Chrome(options=webdriver_options)
        self.driver.set_page_load_timeout(120)

        # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸°
        screen_width, screen_height = pyautogui.size()

        # ì°½ í¬ê¸°ë¥¼ ë„ˆë¹„ ì ˆë°˜, ë†’ì´ ì „ì²´ë¡œ ì„¤ì •
        self.driver.set_window_size(screen_width // 2, screen_height)

        # ì°½ ìœ„ì¹˜ë¥¼ ì™¼ìª½ ìƒë‹¨ì— ë°°ì¹˜
        self.driver.set_window_position(0, 0)

        # ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°
        self.driver.get(self.baseUrl)

        # ì¿ í‚¤ ì„¤ì •
        cookies = self.driver.get_cookies()
        for cookie in cookies:
            self.sess.cookies.set(cookie['name'], cookie['value'])


    def parse_albamon_url(self, url):
        """Albamon ê²€ìƒ‰ URLì„ ë¶„ì„í•˜ì—¬ payloadì— ë§ëŠ” ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
        parsed_url = urllib.parse.urlparse(url)
        query_params = urllib.parse.parse_qs(parsed_url.query)

        # page ì œì™¸í•˜ê³  ê¸°ë³¸ payload ì„¤ì •
        payload = {
            "pagination": {
                "page": 1,  # pageëŠ” ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ìŒ
                "size": 50
            },
            "recruitListType": "AREA",
            "sortTabCondition": {
                "searchPeriodType": "ALL",
                "sortType": "DEFAULT"
            },
            "condition": {
                "areas": [],
                "employmentTypes": query_params.get("employmentTypes", []),
                "excludeKeywords": [],
                "excludeBar": False,
                "excludeNegoAge": query_params.get("excludeNegoAge", ["False"])[0] == "true",
                "excludeNegoWorkWeek": query_params.get("excludeNegoWorkWeek", ["False"])[0] == "true",
                "excludeNegoWorkTime": query_params.get("excludeNegoWorkTime", ["False"])[0] == "true",
                "excludeNegoGender": False,
                "parts": query_params.get("parts", []),
                "similarDongJoin": False,
                "workDayTypes": query_params.get("workDayTypes", []),
                "workPeriodTypes": query_params.get("workPeriodTypes", []),
                "workTimeTypes": query_params.get("workTimeTypes", []),
                "workWeekTypes": query_params.get("workWeekTypes", []),
                "endWorkTime": query_params.get("endWorkTime", [""])[0],
                "startWorkTime": query_params.get("startWorkTime", [""])[0],
                "includeKeyword": query_params.get("includeKeyword", [""])[0],
                "excludeKeywordList": query_params.get("excludeKeywordList", []),
                "age": int(query_params.get("age", [0])[0]),
                "genderType": query_params.get("genderType", ["NONE"])[0],
                "moreThanEducation": False,
                "educationType": query_params.get("educationType", ["ALL"])[0],
                "selectedArea": {"si": "", "gu": "", "dong": ""}
            }
        }

        # areas íŒŒì‹±
        areas = query_params.get("areas", [""])
        parsed_areas = []

        # areasë¥¼ ',' ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        split_areas = []
        for area_group in areas:
            split_areas.extend(area_group.split(","))


        for area in split_areas:
            if not area:
                continue

            si = area[:1] + "000"  # ì‹œ (ì²« ë²ˆì§¸ ìë¦¬ + '000')
            gu = area[:4] if len(area) >= 4 else ""  # 4ìë¦¬ êµ¬
            dong = area if len(area) >= 8 else ""  # 6ìë¦¬ ë™

            # ë§Œì•½ ì‹œ ì½”ë“œ(G000)ë§Œ ì¡´ì¬í•œë‹¤ë©´ guì™€ dongì„ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
            if area == si:
                gu = ""
                dong = ""

            parsed_areas.append({"si": si, "gu": gu, "dong": dong})

        payload["condition"]["areas"] = parsed_areas

        return payload

    def main_request(self, page=1):
        """í˜„ì¬ ë¸Œë¼ìš°ì € URLì„ ê¸°ë°˜ìœ¼ë¡œ API ìš”ì²­"""
        url = "https://bff-general.albamon.com/recruit/search"

        headers = {
            "authority": "bff-general.albamon.com",
            "method": "POST",
            "path": "/recruit/search",
            "scheme": "https",
            "accept": "application/json",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "albamon-domain-type": "pc",
            "content-type": "application/json",
            "origin": "https://www.albamon.com",
            "priority": "u=1, i",
            # "referer": self.driver.current_url,
            "sec-ch-ua": '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133")',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-site",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
        }

        # í˜„ì¬ ë¸Œë¼ìš°ì € URLì„ ê¸°ë°˜ìœ¼ë¡œ payload ìƒì„±
        payload = self.parse_albamon_url(self.driver.current_url)
        payload["pagination"]["page"] = page  # pageëŠ” ë”°ë¡œ ë°›ìŒ

        response = self.sess.post(url, headers=headers, json=payload, timeout=10)

        if response.status_code == 200:
            data = response.json()
            collection_list = data.get("base", {}).get("normal", {}).get("collection", [])
            pagination = data.get("base", {}).get("pagination", {})
            return collection_list, pagination
        else:
            print(f"Error: {response.status_code}")
            return [], {}



    # í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    def get_api_request(self, recruit_no):
        url = f"https://www.albamon.com/jobs/detail/{recruit_no}?logpath=7&productCount=1"

        headers = {
            "authority": "www.albamon.com",
            "method": "GET",
            "path": "/jobs/detail/107967948?logpath=7&productCount=1",
            "scheme": "https",
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "cookie": "_ga=GA1.1.1589651135.1739978839; ConditionId=1187621C-98D0-44C9-AE4E-D1D3869438EF; ab.storage.deviceId.7a5f1472-069a-4372-8631-2f711442ee40=%7B%22g%22%3A%22c3f5d6c8-3939-dca6-cfae-3a6ade1a2651%22%2C%22c%22%3A1739978837484%2C%22l%22%3A1740054277046%7D; AM_USER_UUID=b0949b94-81f4-40d0-9821-b06f97df5dfa",
            "sec-ch-ua": '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133")',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "none",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
        }


        try:
            res = self.sess.get(url, headers=headers, timeout=10)

            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, "html.parser")
                script_tag = soup.find("script", {"id": "__NEXT_DATA__", "type": "application/json"})
                if script_tag:
                    json_data = json.loads(script_tag.string)
                    data = json_data.get("props", {}).get("pageProps", {}).get("data", {})
                    self.log_signal.emit(f"íšŒì‚¬ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ")
                    return data
                else:
                    print("JSON ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # ìƒíƒœ ì½”ë“œê°€ 200ì´ ì•„ë‹Œ ê²½ìš°
                self.log_signal.emit(f"HTTP ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {res.status_code}, ë‚´ìš©: {res.text}")
                return None

        except Exception as e:
            print(f'error : {e}')
            # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë˜ëŠ” ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            self.log_signal.emit(f"ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None

    # ì „ì²´ ê°¯ìˆ˜ ì¡°íšŒ
    def total_cnt_cal(self):
        try:
            collection, pagination = self.main_request(1)

            total_count = pagination.get('totalCount', 0)
            page_size = pagination.get('size', 1)  # 0 ë°©ì§€

            total_page_cnt = math.ceil(total_count / page_size)
            total_product_cnt = total_count

            self.total_cnt = total_product_cnt
            self.total_pages = total_page_cnt

        except Exception as e:
            print(f"Error calculating total count: {e}")
            return 0, 0
