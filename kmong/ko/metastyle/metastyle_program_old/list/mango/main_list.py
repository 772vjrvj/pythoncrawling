from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

import time

driver = None

def setup_driver():
    """Selenium WebDriver ì„¤ì • ë° ë°˜í™˜"""
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless=new")  # ìµœì‹  headless ëª¨ë“œ ì‚¬ìš©
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--disable-infobars")
    options.add_argument("start-maximized")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--allow-running-insecure-content")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/120.0.0.0")

    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def scrape_mango():
    global driver
    """Mango ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìƒí’ˆ URL ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
    driver = setup_driver()
    driver.get("https://shop.mango.com/us/en/c/women/new-now_56b5c5ed")
    time.sleep(5)
    product_links = []
    # Sticky_viewItem__7OMDF í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸° (3ê°œ ì¤‘ 3ë²ˆì§¸ ìš”ì†Œ í´ë¦­)
    view_items = driver.find_elements(By.CLASS_NAME, "Sticky_viewItem__7OMDF")
    if len(view_items) >= 3:
        view_items[2].click()
        time.sleep(3)

    # ìŠ¤í¬ë¡¤ì„ ëê¹Œì§€ ë‚´ë¦¬ê¸°
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        time.sleep(3)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

    # ğŸ’¡ ìŠ¤í¬ë¡¤ ì™„ë£Œ í›„ ë Œë”ë§ ëŒ€ê¸° (a íƒœê·¸ ê°™ì€ ìš”ì†Œê°€ ë¡œë”©ë  ì‹œê°„)
    time.sleep(5)

    # ìƒí’ˆ ë§í¬ ìˆ˜ì§‘

    grid_container = driver.find_element(By.CLASS_NAME, "Grid_grid__fLhp5.Grid_overview___rpEH")
    items = grid_container.find_elements(By.TAG_NAME, "li")

    for item in items:
        try:
            data_slot = item.get_attribute("data-slot")
            print(f"\n[data-slot: {data_slot}]")

            # ì•ˆì „í•œ ë°©ì‹
            a_tags = item.find_elements(By.TAG_NAME, "a")
            if a_tags:
                href = a_tags[0].get_attribute("href")
                if href:
                    product_links.append(href)
                    print(f"ë§í¬: {href}")
            else:
                print(f"[ê²½ê³ ] a íƒœê·¸ ì—†ìŒ - data-slot: {data_slot}")

        except Exception as e:
            print(f"ìƒí’ˆ ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("li ì „ì²´ HTML:", item.get_attribute("outerHTML"))
            continue


    print(f"ì´ ìˆ˜ì§‘ëœ ìƒí’ˆ ë§í¬ ê°œìˆ˜:{len(product_links)}")

    return product_links



def scrape_product_details(url):
    global driver
    driver = setup_driver()  # ì—¬ê¸°ì„œ ìƒˆë¡œ ì—´ê¸°
    driver.get(url)
    """ê°œë³„ ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ë§ (Seleniumë§Œ ì‚¬ìš©)"""
    time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

    product_data = {
        "product_category": None,
        "product_name": None,
        "description": None,
        "composition": [],
    }

    try:
        # product_category ê°€ì ¸ì˜¤ê¸°
        try:
            category_element = driver.find_element(By.CLASS_NAME, "texts_uppercaseS__xdp_M.ProductDetail_tags__Eaooa.Tags_tags__knw43")
            product_data["product_category"] = category_element.text.strip()
        except Exception:
            pass

        # product_name ê°€ì ¸ì˜¤ê¸°
        try:
            name_element = driver.find_element(By.CLASS_NAME, "ProductDetail_title___WrC_.texts_titleL__HgQ5x")
            product_data["product_name"] = name_element.text.strip()
        except Exception:
            pass

        # description ê°€ì ¸ì˜¤ê¸°
        try:
            description_element = driver.find_element(By.ID, "truncate-text")
            paragraphs = description_element.find_elements(By.TAG_NAME, "p")
            product_data["description"] = " ".join([p.text.strip() for p in paragraphs if p.text.strip()])
        except Exception:
            pass

        # ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        try:
            image_grid = driver.find_element(By.CLASS_NAME, "ImageGrid_imageGrid__0lrrn")
            img_elements = image_grid.find_elements(By.TAG_NAME, "li")

            for idx, li in enumerate(img_elements[:4]):  # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€ ìˆ˜ì§‘
                try:
                    img_tag = li.find_element(By.TAG_NAME, "img")
                    srcset = img_tag.get_attribute("srcset")

                    if srcset:
                        first_img_url = srcset.split(",")[0].split(" ")[0]  # srcsetì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°
                        product_data[f"img_{idx+1}"] = first_img_url
                except Exception:
                    pass
        except Exception:
            pass

        # Composition í´ë¦­
        try:
            comp_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CLASS_NAME, "ProductDetail_properties__UStvB.ButtonContentLink_default__oqROh.ButtonContentLink_medium__vKMM6"))
            )
            comp_button.click()
            time.sleep(3)

            # Composition ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
            comp_list = driver.find_elements(By.CLASS_NAME, "Composition_list__9lORd")
            for comp in comp_list:
                product_data["composition"].extend([li.text.strip() for li in comp.find_elements(By.TAG_NAME, "li")])

        except Exception:
            pass

    finally:
        driver.quit()

    return product_data


def main():
    # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    product_links = scrape_mango()
    print(product_links)

    # ê°œë³„ ìƒí’ˆ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
    # product_details_list = ["https://shop.mango.com/us/en/p/women/jackets/leather/suede-leather-jacket_87054804?c=09&l=06", "https://shop.mango.com/us/en/p/women/sweaters-and-cardigans/sweaters/openwork-knitted-polo-neck-sweater_87065767?c=08"]
    product_details_list = []
    for idx, url in enumerate(product_links[:3]):  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìƒìœ„ 3ê°œë§Œ ì‹¤í–‰
        print(f"í¬ë¡¤ë§ ì¤‘: {idx+1}/{len(product_links)} -> {url}")
        details = scrape_product_details(url)
        print(details)
        product_details_list.append(details)

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    for product in product_details_list:
        print("\n==============================")
        print(f"ì¹´í…Œê³ ë¦¬: {product['product_category']}")
        print(f"ìƒí’ˆëª…: {product['product_name']}")
        print(f"ì„¤ëª…: {product['description']}")
        print(f"ì´ë¯¸ì§€: {product['images']}")
        print(f"êµ¬ì„± ìš”ì†Œ: {product['composition']}")

if __name__ == "__main__":
    main()
