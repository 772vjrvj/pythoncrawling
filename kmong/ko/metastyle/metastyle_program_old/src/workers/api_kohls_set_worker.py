import os
import re
import ssl
import time

import pandas as pd
import psutil
import requests
from PyQt5.QtCore import QThread, pyqtSignal
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support import expected_conditions as EC

from src.utils.time_utils import get_current_yyyymmddhhmmss, get_current_formatted_datetime

ssl._create_default_https_context = ssl._create_unverified_context

image_folder = 'images'
image_main_directory = 'oldnavy_images'
company_name = 'kohls'
site_name = 'KOHLS'
excel_filename = ''
baseUrl = "https://www.kohls.com/"
db_folder = 'DB'
file_path = os.path.join(db_folder, f'{site_name}.csv')

# API
class ApiKohlsSetLoadWorker(QThread):
    log_signal = pyqtSignal(str)         # ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_signal = pyqtSignal(float, float)  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_end_signal = pyqtSignal()   # ì¢…ë£Œ ì‹œê·¸ë„


    # ì´ˆê¸°í™”
    def __init__(self, checked_list):
        super().__init__()
        self.baseUrl = baseUrl
        self.sess = requests.Session()
        self.checked_list = checked_list

        self.running = True  # ì‹¤í–‰ ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€
        self.driver = None

        self.checked_model_list = []
        self.main_model = None
        self.product_info_list = []
        self.csv_product_list = []
        self.product_list = []
        self.total_cnt = 0
        self.total_pages = 0
        self.current_page = 0
        self.current_cnt = 0
        self.before_pro_value = 0

        self.columns = [
            'product_id', 'product_name', 'product_url', 'product_title',
            'product_sub_title', 'product_features', 'product_fabric_care',
            'product_img_1', 'product_img_2', 'product_img_3', 'product_img_4',
            'data_success', 'img_success', 'img_path', 'success', 'error'
        ]


    # í”„ë¡œê·¸ë¨ ì‹¤í–‰
    def run(self):
        global image_main_directory, company_name, site_name, excel_filename, baseUrl

        self.log_signal.emit("í¬ë¡¤ë§ ì‹œì‘")

        self.csv_product_list = self.load_products()

        if self.checked_list:
            self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ë“œë¼ì´ë²„ ì„¸íŒ…ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
            self._set_driver()
            self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ë“œë¼ì´ë²„ ì„¸íŒ…ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.")

            self.log_signal.emit(f"ì „ì²´ ìƒí’ˆìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
            check_obj_list = self.total_cnt_cal()
            self.total_cnt = sum(int(obj['total_product_cnt']) for obj in check_obj_list)
            self.total_pages = sum(int(obj['total_page_cnt']) for obj in check_obj_list)

            self.log_signal.emit(f"ì „ì²´ í•­ëª©ìˆ˜ {len(check_obj_list)}ê°œ")
            self.log_signal.emit(f"ì „ì²´ ìƒí’ˆìˆ˜ {self.total_cnt} ê°œ")
            self.log_signal.emit(f"ì „ì²´ í˜ì´ì§€ìˆ˜ {self.total_pages} ê°œ")

            for index, checked_model in enumerate(self.checked_list, start=1):
                if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                    self.log_signal.emit("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break

                self.current_cnt = (int(checked_model['start_page']) - 1) * 48
                self.current_page = int(checked_model['start_page'])
                base_url = checked_model['url']

                for indx, page in enumerate(range(int(checked_model['start_page']) - 1, int(checked_model['end_page'])), start=1):
                    if not self.running:
                        break

                    time.sleep(0.5)
                    self.log_signal.emit(f'{checked_model["name"]}({index}/{len(self.checked_list)})  TotalPage({self.current_page}/{self.total_pages})')
                    
                    ws_value = page * 48
                    page_url = f"{base_url}&WS={ws_value}"
                    self.get_products_from_page(page_url, checked_model)

                    self.current_page = self.current_page + 1




        self.progress_signal.emit(self.before_pro_value, 1000000)
        self.log_signal.emit("=============== í¬ë¡¤ë§ ì¢…ë£Œì¤‘...")
        time.sleep(5)
        self.log_signal.emit("=============== í¬ë¡¤ë§ ì¢…ë£Œ")
        self.progress_end_signal.emit()


    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
    def download_images(self, product):
        product_id = product['product_id']
        category_path = os.path.join(image_folder, site_name, product['category_name'].replace("'", "").replace(" ", "").replace("/", "_").strip())
        os.makedirs(category_path, exist_ok=True)

        img_urls = [product['product_img_1'], product['product_img_2'], product['product_img_3'], product['product_img_4']]
        img_paths = []

        for idx, img_url in enumerate(img_urls, start=1):
            if img_url:
                img_filename = f"{product_id}_{idx}.jpg"
                img_filepath = os.path.join(category_path, img_filename)

                try:
                    response = requests.get(img_url, stream=True)
                    if response.status_code == 200:
                        with open(img_filepath, 'wb') as f:
                            for chunk in response.iter_content(1024):
                                f.write(chunk)
                        img_paths.append(img_filepath)
                    else:
                        self.log_signal.emit(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}")
                except Exception as e:
                    self.log_signal.emit(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")

        product['img_path'] = img_paths
        if len(img_paths) == 4:
            product['img_success'] = 'Y'

        return product


    # CSVì— ìƒˆë¡œìš´ í–‰ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
    def append_to_csv(self, new_rows):
        df = pd.DataFrame(new_rows, columns=self.columns)  # ëª…ì‹œì ìœ¼ë¡œ ì»¬ëŸ¼ ìˆœì„œ ì§€ì •
        if not os.path.exists(file_path):
            df.to_csv(file_path, index=False, mode='w', encoding='utf-8-sig')
        else:
            df.to_csv(file_path, index=False, mode='a', header=False, encoding='utf-8-sig')

    # í”„ë¡œê·¸ë¨ ì¤‘ë‹¨
    def stop(self):
        """ìŠ¤ë ˆë“œ ì¤‘ì§€ë¥¼ ìš”ì²­í•˜ëŠ” ë©”ì„œë“œ"""
        self.running = False

    # í¬ë¡¬ ë„ê¸°
    def _close_chrome_processes(self):
        """ëª¨ë“  Chrome í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        for proc in psutil.process_iter(['pid', 'name']):
            try:
                if 'chrome' in proc.info['name'].lower():
                    proc.kill()  # Chrome í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œ
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                pass

    # ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¸íŒ…
    def _set_driver(self):
        try:
            self._close_chrome_processes()

            chrome_options = Options()
            user_data_dir = f"C:\\Users\\{os.getlogin()}\\AppData\\Local\\Google\\Chrome\\User Data"
            profile = "Default"

            chrome_options.add_argument(f"user-data-dir={user_data_dir}")
            chrome_options.add_argument(f"profile-directory={profile}")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-software-rasterizer")
            chrome_options.add_argument("--start-maximized")
            # chrome_options.add_argument("--headless")  # Headless ëª¨ë“œ ì¶”ê°€

            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            chrome_options.add_argument(f'user-agent={user_agent}')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            download_dir = os.path.abspath("downloads")
            os.makedirs(download_dir, exist_ok=True)

            chrome_options.add_experimental_option('prefs', {
                "download.default_directory": download_dir,
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True
            })

            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

            script = '''
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                window.navigator.chrome = { runtime: {} };
                Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
                Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
                Object.defineProperty(navigator, 'userAgent', { get: () => 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' });
            '''
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {'source': script})

            self.driver = driver
        except WebDriverException as e:
            print(f"Error setting up the WebDriver: {e}")
            self.driver = None

    # ì…€ë ˆë‹ˆì›€ ë“œë¼ì´ë²„ ì„¸íŒ…
    def set_driver(self):
        webdriver_options = webdriver.ChromeOptions()

        # ì´ ì˜µì…˜ì€ Chromeì´ ìë™í™” ë„êµ¬(ì˜ˆ: Selenium)ì— ì˜í•´ ì œì–´ë˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ê°ì§€í•˜ì§€ ì•Šë„ë¡ ë§Œë“­ë‹ˆë‹¤.
        # AutomationControlled ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ì—¬ webdriverê°€ ë¸Œë¼ìš°ì €ë¥¼ ìë™ìœ¼ë¡œ ì œì–´í•˜ëŠ” ê²ƒì„ ìˆ¨ê¹ë‹ˆë‹¤.
        # ì´ëŠ” ì¼ë¶€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìë™í™” ë„êµ¬ê°€ ê°ì§€ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.
        ###### ìë™ ì œì–´ ê°ì§€ ë°©ì§€ #####
        webdriver_options.add_argument('--disable-blink-features=AutomationControlled')

        # Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•  ë•Œ ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì €ë¥¼ ìµœëŒ€í™” ìƒíƒœë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        # ì´ ì˜µì…˜ì€ ì‚¬ìš©ìê°€ ë¸Œë¼ìš°ì €ë¥¼ ì²˜ìŒ ì‹¤í–‰í•  ë•Œ í¬ê¸°ê°€ ìë™ìœ¼ë¡œ ìµœëŒ€ë¡œ ì„¤ì •ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        ##### í™”ë©´ ìµœëŒ€ #####
        webdriver_options.add_argument("--start-maximized")

        # headless ëª¨ë“œë¡œ Chromeì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        # ì´ëŠ” í™”ë©´ì„ í‘œì‹œí•˜ì§€ ì•Šê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.
        # ë¸Œë¼ìš°ì € UI ì—†ì´ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ ì‚¬ìš©í•˜ë©°, ì„œë²„ í™˜ê²½ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.
        ##### í™”ë©´ì´ ì•ˆë³´ì´ê²Œ í•¨ #####
        # webdriver_options.add_argument("--headless")

        #ì´ ì„¤ì •ì€ Chromeì˜ ìë™í™” ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
        #ê¸°ë³¸ì ìœ¼ë¡œ Chromeì€ ìë™í™”ê°€ í™œì„±í™”ëœ ê²½ìš° ë¸Œë¼ìš°ì €ì˜ ì½˜ì†”ì— ê²½ê³  ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
        #ì´ ì˜µì…˜ì„ ì„¤ì •í•˜ë©´ ì´ëŸ¬í•œ ê²½ê³  ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ##### ìë™ ê²½ê³  ì œê±° #####
        webdriver_options.add_experimental_option('useAutomationExtension', False)

        # ì´ ì˜µì…˜ì€ ë¸Œë¼ìš°ì €ì˜ ë¡œê¹…ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
        # enable-loggingì„ ì œì™¸ì‹œí‚¤ë©´, Chromeì˜ ë¡œê¹… ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì§€ ì•Šì•„ ë¶ˆí•„ìš”í•œ ë¡œê·¸ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ##### ë¡œê¹… ë¹„í™œì„±í™” #####
        webdriver_options.add_experimental_option('excludeSwitches', ['enable-logging'])

        # ì´ ì˜µì…˜ì€ enable-automation ìŠ¤ìœ„ì¹˜ë¥¼ ì œì™¸ì‹œí‚µë‹ˆë‹¤.
        # enable-automation ìŠ¤ìœ„ì¹˜ê°€ í™œì„±í™”ë˜ë©´,
        # ìë™í™” ë„êµ¬ë¥¼ ì‚¬ìš© ì¤‘ì„ì„ ì•Œë¦¬ëŠ” ë©”ì‹œì§€ê°€ ë¸Œë¼ìš°ì €ì— í‘œì‹œë©ë‹ˆë‹¤.
        # ì´ë¥¼ ì œì™¸í•˜ë©´ ìë™í™” ë„êµ¬ì˜ ì‚¬ìš©ì´ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ##### ìë™í™” ë„êµ¬ ì‚¬ìš© ê°ì§€ ì œê±° #####
        webdriver_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.driver = webdriver.Chrome(options=webdriver_options)
        self.driver.get(self.baseUrl)

    # ì „ì²´ ê°¯ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    def get_total_count(self, url):
        self.driver.get(url)
        time.sleep(3)
        count_element = self.driver.find_element(By.CSS_SELECTOR, "span.result_count")
        total_cnt = int(re.sub(r'[^0-9]', '', count_element.text)) if count_element else 0
        return total_cnt

    # ì „ì²´ ê°¯ìˆ˜ ì¡°íšŒ
    def total_cnt_cal(self):
        check_obj_list = []
        for index, checked_obj in enumerate(self.checked_list, start=1):
            name = checked_obj['name']
            url = self.get_url(name)

            total_cnt = self.get_total_count(url)
            total_pages = (total_cnt // 48) + (1 if total_cnt % 48 > 0 else 0)

            checked_obj['url'] = url
            checked_obj['total_page_cnt'] = total_pages
            checked_obj['total_product_cnt'] = total_cnt
            check_obj_list.append(checked_obj)
            time.sleep(0.5)

        self.log_signal.emit(f"check_obj_list : {check_obj_list}")

        return check_obj_list


    def get_product_details(self, product_url):
        self.driver.get(product_url)

        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(3)

        # í˜„ì¬ í˜ì´ì§€ì˜ HTMLì„ ê°€ì ¸ì™€ì„œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')

        product_details = {}

        # ğŸ”¹ ì œí’ˆëª… ì¶”ì¶œ
        product_title_tag = soup.select_one("h1.product-title")
        product_details['product_title'] = product_title_tag.text.strip() if product_title_tag else ""

        # ğŸ”¹ ì„œë¸Œ ì œí’ˆëª… ì¶”ì¶œ
        product_sub_title_tag = soup.select_one("div.sub-product-title a")
        product_details['product_sub_title'] = product_sub_title_tag.text.strip() if product_sub_title_tag else ""

        # ğŸ”¹ See More ë²„íŠ¼ í´ë¦­ (ì…€ë ˆë‹ˆì›€ ì‚¬ìš©)
        try:
            see_more_button = self.driver.find_element(By.CSS_SELECTOR, ".seemoreParentDiv button")
            self.driver.execute_script("arguments[0].click();", see_more_button)
            time.sleep(2)  # í˜ì´ì§€ ê°±ì‹  ëŒ€ê¸° í›„ ë‹¤ì‹œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        except Exception as e:
            self.log_signal.emit(f"See More ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")

        # ğŸ”¹ FEATURES ëª©ë¡ ì¶”ì¶œ
        product_features = []
        features_section = soup.find("p", text="FEATURES")
        if features_section:
            ul = features_section.find_next_sibling("ul")
            if ul:
                product_features = [li.text.strip() for li in ul.find_all("li")]

        if not product_features:
            features_section = soup.find("p", text="PRODUCT FEATURES")
            if features_section:
                ul = features_section.find_next_sibling("ul")
                if ul:
                    product_features = [li.text.strip() for li in ul.find_all("li")]

        product_details['product_features'] = product_features

        # ğŸ”¹ FABRIC & CARE ëª©ë¡ ì¶”ì¶œ
        fabric_care = []
        fabric_care_section = soup.find("p", text="FABRIC & CARE")
        if fabric_care_section:
            ul = fabric_care_section.find_next_sibling("ul")
            if ul:
                fabric_care = [li.text.strip() for li in ul.find_all("li")]

        product_details['product_fabric_care'] = fabric_care

        # ğŸ”¹ ëŒ€í‘œ ì´ë¯¸ì§€ (ê³ í•´ìƒë„ srcsetì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°)
        product_details['product_img_1'] = ""
        product_details['product_img_2'] = ""
        product_details['product_img_3'] = ""
        product_details['product_img_4'] = ""

        main_image = soup.select_one(".pdp-large-hero-image img")
        if main_image:
            srcset = main_image.get("srcset")
            if srcset:
                first_img = srcset.split(",")[0].strip().split(" ")[0]  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ
                product_details['product_img_1'] = first_img
            else:
                product_details['product_img_1'] = main_image.get("src", "")

        # ğŸ”¹ ì¶”ê°€ ì´ë¯¸ì§€ (ìµœëŒ€ 3ê°œ)
        image_elements = soup.select(".pdp-large-alt-images .large-alt-image:not(.video) img")
        for idx, img in enumerate(image_elements[:3]):
            product_details[f'product_img_{idx + 2}'] = img.get("src", "")

        return product_details

    # ìƒì„¸ì •ë³´
    def _get_product_details(self, product_url):
        self.driver.get(product_url)

        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(3)

        product_details = {}

        # ğŸ”¹ ì œí’ˆëª… ì¶”ì¶œ
        try:
            product_details['product_title'] = self.driver.find_element(
                By.CSS_SELECTOR, "h1.product-title"
            ).text.strip()
        except:
            product_details['product_title'] = ""

        # ğŸ”¹ ì„œë¸Œ ì œí’ˆëª… ì¶”ì¶œ
        try:
            product_details['product_sub_title'] = self.driver.find_element(
                By.CSS_SELECTOR, "div.sub-product-title a"
            ).text.strip()
        except:
            product_details['product_sub_title'] = ""

        try:
            see_more_button = self.driver.find_element(By.CSS_SELECTOR, ".seemoreParentDiv button")
            self.driver.execute_script("arguments[0].click();", see_more_button)
            time.sleep(2)  # í˜ì´ì§€ê°€ ê°±ì‹ ë  ì‹œê°„ì„ í™•ë³´
        except Exception as e:
            self.log_signal.emit(f"See More ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")

        # ğŸ”¹ FEATURES ëª©ë¡ ì¶”ì¶œ
        try:
            features_section = self.driver.find_element(By.XPATH, "//p[text()='FEATURES']/following-sibling::ul")
            product_details['product_features'] = [li.text.strip() for li in features_section.find_elements(By.TAG_NAME, "li")]
        except:
            product_details['product_features'] = []

        # ğŸ”¹ FABRIC & CARE ëª©ë¡ ì¶”ì¶œ
        try:
            fabric_care_section = self.driver.find_element(By.XPATH, "//p[text()='FABRIC & CARE']/following-sibling::ul")
            product_details['product_fabric_care'] = [li.text.strip() for li in fabric_care_section.find_elements(By.TAG_NAME, "li")]
        except:
            product_details['product_fabric_care'] = []

        # ğŸ”¹ ëŒ€í‘œ ì´ë¯¸ì§€ (ê³ í•´ìƒë„ srcsetì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°)
        try:
            main_image = self.driver.find_element(By.CSS_SELECTOR, ".pdp-large-hero-image img")
            srcset = main_image.get_attribute("srcset")

            if srcset:
                # srcsetì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì„ íƒ
                first_img = srcset.split(",")[0].strip().split(" ")[0]  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ URLë§Œ ì¶”ì¶œ
                product_details['product_img_1'] = first_img
            else:
                # srcsetì´ ì—†ìœ¼ë©´ ì¼ë°˜ src ì‚¬ìš©
                product_details['product_img_1'] = main_image.get_attribute("src")

        except Exception as e:
            self.log_signal.emit(f"ëŒ€í‘œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            product_details['product_img_1'] = ""

        # ğŸ”¹ ì¶”ê°€ ì´ë¯¸ì§€ (ìµœëŒ€ 3ê°œ)
        product_details['product_img_2'] = ""
        product_details['product_img_3'] = ""
        product_details['product_img_4'] = ""

        try:
            image_elements = self.driver.find_elements(By.CSS_SELECTOR, ".pdp-large-alt-images .large-alt-image:not(.video) img")
            for idx, img in enumerate(image_elements[:3]):
                product_details[f'product_img_{idx + 2}'] = img.get_attribute("src")
        except:
            pass

        return product_details


    def load_products(self):
        if not os.path.exists(file_path):
            self.log_signal.emit(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return []

        df = pd.read_csv(file_path, dtype=str)  # ëª¨ë“  ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ì½ìŒ
        products = []
        for _, row in df.iterrows():
            product = {
                'product_id': row.get('product_id', ''),
                'product_name': row.get('product_name', ''),
                'product_url': row.get('product_url', ''),
                'product_title': row.get('product_title', ''),
                'product_sub_title': row.get('product_sub_title', ''),
                'product_features': row.get('product_features', '').split('|'),  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
                'product_fabric_care': row.get('product_fabric_care', '').split('|'),  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜
                'product_img_1': row.get('product_img_1', ''),
                'product_img_2': row.get('product_img_2', ''),
                'product_img_3': row.get('product_img_3', ''),
                'product_img_4': row.get('product_img_4', ''),
                'data_success': row.get('data_success', 'N'),  # ì„±ê³µ ì—¬ë¶€ ì¶”ê°€
                'img_success': row.get('img_success', 'N'),  # ì„±ê³µ ì—¬ë¶€ ì¶”ê°€
                'img_path': row.get('img_path', []),  # ì„±ê³µ ì—¬ë¶€ ì¶”ê°€
                'success': row.get('success', 'N'),  # ì„±ê³µ ì—¬ë¶€ ì¶”ê°€
                'error': row.get('error', '')  # ì„±ê³µ ì—¬ë¶€ ì¶”ê°€
            }
            products.append(product)

        return products


    def skip_products(self, new_product_id):
        for product in self.csv_product_list:
            if product['product_id'] == new_product_id:
                return product['data_success'] == 'Y'
        return False  # ê¸°ë³¸ì ìœ¼ë¡œ Nìœ¼ë¡œ ê°„ì£¼


    def get_products_from_page(self, url, checked_model):
        self.driver.get(url)
        time.sleep(3)  # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ì‹œê°„ì„ í™•ë³´

        # ğŸ”¹ í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤ ë‹¤ìš´ (Lazy Loading ì²˜ë¦¬)
        self.scroll_to_bottom()
        time.sleep(2)  # ìŠ¤í¬ë¡¤ ì´í›„ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # ğŸ”¹ í˜„ì¬ í˜ì´ì§€ì˜ HTMLì„ ê°€ì ¸ì™€ì„œ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        page_source = self.driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        # ğŸ”¹ ìƒí’ˆ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        product_elements = soup.select("#productsContainer > li")

        for product in product_elements:
            self.current_cnt += 1
            try:
                # ğŸ”¹ ì œí’ˆ ID ê°€ì ¸ì˜¤ê¸°
                product_id = product.get("data-id")
                product_element_id = product.get("id")

                skip = self.skip_products(product_id)

                if skip or not product_element_id or "scroll_id_" not in product_element_id:
                    continue

                # ğŸ”¹ ì œí’ˆëª… ê°€ì ¸ì˜¤ê¸°
                product_name = product.select_one(".products-container-right .prod_nameBlock p")
                product_name = product_name.text.strip() if product_name else ""

                # ğŸ”¹ ì œí’ˆ ìƒì„¸ í˜ì´ì§€ URL ê°€ì ¸ì˜¤ê¸°
                product_link_element = product.select_one(".prod_img_block > a")
                product_url = product_link_element["href"] if product_link_element else ""

                if product_url and not product_url.startswith("https://www.kohls.com"):
                    product_url = "https://www.kohls.com" + product_url

                # ğŸ”¹ ì œí’ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                product_details = self.get_product_details(product_url)

                product_data = {
                    "category_name": checked_model['name'],
                    "category_url": checked_model['url'],
                    "product_id": product_id,
                    "product_name": product_name,
                    "product_url": product_url,
                    **product_details
                }

                if all([
                    product_data['product_id'],
                    product_data['product_name'],
                    product_data['product_url'],
                    product_data['product_sub_title'],
                    product_data['product_features'],
                    product_data['product_img_1']
                ]):
                    product_data['data_success'] = 'Y'

                updated_product = self.download_images(product_data)
                self.append_to_csv([updated_product])
                self.log_signal.emit(f'{updated_product}')
                self.product_list.append(product_data)

                pro_value = (self.current_cnt / self.total_cnt) * 1000000
                self.progress_signal.emit(self.before_pro_value, pro_value)
                self.before_pro_value = pro_value
                self.log_signal.emit(f'{checked_model["name"]} TotalProduct({self.current_cnt}/{self.total_cnt})')

            except Exception as e:
                self.log_signal.emit(f"Error processing product: {e}")
                continue


    def scroll_to_bottom(self):
        """ í˜ì´ì§€ì˜ ëê¹Œì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì œí’ˆì„ ë¡œë”© """
        last_height = self.driver.execute_script("return document.body.scrollHeight")

        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # ë¡œë”© ëŒ€ê¸°
            new_height = self.driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                break
            last_height = new_height

    # URL ê°€ì ¸ì˜¤ê¸°
    def get_url(self, name):
        url = ""
        if name:
            if name == 'Women / Bottoms / Pants':
                url = "https://www.kohls.com/catalog/womens-pants-bottoms-clothing.jsp?CN=Gender:Womens+Product:Pants+Category:Bottoms+Department:Clothing&cc=wms-TN3.0-S-pants&kls_sbp=05864698454350754950882754362888169186"
            elif name == 'Women / Bottoms / Skirts & Skorts':
                url = "https://www.kohls.com/catalog/womens-skirts-skorts-bottoms-clothing.jsp?CN=Gender:Womens+Product:Skirts%20%26%20Skorts+Category:Bottoms+Department:Clothing&cc=wms-TN3.0-S-skirtsskorts&kls_sbp=05864698454350754950882754362888169186"
            elif name == 'Women / Bottoms / Shorts':
                url = "https://www.kohls.com/catalog/womens-shorts-bottoms-clothing.jsp?CN=Gender:Womens+Product:Shorts+Category:Bottoms+Department:Clothing&kls_sbp=05864698454350754950882754362888169186"
            elif name == 'Women / Dresses & Jumpsuits':
                url = "https://www.kohls.com/catalog/womens-dresses-clothing.jsp?CN=Gender:Womens+Category:Dresses+Department:Clothing&cc=wms-TN2.0-S-dressesjumpsuits&kls_sbp=05864698454350754950882754362888169186"
            elif name == 'Men / Mesâ€™s Tops / Button-Down Shirts':
                url = "https://www.kohls.com/catalog/mens-button-down-shirts-tops-clothing.jsp?CN=Gender:Mens+Silhouette:Button-Down%20Shirts+Category:Tops+Department:Clothing&cc=mens-TN3.0-S-buttondownshirts&kls_sbp=05864698454350754950882754362888169186"
            elif name == 'Men / Mesâ€™s Bottoms / Casual Pants':
                url = "https://www.kohls.com/catalog/mens-casual-pants-bottoms-clothing.jsp?CN=Gender:Mens+Occasion:Casual+Product:Pants+Category:Bottoms+Department:Clothing&cc=mens-TN3.0-S-casualpants&kls_sbp=05864698454350754950882754362888169186"
            elif name == 'Men / Mesâ€™s Bottoms / Shorts':
                url = "https://www.kohls.com/catalog/mens-shorts-bottoms-clothing.jsp?CN=Gender:Mens+Product:Shorts+Category:Bottoms+Department:Clothing&cc=mens-TN3.0-S-shorts&kls_sbp=05864698454350754950882754362888169186"
        return url







    def get_product_info_list(self, checked_model):
        result_list = []

        # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
        csv_filename = os.path.join(os.getcwd(), f"{checked_model['name']}_{get_current_yyyymmddhhmmss()}.csv")

        # CSV íŒŒì¼ ì´ˆê¸° ìƒì„±
        columns = ["name", "product", "product_id" , "product_no", "description", "image_no", "image_url", "image_name", "success", "reg_date", "page", "error"]
        df = pd.DataFrame(columns=columns)
        df.to_csv(csv_filename, index=False)

        for index, product in enumerate(self.product_info_list):

            if not product:  # productê°€ Noneì¸ì§€ í™•ì¸
                print(f"ê²½ê³ : index {index}ì˜ productê°€ Noneì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            time.sleep(1)
            obj = self.get_api_product_info(product.get('pid'), product.get('cid'))

            if obj:

                product['product'] = obj.get('product')
                product['description'] = obj.get('description')
                product['img_list'] = obj.get('img_list')
                product['product_id'] = product.get('pid')
                product['product_no'] = index + 1

                # images í´ë” ìƒì„±
                images_dir = os.path.join(os.getcwd(), 'images')
                os.makedirs(images_dir, exist_ok=True)

                for ix, image_url in enumerate(product.get('img_list'), start=1):
                    if not self.running:
                        break

                    obj_copy = product.copy()  # ê°ì²´ ë³µì‚¬
                    obj_copy['name'] = checked_model['name']
                    obj_copy['image_no'] = ix + 1
                    obj_copy['image_url'] = image_url
                    obj_copy['success'] = 'N'
                    obj_copy['image_yn'] = 'N'
                    obj_copy['reg_date'] = get_current_formatted_datetime()  # ì‹œê°„ ì¶”ê°€

                    try:
                        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                        # response = requests.get(image_url, stream=True)
                        # response.raise_for_status()

                        # ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
                        img_filename = f"{product.get('pid')}_{ix}.jpg"
                        # img_path = os.path.join(images_dir, img_filename)

                        # ì´ë¯¸ì§€ ì €ì¥
                        # with open(img_path, 'wb') as file:
                            # for chunk in response.iter_content(1024):
                                # file.write(chunk)

                        # obj_copy['success'] = 'Y'  # ì„±ê³µí•˜ë©´ Y
                        obj_copy['image_name'] = img_filename
                        self.log_signal.emit(f"ì„±ê³µ {obj_copy}")
                    except Exception as e:
                        print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url}, ì˜¤ë¥˜: {e}")
                        obj_copy['success'] = 'N'  # ì‹¤íŒ¨í•˜ë©´ N ìœ ì§€
                        obj_copy['error'] = e

                    result_list.append(obj_copy)

                self.current_cnt = self.current_cnt + 1
                pro_value = (self.current_cnt / self.total_cnt) * 1000000
                self.progress_signal.emit(self.before_pro_value, pro_value)
                self.before_pro_value = pro_value

                self.log_signal.emit(f'{checked_model["name"]} TotalPage({self.current_page}/{self.total_pages})  TotalProduct({self.current_cnt}/{self.total_cnt}) Product({index+1}/{len(self.product_info_list)})')

                # 5ê°œë§ˆë‹¤ CSVì— ì €ì¥
                if index % 5 == 0 and index > 0:
                    df = pd.DataFrame(result_list, columns=columns)
                    df.to_csv(csv_filename, mode='a', header=False, index=False)
                    result_list.clear()  # ì €ì¥ í›„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if result_list:
            df = pd.DataFrame(result_list, columns=columns)
            df.to_csv(csv_filename, mode='a', header=False, index=False)





