import mimetypes
import os
import ssl
import time
from io import BytesIO
import json
import re
import sqlite3

import pandas as pd
import requests
from PyQt5.QtCore import QThread, pyqtSignal
from bs4 import BeautifulSoup
from google.cloud import storage
from google.oauth2 import service_account
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

from src.dao.oldnavy.category_list_dao import CategoryListDAO
from src.dao.oldnavy.product_info_dao import ProductInfoDAO
from src.dao.oldnavy.main_dao import MainDAO

from src.model.oldnavy.main_model import MainModel
from src.model.oldnavy.category_list_model import CategoryListModel
from src.model.oldnavy.product_info_model import ProductInfoModel

from src.utils.time_utils import get_current_yyyymmddhhmmss, get_current_formatted_datetime
from src.utils.number_utils import divide_and_truncate_per
from requests.exceptions import RequestException, Timeout, TooManyRedirects
from urllib.parse import urlparse

from src.db.database import Database
from src.db.oldnavy_db import OldNavyDB  # OldNavy í…Œì´ë¸” ìƒì„± í´ë˜ìŠ¤
from src.dao.oldnavy.main_dao import MainDAO    # Main í…Œì´ë¸” DAO




from src.model.oldnavy.category_list_model import CategoryListModel


ssl._create_default_https_context = ssl._create_unverified_context

image_main_directory = 'zalando_images'
company_name = 'zalando'
site_name = 'ZALANDO'
excel_filename = ''
baseUrl = "https://oldnavy.gap.com/"


# API
class ApiOldnavySetLoadWorker(QThread):
    log_signal = pyqtSignal(str)         # ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_signal = pyqtSignal(float, float)  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì‹œê·¸ë„
    progress_end_signal = pyqtSignal()   # ì¢…ë£Œ ì‹œê·¸ë„


    # ì´ˆê¸°í™”
    def __init__(self, checked_list):
        super().__init__()
        self.baseUrl = baseUrl
        self.sess = requests.Session()
        self.checked_list = checked_list
        self.checked_model_list = []
        self.running = True  # ì‹¤í–‰ ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€
        self.driver = None
        self.db_path = "database.db"

        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
        self.db = Database("database.db")  # ê³µí†µ DB ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.oldnavy_db = OldNavyDB(self.db)  # í…Œì´ë¸” ìƒì„± í´ë˜ìŠ¤
        self.main_dao = MainDAO(self.db)  # MAIN DAO ê°ì²´
        self.product_info_dao = ProductInfoDAO(self.db)  # MAIN DAO ê°ì²´
        self.category_list_dao = CategoryListDAO(self.db)  # MAIN DAO ê°ì²´

        self.main_model = None



    # í”„ë¡œê·¸ë¨ ì‹¤í–‰
    def run(self):
        global image_main_directory, company_name, site_name, excel_filename, baseUrl

        self.oldnavy_db.create_tables()
        self.log_signal.emit("âœ… OldNavy í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        latest_main = self.main_dao.find_latest_main_entry()

        # ì¡°ê±´ í™•ì¸
        if latest_main and latest_main.completed_yn == 'N':
            self.log_signal.emit(f"ğŸŸ¡ ì§„í–‰ ì¤‘ì¸ ìµœì‹  ë°ì´í„° ë°œê²¬: {latest_main}")

        else:
            self.log_signal.emit("âœ… ìµœì‹  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.log_signal.emit("í¬ë¡¤ë§ ì‹œì‘")

            current_cnt = 0
            current_page = 0
            before_pro_value = 0
            result_list = []

            if self.checked_list:
                self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì„ ì‹œë„ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
                self.login()
                self.log_signal.emit("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì— ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.")
                current_time = get_current_formatted_datetime()

                self.log_signal.emit(f"ì „ì²´ ìƒí’ˆìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
                check_obj_list = self.total_cnt_cal()
                total_cnt = sum(int(obj['total_product_cnt']) for obj in check_obj_list)
                total_pages = sum(int(obj['total_page_cnt']) for obj in check_obj_list)

                self.log_signal.emit(f"ì „ì²´ í•­ëª©ìˆ˜ {len(check_obj_list)}ê°œ")
                self.log_signal.emit(f"ì „ì²´ ìƒí’ˆìˆ˜ {total_cnt} ê°œ")
                self.log_signal.emit(f"ì „ì²´ í˜ì´ì§€ìˆ˜ {total_pages} ê°œ")

                # main DB insert
                self.insert_main_model(check_obj_list, total_cnt, total_pages, current_time)

                # category_list DB insert
                self.insert_category_list_models(check_obj_list, current_time)

                # product_info DB insert
                self.insert_product_models_main()


            self.progress_signal.emit(before_pro_value, 1000000)
            self.log_signal.emit(f"=============== ì²˜ë¦¬ ë°ì´í„° ìˆ˜ : {len(result_list)}")
            self.log_signal.emit("=============== í¬ë¡¤ë§ ì¢…ë£Œ")
            self.progress_end_signal.emit()


    def insert_main_model(self, check_obj_list, total_cnt, total_pages, current_time):
        # insert_main_entry ë©”ì„œë“œ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ì‚½ì…
        new_entry = MainModel(
            no=None,  # Auto Increment í•„ë“œ
            now_category=check_obj_list[0].name,
            now_page_no=0,
            now_product_no=0,
            total_page_cnt=total_cnt,
            total_product_cnt=total_pages,
            completed_yn='N',
            update_date=current_time,
            reg_date=current_time,
            deleted_yn='N'
        )
        self.main_model = self.main_dao.insert_main_entry(new_entry)
        self.log_signal.emit(f"Inserted inserted_main_entry: {self.main_model}")


    def insert_category_list_models(self, check_obj_list, current_time):
        for index, check_obj in enumerate(check_obj_list, start=1):
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                self.log_signal.emit("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            name = check_obj['name']
            start_page = int(check_obj['start_page'])
            end_page = int(check_obj['end_page'])
            cid = self.get_cid(name)
            total_page_cnt = int(check_obj['total_page_cnt'])
            total_product_cnt = int(check_obj['total_product_cnt'])

            category = CategoryListModel(
                no=None,  # Auto Increment
                pno=self.main_model.no,
                cid=cid,
                category=name,
                input_start_page=start_page,
                input_end_page=end_page,
                real_start_page=start_page,
                real_end_page=total_page_cnt if end_page >= total_page_cnt else end_page,
                total_page_cnt=total_page_cnt,
                total_product_cnt=total_product_cnt,
                now_page_no=0,
                now_product_no=0,
                completed_yn='N',
                update_date=current_time,
                reg_date=current_time,
                deleted_yn='N'
            )
            inserted_category_entry = self.category_list_dao.insert_category(category)
            self.checked_model_list.append(inserted_category_entry)


    def insert_product_models_main(self):
        for index, checked_model in enumerate(self.checked_model_list, start=1):
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                self.log_signal.emit("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            all_detail_list = {}
            for indx, page in enumerate(range(int(checked_model['real_start_page']) - 1, int(checked_model['real_end_page'])), start=1):
                if not self.running:
                    break
                detail_list = self.get_api_request(checked_model['cid'], page)
                for pid in detail_list:
                    if pid not in all_detail_list:
                        all_detail_list[pid] = {
                            "page_no": page,
                            "pid": pid,
                            "product_no": None
                        }
            all_detail_list = list(all_detail_list.values())  # ë¦¬ìŠ¤íŠ¸ ë³€í™˜
            inserted_products = self.insert_product_models(checked_model, all_detail_list)










    def insert_product_models(self, checked_model, all_detail_list):

        # CID ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ProductInfoModel ë¦¬ìŠ¤íŠ¸ ìƒì„± (ë‚˜ë¨¸ì§€ í•„ë“œëŠ” ê³µë°±)
        product_models = [
            ProductInfoModel(
                no=None,         # Auto Increment
                pno=checked_model['no'],           # ê¸°ë³¸ê°’
                cid=checked_model['cid'],         # CID ìœ ì§€
                category=checked_model['name'],     # ê³µë°±
                pid=detail['pid'],
                product="",
                description="",
                page_no=detail['page'],
                product_no=detail_idx + 1,
                img_list="",
                success_yn="N",  # ê¸°ë³¸ê°’
                main_url=f"https://api.gap.com/commerce/search/products/v2/cc?cid{checked_model['cid']}",
                detail_url=f"https://oldnavy.gap.com/browse/product.do?cid{checked_model['cid']}&pid{detail['pid']}",
                error_message="",
                reg_date="0000-00-00",  # ê¸°ë³¸ê°’
                deleted_yn="N"  # ê¸°ë³¸ê°’
            )
            for detail_idx, detail in all_detail_list
        ]

        inserted_products = self.product_info_dao.insert_all(product_models)
        return inserted_products



    # í”„ë¡œê·¸ë¨ ì¤‘ë‹¨
    def stop(self):
        """ìŠ¤ë ˆë“œ ì¤‘ì§€ë¥¼ ìš”ì²­í•˜ëŠ” ë©”ì„œë“œ"""
        self.running = False


    # ë¡œê·¸ì¸ ì¿ í‚¤ê°€ì ¸ì˜¤ê¸°
    def login(self):
        webdriver_options = webdriver.ChromeOptions()

        # ì´ ì˜µì…˜ì€ Chromeì´ ìë™í™” ë„êµ¬(ì˜ˆ: Selenium)ì— ì˜í•´ ì œì–´ë˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ê°ì§€í•˜ì§€ ì•Šë„ë¡ ë§Œë“­ë‹ˆë‹¤.
        # AutomationControlled ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ì—¬ webdriverê°€ ë¸Œë¼ìš°ì €ë¥¼ ìë™ìœ¼ë¡œ ì œì–´í•˜ëŠ” ê²ƒì„ ìˆ¨ê¹ë‹ˆë‹¤.
        # ì´ëŠ” ì¼ë¶€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìë™í™” ë„êµ¬ê°€ ê°ì§€ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.
        ###### ìë™ ì œì–´ ê°ì§€ ë°©ì§€ #####
        webdriver_options.add_argument('--disable-blink-features=AutomationControlled')

        # Chrome ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•  ë•Œ ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì €ë¥¼ ìµœëŒ€í™” ìƒíƒœë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        # ì´ ì˜µì…˜ì€ ì‚¬ìš©ìê°€ ë¸Œë¼ìš°ì €ë¥¼ ì²˜ìŒ ì‹¤í–‰í•  ë•Œ í¬ê¸°ê°€ ìë™ìœ¼ë¡œ ìµœëŒ€ë¡œ ì„¤ì •ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        ##### í™”ë©´ ìµœëŒ€ #####
        webdriver_options.add_argument("--start-maximized")

        # headless ëª¨ë“œë¡œ Chromeì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        # ì´ëŠ” í™”ë©´ì„ í‘œì‹œí•˜ì§€ ì•Šê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.
        # ë¸Œë¼ìš°ì € UI ì—†ì´ ì‘ì—…ì„ ìˆ˜í–‰í•  ë•Œ ì‚¬ìš©í•˜ë©°, ì„œë²„ í™˜ê²½ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.
        ##### í™”ë©´ì´ ì•ˆë³´ì´ê²Œ í•¨ #####
        webdriver_options.add_argument("--headless")

        #ì´ ì„¤ì •ì€ Chromeì˜ ìë™í™” ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
        #ê¸°ë³¸ì ìœ¼ë¡œ Chromeì€ ìë™í™”ê°€ í™œì„±í™”ëœ ê²½ìš° ë¸Œë¼ìš°ì €ì˜ ì½˜ì†”ì— ê²½ê³  ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
        #ì´ ì˜µì…˜ì„ ì„¤ì •í•˜ë©´ ì´ëŸ¬í•œ ê²½ê³  ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ##### ìë™ ê²½ê³  ì œê±° #####
        webdriver_options.add_experimental_option('useAutomationExtension', False)

        # ì´ ì˜µì…˜ì€ ë¸Œë¼ìš°ì €ì˜ ë¡œê¹…ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
        # enable-loggingì„ ì œì™¸ì‹œí‚¤ë©´, Chromeì˜ ë¡œê¹… ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì§€ ì•Šì•„ ë¶ˆí•„ìš”í•œ ë¡œê·¸ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ##### ë¡œê¹… ë¹„í™œì„±í™” #####
        webdriver_options.add_experimental_option('excludeSwitches', ['enable-logging'])

        # ì´ ì˜µì…˜ì€ enable-automation ìŠ¤ìœ„ì¹˜ë¥¼ ì œì™¸ì‹œí‚µë‹ˆë‹¤.
        # enable-automation ìŠ¤ìœ„ì¹˜ê°€ í™œì„±í™”ë˜ë©´,
        # ìë™í™” ë„êµ¬ë¥¼ ì‚¬ìš© ì¤‘ì„ì„ ì•Œë¦¬ëŠ” ë©”ì‹œì§€ê°€ ë¸Œë¼ìš°ì €ì— í‘œì‹œë©ë‹ˆë‹¤.
        # ì´ë¥¼ ì œì™¸í•˜ë©´ ìë™í™” ë„êµ¬ì˜ ì‚¬ìš©ì´ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ##### ìë™í™” ë„êµ¬ ì‚¬ìš© ê°ì§€ ì œê±° #####
        webdriver_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.driver = webdriver.Chrome(options=webdriver_options)
        self.driver.set_page_load_timeout(120)
        self.driver.get(self.baseUrl)
        cookies = self.driver.get_cookies()
        for cookie in cookies:
            self.sess.cookies.set(cookie['name'], cookie['value'])
        self.driver.quit()


    def main_request(self, cid, pageNumber):

        url = "https://api.gap.com/commerce/search/products/v2/cc"

        headers = {
            "accept": "*/*",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "origin": "https://oldnavy.gap.com",
            "referer": "https://oldnavy.gap.com/",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-site",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
            "x-client-application-name": "Browse"
        }

        params = {
            "brand": "on",
            "market": "us",
            "cid": cid,
            "locale": "en_US",
            "pageSize": "300",
            "ignoreInventory": "false",
            "includeMarketingFlagsDetails": "true",
            "enableDynamicFacets": "true",
            "enableSwatchSort": "true",
            "sortSwatchesBy": "bestsellers",
            "pageNumber": pageNumber,
            "vendor": "Certona",
        }

        try:
            res = self.sess.get(url, params=params, headers=headers, timeout=10)

            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if res.status_code == 200:
                try:
                    response_json = res.json()  # JSON ì‘ë‹µ íŒŒì‹±
                    return {
                        "total_page_cnt": response_json.pageNumberTotal,
                        "total_product_cnt": response_json.totalColors
                    }
                except ValueError as e:
                    # JSON íŒŒì‹± ì‹¤íŒ¨
                    self.log_signal.emit(f"JSON íŒŒì‹± ì—ëŸ¬: {e}")
                    return None
            else:
                # ìƒíƒœ ì½”ë“œê°€ 200ì´ ì•„ë‹Œ ê²½ìš°
                self.log_signal.emit(f"HTTP ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {res.status_code}, ë‚´ìš©: {res.text}")
                return None

        except Exception as e:
            # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë˜ëŠ” ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            self.log_signal.emit(f"ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None


    # í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    def get_api_request(self, cid, pageNumber):

        url = "https://api.gap.com/commerce/search/products/v2/cc"

        headers = {
            "accept": "*/*",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "origin": "https://oldnavy.gap.com",
            "referer": "https://oldnavy.gap.com/",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-site",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36",
            "x-client-application-name": "Browse"
        }

        params = {
            "brand": "on",
            "market": "us",
            "cid": cid,
            "locale": "en_US",
            "pageSize": "300",
            "ignoreInventory": "false",
            "includeMarketingFlagsDetails": "true",
            "enableDynamicFacets": "true",
            "enableSwatchSort": "true",
            "sortSwatchesBy": "bestsellers",
            "pageNumber": pageNumber,
            "vendor": "Certona",
        }

        try:
            res = self.sess.get(url, params=params, headers=headers, timeout=10)

            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if res.status_code == 200:
                try:
                    response_json = res.json()  # JSON ì‘ë‹µ íŒŒì‹±
                    return [category.get("ccList", []) for category in response_json.get("categories", [])]
                except ValueError as e:
                    # JSON íŒŒì‹± ì‹¤íŒ¨
                    self.log_signal.emit(f"JSON íŒŒì‹± ì—ëŸ¬: {e}")
                    return None
            else:
                # ìƒíƒœ ì½”ë“œê°€ 200ì´ ì•„ë‹Œ ê²½ìš°
                self.log_signal.emit(f"HTTP ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {res.status_code}, ë‚´ìš©: {res.text}")
                return None

        except Exception as e:
            # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë˜ëŠ” ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            self.log_signal.emit(f"ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return None


    # ìƒì„¸ë³´ê¸° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    def get_detail_data(self, html):
        images = []
        brand_name = ''
        product_name = ''
        detail = []
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ [ì‹œì‘] ====================
            img_list = soup.find_all('li', class_='LiPgRT DlJ4rT S3xARh') if soup else []

            if len(img_list) < 1:
                self.log_signal.emit("Image list not found")

            images = set()
            for index, view in enumerate(img_list):
                img = view.find('img')

                if img:
                    img_url = img['src'] if 'src' in img.attrs else ''
                    images.add(img_url)

            images = list(images)
            images = images[:2]

            product_tag = soup.find('span', class_='EKabf7 R_QwOV')
            product_name = product_tag.get_text(strip=True) if product_tag else ''

            brand_tag = soup.find('span', class_='OBkCPz Z82GLX m3OCL3 HlZ_Tf _5Yd-hZ')
            brand_name = brand_tag.get_text(strip=True) if brand_tag else ''

            # <div> íƒœê·¸ ì¤‘ 'data-testid' ì†ì„±ì´ 'pdp-accordion-details'ì¸ ìš”ì†Œ ì°¾ê¸°
            accordion_details = soup.find('div', {'data-testid': 'pdp-accordion-details'})

            # <dl> íƒœê·¸ ì•ˆì˜ ëª¨ë“  <div>ë¥¼ ì°¾ì•„ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            dl_items = accordion_details.find_all('div', class_='qMOFyE') if accordion_details else []

            # í…ìŠ¤íŠ¸ë¥¼ ë‹´ì„ ë°°ì—´ ì´ˆê¸°í™”

            # <dl> ì•ˆì˜ ëª¨ë“  <div>ì—ì„œ <dt>ì™€ <dd> í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ë°°ì—´ì— ë‹´ê¸°
            for item in dl_items:
                dt = item.find('dt')  # <dt> ìš”ì†Œ
                dd = item.find('dd')  # <dd> ìš”ì†Œ
                if dt and dd:
                    # dtì™€ ddì˜ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“¤ê³ , ì´ë¥¼ ë°°ì—´ì— ì¶”ê°€
                    detail.append(f'{dt.get_text(strip=True)} {dd.get_text(strip=True)}')
        except Exception as e:
            self.log_signal.emit(f"Error in process_detail_data: {e}")
        finally:
            return images, brand_name, product_name, detail


    # URL ê°€ì ¸ì˜¤ê¸°
    def get_cid(self, item):
        cid = ""
        if item:
            name = item

            if name == 'Now Trending!':
                cid = "3028309"
            elif name == 'Activewear':
                cid = "3028158"
            elif name == 'Women':
                cid = "1185233"
            elif name == 'Men':
                cid = "1031099"
            elif name == 'Girls':
                cid = "1185229"
            elif name == 'Boys':
                cid = "1185232"
            elif name == 'Toddler':
                cid = "1185224"
            elif name == 'Baby':
                cid = "1185226"
            elif name == 'Maternity':
                cid = "1185228"
        return cid


    # ì¹´í…Œê³ ë¦¬ë³„ ì „ì²´ ê°œìˆ˜
    def process_total_data(self, html):
        total_cnt = 0
        total_page = 0
        try:
            soup = BeautifulSoup(html, 'html.parser')

            product_count_tag = soup.find('span', class_='voFjEy _2kjxJ6 m3OCL3 Yb63TQ lystZ1 m3OCL3') if soup else None

            if product_count_tag:
                total_cnt = re.sub(r'\D', '', product_count_tag.text)  # ìˆ«ìë§Œ ì¶”ì¶œ


            total_page_element = soup.find('span', class_='voFjEy _2kjxJ6 m3OCL3 HlZ_Tf jheIXc Gj7Swn') if soup else None

            # ìˆ«ì ì¶”ì¶œ
            if total_page_element:
                # "Page 2 of 428"ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ì ì¶”ì¶œ
                match = re.search(r'\b\d[\d,]*$', total_page_element.text.strip())
                if match:
                    # ì½¤ë§ˆ ì œê±°
                    total_page = match.group().replace(',', '')

        except Exception as e:
            self.log_signal.emit(f"Error : {e}")
        finally:
            return int(total_cnt), int(total_page)


    # ìƒì„¸ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    def process_data(self, html):
        data_list = []
        total_page = 0
        try:
            soup = BeautifulSoup(html, 'html.parser')
            board_list = soup.find_all('div', class_='_5qdMrS _75qWlu iOzucJ') if soup else None

            if not board_list:
                self.log_signal.emit("Board list not found")
                return []

            if len(board_list) > 0:

                for index, view in enumerate(board_list):
                    a_tag = view.find('a', class_='_LM tCiGa7 ZkIJC- JT3_zV CKDt_l CKDt_l LyRfpJ')

                    if a_tag:
                        href_text = a_tag['href'] if 'href' in a_tag.attrs else ''
                        data_list.append(href_text)

            total_page_element = soup.find('span', class_='voFjEy _2kjxJ6 m3OCL3 HlZ_Tf jheIXc Gj7Swn') if soup else None

            # ìˆ«ì ì¶”ì¶œ
            if total_page_element:
                # "Page 2 of 428"ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ì ì¶”ì¶œ
                match = re.search(r'\b\d[\d,]*$', total_page_element.text.strip())
                if match:
                    # ì½¤ë§ˆ ì œê±°
                    total_page = match.group().replace(',', '')
        except Exception as e:
            self.log_signal.emit(f"Error : {e}")
        finally:
            return data_list, total_page


    # ì „ì²´ ê°¯ìˆ˜ ì¡°íšŒ
    def total_cnt_cal(self):
        check_obj_list = []
        for index, checked_obj in enumerate(self.checked_list, start=1):
            name = checked_obj['name']

            cid = self.get_cid(name)
            cnt_result = self.main_request(cid, 0)

            checked_obj['cid'] = cid
            checked_obj['total_page_cnt'] = cnt_result['total_page_cnt']
            checked_obj['total_product_cnt'] = cnt_result['total_product_cnt']

            check_obj_list.append(checked_obj)

            time.sleep(1)

        return check_obj_list



    def sub_request(self, url, timeout=30):

        # HTTP ìš”ì²­ í—¤ë” ì„¤ì •
        headers = {
            'method': 'GET',
            'scheme': 'https',
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'accept-encoding': 'gzip, deflate, br, zstd',
            'accept-language': 'ko,en;q=0.9,en-US;q=0.8',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0',
        }

        try:

            # POST ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µ ë°›ê¸°
            # response = requests.get(url, headers=headers, params=payload, timeout=timeout)
            response = requests.get(url, headers=headers, timeout=timeout)

            # ì‘ë‹µì˜ ì¸ì½”ë”©ì„ UTF-8ë¡œ ì„¤ì •
            response.encoding = 'utf-8'

            # HTTP ìƒíƒœ ì½”ë“œê°€ 200ì´ ì•„ë‹Œ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            response.raise_for_status()

            # ìƒíƒœ ì½”ë“œê°€ 200ì¸ ê²½ìš° ì‘ë‹µ JSON ë°˜í™˜
            if response.status_code == 200:
                return response.text  # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ë°˜í™˜
            else:
                # ìƒíƒœ ì½”ë“œê°€ 200ì´ ì•„ë‹Œ ê²½ìš° ë¡œê·¸ ê¸°ë¡
                self.log_signal.emit(f"Unexpected status code: {response.status_code}")
                return None

        # íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ ì²˜ë¦¬
        except Timeout:
            self.log_signal.emit("Request timed out")
            return None

        # ë„ˆë¬´ ë§ì€ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë°œìƒ ì‹œ ì²˜ë¦¬
        except TooManyRedirects:
            self.log_signal.emit("Too many redirects")
            return None

        # ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜ ì²˜ë¦¬
        except ConnectionError:
            self.log_signal.emit("Network connection error")
            return None

        # ê¸°íƒ€ ëª¨ë“  ì˜ˆì™¸ ì²˜ë¦¬
        except RequestException as e:
            self.log_signal.emit(f"Request failed: {e}")
            return None

        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ì²˜ë¦¬
        except Exception as e:
            self.log_signal.emit(f"Unexpected exception: {e}")
            return None


    # ì—‘ì…€ í•œê»€ì”© ì €ì¥
    def save_to_excel_one_by_one(self, results, file_name, obj, sheet_name='Sheet1'):
        try:
            # ê²°ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not results:
                self.log_signal.emit("ê²°ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                obj['excel_save'] = 'X'
            else:
                # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if os.path.exists(file_name):
                    # íŒŒì¼ì´ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ì½ì–´ì˜¤ê¸°
                    df_existing = pd.read_excel(file_name, sheet_name=sheet_name, engine='openpyxl')

                    # ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                    df_new = pd.DataFrame(results)

                    # ê¸°ì¡´ ë°ì´í„°ì— ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
                    for index, row in df_new.iterrows():
                        # ê¸°ì¡´ DataFrameì— í•œ í–‰ì”© ì¶”ê°€í•˜ëŠ” ë¶€ë¶„
                        df_existing = pd.concat([df_existing, pd.DataFrame([row])], ignore_index=True)

                    # ì—‘ì…€ íŒŒì¼ì— ë§ë¶™ì´ê¸° (indexëŠ” ì œì™¸)
                    with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
                        df_existing.to_excel(writer, sheet_name=sheet_name, index=False)

                    self.log_signal.emit('ì—‘ì…€ ì¶”ê°€ ì„±ê³µ')

                else:
                    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                    df = pd.DataFrame(results)
                    with pd.ExcelWriter(file_name, engine='openpyxl') as writer:
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                        self.log_signal.emit('ì—‘ì…€ ì¶”ê°€ ì„±ê³µ')

                obj['excel_save'] = 'O'

        except Exception as e:
            # ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ì²˜ë¦¬
            self.log_signal.emit(f'ì—‘ì…€ ì—ëŸ¬ ë°œìƒ: {e}')
            obj['excel_save'] = 'X'
            obj['error_message'] = e


    # êµ¬ê¸€ í´ë¼ìš°ë“œ ì—…ë¡œë“œ
    def google_cloud_upload(self, site_name, category, product_name, image_url, obj):
        try:
            # í”„ë¡œê·¸ë¨ ì‹¤í–‰ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            base_path = os.getcwd()
            service_account_path = os.path.join(base_path, "styleai-ai-designer-ml-external.json")
            user_config_path = os.path.join(base_path, "user.json")

            # user.jsonì—ì„œ ì„¤ì • ê°’ ë¡œë“œ
            with open(user_config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)

            project_id = user_config.get("project_id")
            bucket_name = user_config.get("bucket")

            if not project_id or not bucket_name:
                raise ValueError("Invalid configuration in user.json. Check 'project_id' and 'bucket' fields.")

            # GCP í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ ì‚¬ìš©)
            credentials = service_account.Credentials.from_service_account_file(service_account_path)
            storage_client = storage.Client(credentials=credentials, project=project_id)
            bucket = storage_client.bucket(bucket_name)

            # ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ URLì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            response = requests.get(image_url)
            response.raise_for_status()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬

            # ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬
            image_data = BytesIO(response.content)

            # URL íŒŒì‹±
            parsed_url = urlparse(image_url)

            # pathì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ì¶œ
            image_name = parsed_url.path.split('/')[-1]

            # ì—…ë¡œë“œí•  ê²½ë¡œ ì„¤ì •: site_name/category/product_name/media_...
            # blob_name = f"test_program_20250117/{site_name}/{category}_{image_name}"
            blob_name = f"{site_name}/{category}/{category}_{image_name}"

            # ì´ë¯¸ì§€ì˜ MIME íƒ€ì…ì„ ìë™ìœ¼ë¡œ ê°ì§€
            mime_type, _ = mimetypes.guess_type(image_url)
            if not mime_type:
                mime_type = "application/octet-stream"  # MIME íƒ€ì…ì„ ê°ì§€í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •

            # Cloud Storageì— ì´ë¯¸ì§€ ì—…ë¡œë“œ
            blob = bucket.blob(blob_name)
            blob.upload_from_file(image_data, content_type=mime_type)

            # ì´ë¯¸ì§€ ì—…ë¡œë“œ í™•ì¸
            if blob.exists():  # ì—…ë¡œë“œ í™•ì¸
                self.log_signal.emit(f"success {image_url} -> {bucket_name}/{blob_name}.")
                obj['image_name'] = f"{category}_{image_name}"
            else:
                obj['error_message'] = f"Image upload failed for {image_url}. Check the destination bucket."
                obj['image_success'] = 'X'
                self.log_signal.emit(f"Image upload failed for {image_url}. Check the destination bucket.")

        except requests.RequestException as e:
            self.log_signal.emit(f"Error downloading image from {image_url}: {str(e)}")
            obj['error_message'] = f"Error downloading image from {image_url}: {str(e)}"
            obj['image_success'] = 'X'
        except json.JSONDecodeError:
            self.log_signal.emit("Error reading or parsing user.json. Check its content.")
            obj['error_message'] = "Error reading or parsing user.json. Check its content."
            obj['image_success'] = 'X'
        except FileNotFoundError as e:
            self.log_signal.emit(f"File not found: {str(e)}")
            obj['error_message'] = f"File not found: {str(e)}"
            obj['image_success'] = 'X'
        except ValueError as e:
            self.log_signal.emit(str(e))
            obj['error_message'] = f"{str(e)}"
            obj['image_success'] = 'X'
        except Exception as e:
            self.log_signal.emit(f"An unexpected error occurred: {str(e)}")
            obj['error_message'] = f"An unexpected error occurred: {str(e)}"
            obj['image_success'] = 'X'


        # í•´ë‹¹ ê²½ë¡œì— ìˆëŠ” ëª¨ë“  ì´ë¯¸ì§€ ëª©ë¡ ì¶œë ¥ (site_name/category/product_name/ ê²½ë¡œ)
        # blobs = list(bucket.list_blobs(prefix=f"{site_name}"))  # ê²½ë¡œ ë‚´ì˜ ëª¨ë“  íŒŒì¼ ë‚˜ì—´
        # if any(blob.name == blob_name for blob in blobs):
        #     self.log_signal.emit(f"ì—…ë¡œë“œ ì™„ë£Œ: {blob_name}")
        # else:
        #     self.log_signal.emit(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {blob_name}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


    # ì´ë¯¸ì§€ ë¡œì»¬ ë‹¤ìš´ë¡œë“œ
    def download_image(self, image_url, site_name, category, product_name, obj):
        global image_main_directory
        local_file_path = ''

        try:
            # ì´ë¯¸ì§€ ì´ë¦„ ë³€ê²½: URLì—ì„œ 'media/...' ë¶€ë¶„ì„ 'media_'ë¡œ ë³€ê²½
            # image_name = image_url.split("media/")[-1].replace("/", "_")  # 'media/...'ë¥¼ 'media_...'ë¡œ ë³€ê²½
            image_name = image_url.split("/")[-1]
            obj['image_name'] = image_name
            # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
            local_directory = os.getcwd()  # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬

            # 'metastyle_images' í´ë”ë¥¼ ìµœìƒìœ„ë¡œ ì„¤ì •
            image_directory = os.path.join(local_directory, image_main_directory)

            # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •: site_name/category/product_name/media_...
            local_file_path = os.path.join(image_directory, site_name, category, product_name, image_name)

            # ë¡œì»¬ ë””ë ‰í† ë¦¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
            if not os.path.exists(os.path.dirname(local_file_path)):
                os.makedirs(os.path.dirname(local_file_path))

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            response = requests.get(image_url)
            response.raise_for_status()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬ (ì˜ˆ: 404, 500 ë“±)

            # ë¡œì»¬ì— ì´ë¯¸ì§€ ì €ì¥
            with open(local_file_path, 'wb') as f:
                f.write(response.content)

        except requests.exceptions.MissingSchema:
            obj['error_message'] = f"Error: Invalid URL {image_url}. The URL format seems incorrect."
            obj['image_success'] = 'X'
        except requests.exceptions.RequestException as e:
            obj['error_message'] = f"Error downloading the image from {image_url}: {e}"
            obj['image_success'] = 'X'
        except OSError as e:
            obj['error_message'] = f"Error saving the image to {local_file_path}: {e}"
            obj['image_success'] = 'X'
        except Exception as e:
            obj['error_message'] = f"Unexpected error: {e}"
            obj['image_success'] = 'X'







