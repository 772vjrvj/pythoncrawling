import os
import time
import pyautogui
import pyperclip
import urllib.parse
from bs4 import BeautifulSoup
import re
import pandas as pd

import os
import subprocess
from src.utils.time_utils import get_current_yyyymmddhhmmss
import random
# âœ… ì „ì—­ ë³€ìˆ˜ ì„ ì–¸
current_url = ""
keyword = ""
page = 1
last_page = 2
result_list = []
result_list_index = 0
folder_path = ""
columns = ["ìƒí’ˆëª…", "ìƒí˜¸ëª…","ì‚¬ì—…ì¥ì†Œì¬ì§€", "ì—°ë½ì²˜", "URL", "í‚¤ì›Œë“œ"]
excel_name = "ì¿ íŒ¡"
urls_list= []

def extract_last_page(soup):
    global last_page

    # âœ… class ì†ì„±ì´ 'Pagination_pagination__'ë¡œ ì‹œì‘í•˜ëŠ” ìš”ì†Œ ì°¾ê¸°
    pagination = soup.find('div', class_=re.compile(r'^Pagination_pagination__'))
    if not pagination:
        print("âŒ í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return last_page

    page_numbers = []
    for a_tag in pagination.find_all('a', attrs={'data-page': True}):
        title = a_tag.get('title', '')
        if title not in ['ì´ì „', 'ë‹¤ìŒ']:
            try:
                page_num = int(a_tag['data-page'])
                page_numbers.append(page_num)
            except ValueError:
                continue

    if page_numbers:
        last_page = max(page_numbers)
        print(f"âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œë¨: {last_page}")
    else:
        print("âŒ ìœ íš¨í•œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return last_page


def extract_product_urls(soup):
    base_url = "https://www.coupang.com"

    ul = soup.find('ul', id='product-list')
    if not ul:
        print("âŒ 'product-list' UL íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    urls = set()

    for li in ul.find_all('li', attrs={"data-sentry-component": "ProductItem"}):
        a_tag = li.find('a', href=True)
        if a_tag:
            href = a_tag['href']
            if not href.startswith("http"):
                href = base_url + href
            urls.add(href)

    url_list = sorted(list(urls))

    print(f"\nâœ… ì´ {len(url_list)}ê°œ ìƒí’ˆ URL ì¶”ì¶œë¨:\n")
    return url_list

def crawl_once():
    global current_url, keyword, page, last_page, folder_path, result_list_index, urls_list

    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    # í˜ì´ì§€ ë¡œë”© ë° HTML ì €ì¥
    pyautogui.moveTo(10, 10)
    pyautogui.click()
    time.sleep(0.5)

    pyautogui.hotkey('ctrl', 'l')
    time.sleep(0.3)
    pyautogui.hotkey('ctrl', 'c')
    time.sleep(0.3)

    # URL ê°€ì ¸ì˜¤ê¸°
    current_url = pyperclip.paste()
    print(f"ğŸ“‹ í˜„ì¬ URL: {current_url}")

    pyautogui.moveTo(300, 400)
    pyautogui.click()
    time.sleep(0.3)

    for _ in range(11):
        pyautogui.scroll(-1000)
        time.sleep(0.3)

    pyautogui.hotkey('ctrl', 'u')
    time.sleep(5)

    pyautogui.hotkey('ctrl', 'a')
    time.sleep(2)
    pyautogui.hotkey('ctrl', 'c')
    time.sleep(2)

    html_source = pyperclip.paste()
    print("HTML ê¸¸ì´:", len(html_source))
    print("ì‹œì‘ë¶€ë¶„:", html_source[:200])

    parsed = urllib.parse.urlparse(current_url)
    query = urllib.parse.parse_qs(parsed.query)
    keyword_encoded = query.get("q", [""])[0]
    page_str = query.get("page", ["1"])[0]

    keyword = urllib.parse.unquote(keyword_encoded)
    page = int(page_str)

    filename = f"ì¿ íŒ¡_{keyword}_{page}.html"
    save_path = os.path.join(folder_path, filename)

    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(html_source)

    pyautogui.hotkey('ctrl', 'w')
    time.sleep(0.5)



    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")

    with open(save_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    last_page = extract_last_page(soup)
    urls = extract_product_urls(soup)

    if urls_list and urls_list[-1] == urls:
        return False
    else:
        urls_list.append(urls)

    if os.path.exists(save_path):
        os.remove(save_path)
        print(f"ğŸ—‘ï¸ HTML íŒŒì¼ ì‚­ì œë¨: {save_path}")

    for i, url in enumerate(urls, start=1):
        if i == 2:
            break
        result_list_index += 1
        data_detail(i, url)

    return True


def data_detail(i, url):
    global result_list, keyword, page, folder_path, excel_name
    print(f'i : {i}, url : {url}')
    # âœ… ë¸Œë¼ìš°ì €ì— URL ì…ë ¥ í›„ ì´ë™
    pyautogui.hotkey('ctrl', 'l')
    time.sleep(0.3)
    pyperclip.copy(url)
    pyautogui.hotkey('ctrl', 'v')
    time.sleep(0.3)
    pyautogui.press('enter')
    time.sleep(3)

    # âœ… 1ë‹¨ê³„: ì•„ë˜ ë°©í–¥í‚¤ë¡œ 30ë²ˆ ë¹ ë¥´ê²Œ ìŠ¤í¬ë¡¤
    for _ in range(20):
        pyautogui.press('pagedown')
        time.sleep(0.3)  # ì‚´ì§ ë¹ ë¥´ê²Œ, ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤

    # âœ… 2ë‹¨ê³„: ë§ˆì§€ë§‰ì— ìŠ¤í¬ë¡¤ ëê¹Œì§€ ë‚´ë¦¬ê¸°
    for _ in range(3):
        pyautogui.press('end')
        time.sleep(0.3)  # ë¡œë”© ëŒ€ê¸° ì‹œê°„

    # âœ… HTML ë³µì‚¬
    pyautogui.hotkey('ctrl', 'u')
    time.sleep(random.uniform(4.5, 5.5))
    pyautogui.hotkey('ctrl', 'a')
    time.sleep(random.uniform(1, 2))
    pyautogui.hotkey('ctrl', 'c')
    time.sleep(random.uniform(2, 3))

    x = random.randint(100, 500)
    y = random.randint(100, 500)
    pyautogui.moveTo(x, y, duration=0.5)

    pyautogui.hotkey('ctrl', 'w')
    time.sleep(random.uniform(0.5, 1))

    html_source = pyperclip.paste()
    print("HTML ê¸¸ì´:", len(html_source))
    print("ì‹œì‘ë¶€ë¶„:", html_source[:200])

    # âœ… íŒŒì¼ ì €ì¥
    filename = f"ì¿ íŒ¡_{keyword}_{page}_{i}.html"
    save_path = os.path.join(folder_path, filename)

    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(html_source)

    # âœ… íŒë§¤ì ì •ë³´ ì¶”ì¶œ
    soup = BeautifulSoup(html_source, 'html.parser')

    seller_info = {
        "ìƒí’ˆëª…": "",
        "ìƒí˜¸ëª…": "",
        "ì‚¬ì—…ì¥ì†Œì¬ì§€": "",
        "ì—°ë½ì²˜": "",
        "URL": url,
        "í‚¤ì›Œë“œ": keyword
    }

    # âœ… ìƒí’ˆëª… ì¶”ì¶œ
    title_tag = soup.find("h1", attrs={"data-sentry-component": "ProductTitle"})
    if title_tag:
        seller_info["ìƒí’ˆëª…"] = title_tag.get_text(strip=True)
    else:
        print("âŒ ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # âœ… íŒë§¤ì ì •ë³´ í…Œì´ë¸” ì¶”ì¶œ
    container = soup.find("div", class_="product-item__table product-seller")
    if container:
        table = container.find("table", class_=re.compile(r"prod-delivery-return-policy-table"))
        if table:
            rows = table.find_all("tr")
            for row in rows:
                ths = row.find_all("th")
                tds = row.find_all("td")

                for i in range(min(len(ths), len(tds))):
                    label = ths[i].get_text(strip=True)
                    value = tds[i].get_text(strip=True)

                    if "ìƒí˜¸" in label:
                        seller_info["ìƒí˜¸ëª…"] = value
                    elif "ì†Œì¬ì§€" in label:
                        seller_info["ì‚¬ì—…ì¥ì†Œì¬ì§€"] = value
                    elif "ì—°ë½ì²˜" in label:
                        seller_info["ì—°ë½ì²˜"] = value

    print(f'{get_current_yyyymmddhhmmss()} ì—°ë½ì²˜ : {seller_info["ì—°ë½ì²˜"]}')
    print(f'{get_current_yyyymmddhhmmss()} ìƒí’ˆëª… : {seller_info["ìƒí’ˆëª…"]}')
    print(f'{get_current_yyyymmddhhmmss()} ìƒí˜¸ëª… : {seller_info["ìƒí˜¸ëª…"]}')
    print(f'{get_current_yyyymmddhhmmss()} ì‚¬ì—…ì¥ì†Œì¬ì§€ : {seller_info["ì‚¬ì—…ì¥ì†Œì¬ì§€"]}')



    # âœ… ì¤‘ë³µ ì²´í¬ í›„ ì¶”ê°€
    result_list.append(seller_info)

    if os.path.exists(save_path):
        os.remove(save_path)
        print(f"ğŸ—‘ï¸ HTML íŒŒì¼ ì‚­ì œë¨: {save_path}")

    if result_list_index % 5 == 0:
        df = pd.DataFrame(result_list, columns=columns)
        if not os.path.exists(excel_name):
            df.to_csv(excel_name, mode='a', header=True, index=False, encoding="utf-8-sig")
        else:
            df.to_csv(excel_name, mode='a', header=False, index=False, encoding="utf-8-sig")
        result_list.clear()

    time.sleep(random.uniform(5, 7))



def main():
    global page, current_url, last_page, folder_path, excel_name
    folder_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "html")
    excel_name = f"ì¿ íŒ¡_{get_current_yyyymmddhhmmss()}.csv"


    print("â–¶ ì‚¬ìš©ìì—ê²Œ ì¿ íŒ¡ ë¸Œë¼ìš°ì € ë¡œê·¸ì¸ í›„ í™•ì¸ ëŒ€ê¸°...")
    input("âœ… ì¿ íŒ¡ ë¡œê·¸ì¸ í›„ í™”ë©´ ìµœëŒ€í™” + ê²€ìƒ‰ ì™„ë£Œ â†’ Enter í‚¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")

    try:
        # âœ… ì²« í˜ì´ì§€ í¬ë¡¤ë§
        print(f"â–¶ í˜ì´ì§€ {page} ì§„í–‰ ============================================")
        crawl_once()

        # âœ… ë‹¤ìŒ í˜ì´ì§€ë¶€í„° ìë™ ë°˜ë³µ
        while True:
            page += 1

            if page > last_page:
                print(f"âœ…page : {page}")
                print(f"âœ…last_page : {last_page}")

            # âœ… current_urlì˜ page ê°’ ìˆ˜ì •
            parsed = urllib.parse.urlparse(current_url)
            query = urllib.parse.parse_qs(parsed.query)
            query['page'] = [str(page)]

            new_query = urllib.parse.urlencode(query, doseq=True)
            current_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{new_query}"
            print(f"\nğŸ” ë‹¤ìŒ í˜ì´ì§€ URL: {current_url}")

            if page % 3 == 0:
                # í¬ë¡¬ ê°•ì œ ì¢…ë£Œ
                os.system("taskkill /f /im chrome.exe")
                time.sleep(random.uniform(1200, 1320))  # ì¢…ë£Œ ëŒ€ê¸°

                # í¬ë¡¬ ì‹¤í–‰ (ì‚¬ìš©ì í”„ë¡œí•„ ìœ ì§€)
                chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"

                subprocess.Popen([chrome_path, current_url])
                time.sleep(2)  # ì¿ íŒ¡ ë¡œë”© ëŒ€ê¸°

            # âœ… ë¸Œë¼ìš°ì € ìë™ ì´ë™
            pyautogui.hotkey('ctrl', 'l')
            time.sleep(0.3)
            pyperclip.copy(current_url)
            pyautogui.hotkey('ctrl', 'v')
            time.sleep(0.3)
            pyautogui.press('enter')
            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

            print(f"â–¶ í˜ì´ì§€ {page} ì§„í–‰")
            rs = crawl_once()
            if not rs:
                break

        if result_list:
            df = pd.DataFrame(result_list, columns=columns)
            df.to_csv(excel_name, mode='a', header=False, index=False, encoding="utf-8-sig")
            result_list.clear()

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
