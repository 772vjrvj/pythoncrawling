import random
import ssl
import pyautogui  # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
import pygetwindow as gw
import time
import threading
import os
import pandas as pd
import pyperclip
import urllib.parse
import re
import subprocess

from PyQt5.QtCore import pyqtSignal
from bs4 import BeautifulSoup
from src.utils.file_utils import FileUtils
from src.workers.api_base_worker import BaseApiWorker

class ApiCoupangSetLoadWorker(BaseApiWorker):

    def __init__(self, setting):
        super().__init__()

        self.result_list = []
        self.current_cnt = 0
        self.current_total_cnt = 0
        self.urls_list = []
        self.keyword = ''
        self.current_url = ''
        self.before_pro_value = 0
        self.file_driver = None
        self.excel_driver = None
        self.running = True
        self.csv_filename = ""
        self.page = 1
        self.site_name = 'ì¿ íŒ¡'
        self.columns = ["ìƒí’ˆëª…", "ìƒí˜¸ëª…","ì‚¬ì—…ì¥ì†Œì¬ì§€", "ì—°ë½ì²˜", "URL", "PAGE", "í‚¤ì›Œë“œ"]
        self.base_url = "https://www.coupang.com"
        self.current_page = 0
        # âœ… ì„¤ì •ê°’ ì„¸íŒ…
        self.html_source_delay_time = self._get_setting_value(setting, "html_source_delay_time")
        self.chrome_delay_time = self._get_setting_value(setting, "chrome_delay_time")


    def _get_setting_value(self, setting_list, code_name):
        for item in setting_list:
            if item.get("code") == code_name:
                return item.get("value")
        return None  # ë˜ëŠ” ê¸°ë³¸ê°’ 0 ë“±



    def init(self):
        
        self.log_signal_func("ë“œë¼ì´ë²„ ì„¸íŒ… ========================================")
        self.log_signal_func(f"ì œí’ˆ ë”œë ˆì´ : {self.html_source_delay_time}")
        self.log_signal_func(f"í¬ë¡¬ ë”œë ˆì´ : {self.chrome_delay_time}")

        self.file_driver = FileUtils(self.log_signal_func)

        event = threading.Event()  # OK ë²„íŠ¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°í•  ì´ë²¤íŠ¸ ê°ì²´

        # ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì°½ ìš”ì²­
        self.msg_signal_func("ì¿ íŒ¡ ë¡œê·¸ì¸ -> í˜ì´ì§€ ê²€ìƒ‰ í›„ í™•ì¸ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”", "info", event)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.log_signal_func("ğŸ“¢ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...")
        event.wait()  # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥´ë©´ í•´ì œë¨

        # í˜„ì¬ í•´ìƒë„ ê°€ì ¸ì˜¤ê¸°
        screen_width, screen_height = pyautogui.size()

        # 'Chrome'ì´ í¬í•¨ëœ ì°½ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (visible ì†ì„± ì‚¬ìš©)
        chrome_windows = [w for w in gw.getWindowsWithTitle('Chrome') if w.visible]

        if chrome_windows:
            chrome_win = chrome_windows[0]  # ì²« ë²ˆì§¸ í¬ë¡¬ ì°½ ì„ íƒ
            chrome_win.moveTo(0, 0)
            chrome_win.resizeTo(screen_width // 2, screen_height)
            self.log_signal_func("âœ… í¬ë¡¬ ì°½ì„ ì™¼ìª½ ì ˆë°˜ìœ¼ë¡œ ì´ë™ì‹œì¼°ìŠµë‹ˆë‹¤.")
        else:
            self.log_signal_func("âŒ í¬ë¡¬ ì°½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        pyautogui.sleep(1)  # ì°½ í¬ì»¤ì‹± ëŒ€ê¸°
        pyautogui.press('home')  # ë˜ëŠ” ì•„ë˜ì²˜ëŸ¼ ë§ˆìš°ìŠ¤ íœ ë¡œë„ ê°€ëŠ¥


    # ë©”ì¸
    def main(self):

        try:
            self.csv_filename = self.file_driver.get_csv_filename(self.site_name)
            df = pd.DataFrame(columns=self.columns)
            df.to_csv(self.csv_filename, index=False, encoding="utf-8-sig")

            # âœ… ì²« í˜ì´ì§€ í¬ë¡¤ë§
            self.log_signal_func(f"ë©”ì¸ ì‹œì‘")
            self.log_signal_func(f"â–¶ í˜ì´ì§€ {self.page} ì§„í–‰")
            self.main_crawl()

            # âœ… ë‹¤ìŒ í˜ì´ì§€ë¶€í„° ìë™ ë°˜ë³µ
            while True:
                if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                    self.log_signal_func("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break

                self.log_signal_func(f'\n\n\n\n')
                self.log_signal_func(f'â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– ')
                self.page += 1

                # âœ… current_urlì˜ page ê°’ ìˆ˜ì •
                parsed = urllib.parse.urlparse(self.current_url)
                query = urllib.parse.parse_qs(parsed.query)

                keyword_encoded = query.get("q", [""])[0]
                self.keyword = urllib.parse.unquote(keyword_encoded)

                query['page'] = [str(self.page)]

                new_query = urllib.parse.urlencode(query, doseq=True)
                self.current_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{new_query}"

                # âœ… ë¸Œë¼ìš°ì € ìë™ ì´ë™
                pyautogui.hotkey('ctrl', 'l')
                time.sleep(0.3)
                pyperclip.copy(self.current_url)
                pyautogui.hotkey('ctrl', 'v')
                time.sleep(0.3)
                pyautogui.press('enter')
                time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

                self.log_signal_func(f"â–¶ í˜ì´ì§€ {self.page} ì§„í–‰")
                rs = self.main_crawl()
                if not rs:
                    break

                if self.page % 2 == 0:
                    self.log_signal_func(f"ê°ì§€ë´‡ ìš°íšŒ í¬ë¡¬ ê°•ì œì¢…ë£Œ")
                    # í¬ë¡¬ ê°•ì œ ì¢…ë£Œ
                    os.system("taskkill /f /im chrome.exe")
                    self.log_signal_func(f"í¬ë¡¬ ì¢…ë£Œ ëŒ€ê¸°ì¤‘ ì…ë‹ˆë‹¤. {self.chrome_delay_time}ì´ˆ í›„ì— ì—´ë¦½ë‹ˆë‹¤. ë‹¤ë¥¸ ì‘ì—…ì„ í•˜ì§€ë§ˆì„¸ìš”.")
                    self.request_chrome_delay()  # ì¹´ìš´íŠ¸ë‹¤ìš´ íŒì—… ìš”ì²­ + sleep
                    # í¬ë¡¬ ì‹¤í–‰ (ì‚¬ìš©ì í”„ë¡œí•„ ìœ ì§€)
                    chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
                    subprocess.Popen([chrome_path, self.current_url])
                    time.sleep(2)  # ì¿ íŒ¡ ë¡œë”© ëŒ€ê¸°
                    self.log_signal_func(f"í¬ë¡¬ ì‹œì‘")
                
            if self.result_list:
                df = pd.DataFrame(self.result_list, columns=self.columns)
                df.to_csv(self.csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")
                self.result_list.clear()

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


    # ë§ˆë¬´ë¦¬
    def destroy(self):
        self.progress_signal.emit(self.before_pro_value, 1000000)
        self.log_signal_func("=============== í¬ë¡¤ë§ ì¢…ë£Œì¤‘...")
        time.sleep(5)
        self.log_signal_func("=============== í¬ë¡¤ë§ ì¢…ë£Œ")
        self.progress_end_signal.emit()


    # ë©”ì¸ í¬ë¡¤ë§
    def main_crawl(self):

        # ì™¼ìª½ ëìœ¼ë¡œ ì´ë™
        pyautogui.moveTo(10, 10)
        # í´ë¦­
        pyautogui.click()
        time.sleep(0.5)

        # URL ì°½ í™œì„±í™”
        pyautogui.hotkey('ctrl', 'l')
        time.sleep(0.3)
        # URL ë³µì‚¬
        pyautogui.hotkey('ctrl', 'c')
        time.sleep(0.3)

        if self.page == 1:
            # URL ê°€ì ¸ì˜¤ê¸°
            self.current_url = pyperclip.paste()
            self.log_signal_func(f"ğŸ“‹ í˜„ì¬ URL í™•ì¸: {self.current_url}")

            parsed = urllib.parse.urlparse(self.current_url)
            query = urllib.parse.parse_qs(parsed.query)

            keyword_encoded = query.get("q", [""])[0]
            self.keyword = urllib.parse.unquote(keyword_encoded)

        # ìŠ¤í¬ë¡¤ì„ ìœ„í•´ ë‚´ë¶€ html ë§í¬ë‚˜ ë²„íŠ¼ ì—†ëŠ” ê³³ í´ë¦­
        pyautogui.moveTo(300, 400)
        pyautogui.click()
        time.sleep(0.3)

        soup = self.get_soup()

        urls = self.extract_product_urls(soup)

        self.current_total_cnt = len(urls)

        if self.urls_list and self.urls_list[-1] == urls:
            return False
        else:
            self.urls_list.append(urls)

        for i, url in enumerate(urls, start=0):
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                self.log_signal_func("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            self.current_cnt += 1
            self.log_signal_func(f'\n\n')
            self.log_signal_func(f'==================================================')
            self.log_signal_func(f'PAGE : {self.page} ({i+1}/{self.current_total_cnt})')
            self.log_signal_func(f'ëˆ„ì  ìƒí’ˆìˆ˜ : {self.current_cnt}')
            self.data_detail_crawl(i, url)
            self.log_signal_func(f'==================================================')

        return True


    # ë©”ì¸ í˜ì´ì§€ url ì–»ê¸°
    def extract_product_urls(self, soup):

        ul = soup.find('ul', id='product-list')
        if not ul:
            self.log_signal_func("âŒ 'product-list' UL íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        urls = set()

        for li in ul.find_all('li', attrs={"data-sentry-component": "ProductItem"}):
            a_tag = li.find('a', href=True)
            if a_tag:
                href = a_tag['href']
                if not href.startswith("http"):
                    href = self.base_url + href
                urls.add(href)

        url_list = sorted(list(urls))

        self.log_signal_func(f"âœ… ì´ {len(url_list)}ê°œ ìƒí’ˆ URL ì¶”ì¶œë¨")
        return url_list

    
    # ìƒì„¸ìƒí’ˆ ë°ì´í„° í¬ë¡¤ë§
    def data_detail_crawl(self, i, url):
        # âœ… ë¸Œë¼ìš°ì €ì— URL ì…ë ¥ í›„ ì´ë™
        pyautogui.hotkey('ctrl', 'l')
        time.sleep(0.3)
        pyperclip.copy(url)
        pyautogui.hotkey('ctrl', 'v')
        time.sleep(0.3)
        pyautogui.press('enter')
        time.sleep(3)

        soup = self.get_soup()

        seller_info = {
            "ìƒí’ˆëª…": "",
            "ìƒí˜¸ëª…": "",
            "ì‚¬ì—…ì¥ì†Œì¬ì§€": "",
            "ì—°ë½ì²˜": "",
            "URL": url,
            "PAGE": self.page,
            "í‚¤ì›Œë“œ": self.keyword
        }

        # âœ… ìƒí’ˆëª… ì¶”ì¶œ
        title_tag = soup.find("h1", attrs={"data-sentry-component": "ProductTitle"})
        if title_tag:
            seller_info["ìƒí’ˆëª…"] = title_tag.get_text(strip=True)
        else:
            self.log_signal_func("âŒ ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # âœ… íŒë§¤ì ì •ë³´ í…Œì´ë¸” ì¶”ì¶œ
        container = soup.find("div", class_="product-item__table product-seller")
        if container:
            table = container.find("table", class_=re.compile(r"prod-delivery-return-policy-table"))
            if table:
                rows = table.find_all("tr")
                for row in rows:
                    ths = row.find_all("th")
                    tds = row.find_all("td")

                    for i in range(min(len(ths), len(tds))):
                        label = ths[i].get_text(strip=True)
                        value = tds[i].get_text(strip=True)

                        if "ìƒí˜¸" in label:
                            seller_info["ìƒí˜¸ëª…"] = value
                        elif "ì†Œì¬ì§€" in label:
                            seller_info["ì‚¬ì—…ì¥ì†Œì¬ì§€"] = value
                        elif "ì—°ë½ì²˜" in label:
                            seller_info["ì—°ë½ì²˜"] = value



        self.log_signal_func(f'ì œí’ˆì •ë³´ : {seller_info}')
        self.log_signal_func(f'ì—°ë½ì²˜ :{seller_info['ì—°ë½ì²˜']}')

        # âœ… ì¤‘ë³µ ì²´í¬ í›„ ì¶”ê°€
        self.result_list.append(seller_info)

        if len(self.result_list) % 5 == 0:
            df = pd.DataFrame(self.result_list, columns=self.columns)
            if not os.path.exists(self.csv_filename):
                df.to_csv(self.csv_filename, mode='a', header=True, index=False, encoding="utf-8-sig")
            else:
                df.to_csv(self.csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")
            self.result_list.clear()

        time.sleep(random.uniform(5, 7))
        

    # soup ì–»ê¸°
    def get_soup(self):

        # âœ… 1ë‹¨ê³„: ì•„ë˜ ë°©í–¥í‚¤ë¡œ 20ë²ˆ ë¹ ë¥´ê²Œ ìŠ¤í¬ë¡¤
        for _ in range(20):
            pyautogui.press('pagedown')
            time.sleep(0.3)  # ì‚´ì§ ë¹ ë¥´ê²Œ, ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¡¤

        # âœ… 2ë‹¨ê³„: ë§ˆì§€ë§‰ì— ìŠ¤í¬ë¡¤ ëê¹Œì§€ ë‚´ë¦¬ê¸°
        for _ in range(3):
            pyautogui.press('end')
            time.sleep(0.3)  # ë¡œë”© ëŒ€ê¸° ì‹œê°„

        pyautogui.hotkey('ctrl', 'u')
        time.sleep(random.uniform(self.html_source_delay_time, self.html_source_delay_time + 2))
        pyautogui.hotkey('ctrl', 'a')
        time.sleep(2)
        pyautogui.hotkey('ctrl', 'c')
        time.sleep(2)
        pyautogui.hotkey('ctrl', 'w')
        time.sleep(0.5)

        html_source = pyperclip.paste()
        self.log_signal_func(f"HTML ê¸¸ì´: {len(html_source)}")
        self.log_signal_func(f"ì‹œì‘ë¶€ë¶„: {html_source[:200]}")

        soup = BeautifulSoup(html_source, 'html.parser')

        return soup


    # í¬ë¡¬ ë”œë ˆì´ ì¹´ìš´íŠ¸
    def request_chrome_delay(self):
        # ğŸ‘‰ UIì—ê²Œ ì¹´ìš´íŠ¸ë‹¤ìš´ íŒì—… ìš”ì²­
        self.show_countdown_signal_func(self.chrome_delay_time)

        # ğŸ‘‰ ì‹¤ì œ ëŒ€ê¸°ëŠ” workerê°€ ì§ì ‘ ì§„í–‰
        for remaining in range(self.chrome_delay_time, 0, -1):
            # self.log_signal_func(f"â³ ë‚¨ì€ ì‹œê°„: {remaining}ì´ˆ")
            time.sleep(1)


    # ì •ì§€    
    def stop(self):
        self.running = False

