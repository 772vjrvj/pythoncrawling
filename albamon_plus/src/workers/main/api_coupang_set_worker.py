import os
import random
import re
import subprocess
import threading
import time
import urllib.parse

import pandas as pd
import pyautogui  # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
import pygetwindow as gw
import pyperclip
from bs4 import BeautifulSoup

from src.utils.file_utils import FileUtils
from src.workers.api_base_worker import BaseApiWorker


class ApiCoupangSetLoadWorker(BaseApiWorker):

    def __init__(self, setting):
        super().__init__()

        self.current_detail_url = None
        self.result_list = []
        self.current_cnt = 0
        self.current_total_cnt = 0
        self.urls_list = []
        self.keyword = ''
        self.current_url = ''
        self.before_pro_value = 0
        self.file_driver = None
        self.excel_driver = None
        self.running = True
        self.csv_filename = ""
        self.page = 1
        self.site_name = 'ì¿ íŒ¡'
        self.columns = ["ìƒí’ˆëª…", "ìƒí˜¸ëª…","ì‚¬ì—…ì¥ì†Œì¬ì§€", "ì—°ë½ì²˜", "URL", "PAGE", "í‚¤ì›Œë“œ"]
        self.base_url = "https://www.coupang.com"
        self.current_page = 0
        # âœ… ì„¤ì •ê°’ ì„¸íŒ…
        self.html_source_delay_time = self.get_setting_value(setting, "html_source_delay_time")
        self.chrome_delay_time = self.get_setting_value(setting, "chrome_delay_time")
        self.st_cnt = 0
        self.ed_cnt = 0
        self.st_tm = None
        self.ed_tm = time.time()



    def init(self):
        
        self.log_signal_func("ë“œë¼ì´ë²„ ì„¸íŒ… ========================================")
        self.log_signal_func(f"ì œí’ˆ ë”œë ˆì´ : {self.html_source_delay_time}")
        self.log_signal_func(f"í¬ë¡¬ ë”œë ˆì´ : {self.chrome_delay_time}")

        self.file_driver = FileUtils(self.log_signal_func)

        event = threading.Event()  # OK ë²„íŠ¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°í•  ì´ë²¤íŠ¸ ê°ì²´

        # ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì°½ ìš”ì²­
        self.msg_signal_func("ì¿ íŒ¡ ë¡œê·¸ì¸ -> í˜ì´ì§€ ê²€ìƒ‰ í›„ í™•ì¸ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”", "info", event)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.log_signal_func("ğŸ“¢ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...")
        event.wait()  # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥´ë©´ í•´ì œë¨

        self.log_signal_func('ğŸ“¢ ë§ˆìš°ìŠ¤ì™€ í‚¤ë³´ë“œë¥¼ ì ˆëŒ€ ì¡°ì‘í•˜ì§€ë§ˆì„¸ìš”.')
        self.log_signal_func('ğŸ“¢ ì¡°ì‘í•˜ë©´ ì—ëŸ¬ê°€ ë‚©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ë‹¤ì‹œ ì§„í–‰í•´ì£¼ì„¸ìš”.')
        time.sleep(2)

        # í˜„ì¬ í•´ìƒë„ ê°€ì ¸ì˜¤ê¸°
        screen_width, screen_height = pyautogui.size()

        # 'Chrome'ì´ í¬í•¨ëœ ì°½ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (visible ì†ì„± ì‚¬ìš©)
        chrome_windows = [w for w in gw.getWindowsWithTitle('Chrome') if w.visible]

        if chrome_windows:
            chrome_win = chrome_windows[0]  # ì²« ë²ˆì§¸ í¬ë¡¬ ì°½ ì„ íƒ
            chrome_win.moveTo(0, 0)
            chrome_win.resizeTo(screen_width // 2, screen_height)
            self.log_signal_func("âœ… í¬ë¡¬ ì°½ì„ ì™¼ìª½ ì ˆë°˜ìœ¼ë¡œ ì´ë™ì‹œì¼°ìŠµë‹ˆë‹¤.")
        else:
            self.log_signal_func("âŒ í¬ë¡¬ ì°½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        pyautogui.sleep(1)  # ì°½ í¬ì»¤ì‹± ëŒ€ê¸°
        pyautogui.press('home')  # ë˜ëŠ” ì•„ë˜ì²˜ëŸ¼ ë§ˆìš°ìŠ¤ íœ ë¡œë„ ê°€ëŠ¥


    # ë©”ì¸
    def main(self):

        try:
            self.csv_filename = self.file_driver.get_csv_filename(self.site_name)
            df = pd.DataFrame(columns=self.columns)
            df.to_csv(self.csv_filename, index=False, encoding="utf-8-sig")

            # âœ… ì²« í˜ì´ì§€ í¬ë¡¤ë§
            self.log_signal_func(f"ë©”ì¸ ì‹œì‘")
            self.log_signal_func(f"â–¶ í˜ì´ì§€ {self.page} ì§„í–‰")
            self.main_crawl()
            # self.chrome_reset(name='main')

            # âœ… ë‹¤ìŒ í˜ì´ì§€ë¶€í„° ìë™ ë°˜ë³µ
            while True:
                if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                    self.log_signal_func("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break

                self.log_signal_func(f'\n\n\n\n')
                self.log_signal_func(f'â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– ')
                self.page += 1

                # âœ… current_urlì˜ page ê°’ ìˆ˜ì •
                parsed = urllib.parse.urlparse(self.current_url)
                query = urllib.parse.parse_qs(parsed.query)

                keyword_encoded = query.get("q", [""])[0]
                self.keyword = urllib.parse.unquote(keyword_encoded)

                query['page'] = [str(self.page)]

                new_query = urllib.parse.urlencode(query, doseq=True)
                self.current_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{new_query}"

                # âœ… ë¸Œë¼ìš°ì € ìë™ ì´ë™
                pyautogui.hotkey('ctrl', 'l')
                time.sleep(0.3)
                pyperclip.copy(self.current_url)
                pyautogui.hotkey('ctrl', 'v')
                time.sleep(0.3)
                pyautogui.press('enter')
                time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

                self.log_signal_func(f"â–¶ í˜ì´ì§€ {self.page} ì§„í–‰")
                rs = self.main_crawl()
                if not rs:
                    break

                if self.result_list:
                    df = pd.DataFrame(self.result_list, columns=self.columns)
                    df.to_csv(self.csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")
                    self.result_list.clear()

                # self.chrome_reset(name='main')

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


    # ë§ˆë¬´ë¦¬
    def destroy(self):
        self.progress_signal.emit(self.before_pro_value, 1000000)
        self.log_signal_func("=============== í¬ë¡¤ë§ ì¢…ë£Œì¤‘...")
        time.sleep(5)
        self.log_signal_func("=============== í¬ë¡¤ë§ ì¢…ë£Œ")
        self.progress_end_signal.emit()


    def get_setting_value(self, setting_list, code_name):
        for item in setting_list:
            if item.get("code") == code_name:
                return item.get("value")
        return None  # ë˜ëŠ” ê¸°ë³¸ê°’ 0 ë“±


    # ë©”ì¸ í¬ë¡¤ë§
    def main_crawl(self):
        # ì™¼ìª½ ëìœ¼ë¡œ ì´ë™
        pyautogui.moveTo(10, 10)
        # í´ë¦­
        pyautogui.click()
        time.sleep(0.5)

        # URL ì°½ í™œì„±í™”
        pyautogui.hotkey('ctrl', 'l')
        time.sleep(0.5)
        # URL ë³µì‚¬
        pyautogui.hotkey('ctrl', 'c')
        time.sleep(0.5)

        if self.page == 1:
            # URL ê°€ì ¸ì˜¤ê¸°
            self.current_url = pyperclip.paste()
            self.log_signal_func(f"ğŸ“‹ í˜„ì¬ URL í™•ì¸: {self.current_url}")

            parsed = urllib.parse.urlparse(self.current_url)
            query = urllib.parse.parse_qs(parsed.query)

            keyword_encoded = query.get("q", [""])[0]
            self.keyword = urllib.parse.unquote(keyword_encoded)

        # ìŠ¤í¬ë¡¤ì„ ìœ„í•´ ë‚´ë¶€ html ë§í¬ë‚˜ ë²„íŠ¼ ì—†ëŠ” ê³³ í´ë¦­
        pyautogui.moveTo(300, 400)
        pyautogui.click()
        time.sleep(0.3)

        soup = self.get_soup(name='main', retry=False)

        urls = self.extract_product_urls(soup)

        self.current_total_cnt = len(urls)

        if self.urls_list and self.urls_list[-1] == urls:
            return False
        else:
            self.urls_list.append(urls)

        for i, url in enumerate(urls, start=0):
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                self.log_signal_func("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            self.current_cnt += 1
            self.log_signal_func(f'\n\n')
            self.log_signal_func(f'==================================================')
            self.log_signal_func(f'PAGE : {self.page} ({i+1}/{self.current_total_cnt})')
            self.log_signal_func(f'ëˆ„ì  ìƒí’ˆìˆ˜ : {self.current_cnt}')
            self.current_detail_url = url
            self.data_detail_crawl()
            self.log_signal_func(f'==================================================')
            
        return True


    # ë©”ì¸ í˜ì´ì§€ url ì–»ê¸°
    def extract_product_urls(self, soup):

        ul = soup.find('ul', id='productList') or soup.find('ul', id='product-list')

        if not ul:
            self.log_signal_func("âŒ 'productList' ë˜ëŠ” 'product-list' UL íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        urls = set()
        lis = ul.find_all('li', attrs={"data-sentry-component": "ProductItem"}) or ul.find_all('li', class_="search-product")

        rocket_cnt =0
        for li in lis:
            # âœ… 'ë¡œì¼“ë°°ì†¡' ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ skip
            rocket_img = li.find('img', alt="ë¡œì¼“ë°°ì†¡")
            if rocket_img:
                # í´ë˜ìŠ¤ ì´ë¦„ì— "ProductUnit_productName"ì´ í¬í•¨ëœ div ì°¾ê¸°
                # name_div = li.find('div', class_=lambda c: c and "ProductUnit_productName" in c) or li.find('div', class_="name")
                # text = name_div.get_text(strip=True) if name_div else "(ìƒí’ˆëª… ì—†ìŒ)"
                # self.log_signal_func(f'ë¡œì¼“ ì œì™¸ ìƒí’ˆ : {text}')
                rocket_cnt += 1
                continue

            a_tag = li.find('a', href=True)
            if a_tag:
                href = a_tag['href']
                if not href.startswith("http"):
                    href = self.base_url + href
                urls.add(href)

        self.log_signal_func(f"âœ… ë¡œì¼“ìƒí’ˆ ì œì™¸ {rocket_cnt}ê°œ")
        url_list = sorted(list(urls))

        self.log_signal_func(f"âœ… ì´ {len(url_list)}ê°œ ìƒí’ˆ URL ì¶”ì¶œë¨")
        return url_list

    
    # ìƒì„¸ìƒí’ˆ ë°ì´í„° í¬ë¡¤ë§
    def data_detail_crawl(self):
        # âœ… ë¸Œë¼ìš°ì €ì— URL ì…ë ¥ í›„ ì´ë™
        pyautogui.hotkey('ctrl', 'l')
        time.sleep(0.3)
        pyperclip.copy(self.current_detail_url)
        pyautogui.hotkey('ctrl', 'v')
        time.sleep(0.3)
        pyautogui.press('enter')
        time.sleep(3)

        soup = self.get_soup(name='detail', retry=False)

        seller_info = {
            "ìƒí’ˆëª…": "",
            "ìƒí˜¸ëª…": "",
            "ì‚¬ì—…ì¥ì†Œì¬ì§€": "",
            "ì—°ë½ì²˜": "",
            "URL": self.current_detail_url,
            "PAGE": self.page,
            "í‚¤ì›Œë“œ": self.keyword
        }

        # âœ… ìƒí’ˆëª… ì¶”ì¶œ
        title_tag = soup.find('h1', class_="prod-buy-header__title") or soup.find('h1', class_="product-title")
        if title_tag:
            seller_info["ìƒí’ˆëª…"] = title_tag.get_text(strip=True)
        else:
            self.log_signal_func("âŒ ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # âœ… íŒë§¤ì ì •ë³´ í…Œì´ë¸” ì¶”ì¶œ
        container = soup.find("div", class_="product-item__table product-seller")
        if container:
            table = container.find("table", class_=re.compile(r"prod-delivery-return-policy-table"))
            if table:
                rows = table.find_all("tr")
                for row in rows:
                    ths = row.find_all("th")
                    tds = row.find_all("td")

                    for i in range(min(len(ths), len(tds))):
                        label = ths[i].get_text(strip=True)
                        value = tds[i].get_text(strip=True)

                        if "ìƒí˜¸" in label:
                            seller_info["ìƒí˜¸ëª…"] = value
                        elif "ì†Œì¬ì§€" in label:
                            seller_info["ì‚¬ì—…ì¥ì†Œì¬ì§€"] = value
                        elif "ì—°ë½ì²˜" in label:
                            seller_info["ì—°ë½ì²˜"] = value

        self.log_signal_func(f'ì œí’ˆì •ë³´ : {seller_info}')
        self.log_signal_func(f'ì—°ë½ì²˜ :{seller_info['ì—°ë½ì²˜']}')

        # âœ… ì¤‘ë³µ ì²´í¬ í›„ ì¶”ê°€
        self.result_list.append(seller_info)

        if len(self.result_list) % 5 == 0:
            df = pd.DataFrame(self.result_list, columns=self.columns)
            if not os.path.exists(self.csv_filename):
                df.to_csv(self.csv_filename, mode='a', header=True, index=False, encoding="utf-8-sig")
            else:
                df.to_csv(self.csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")
            self.result_list.clear()

        time.sleep(random.uniform(1, 3))
        

    # soup ì–»ê¸°
    def get_soup(self, name, retry=False):
        # âœ… 1ë‹¨ê³„: ì•„ë˜ ë°©í–¥í‚¤ë¡œ 20ë²ˆ ë¹ ë¥´ê²Œ ìŠ¤í¬ë¡¤
        for _ in range(10):
            pyautogui.press('pagedown')
            time.sleep(0.3)

        # âœ… 2ë‹¨ê³„: ë§ˆì§€ë§‰ì— ìŠ¤í¬ë¡¤ ëê¹Œì§€ ë‚´ë¦¬ê¸°
        for _ in range(3):
            pyautogui.press('end')
            time.sleep(0.3)

        # âœ… HTML ì†ŒìŠ¤ ë³´ê¸° -> ë³µì‚¬ -> ë‹«ê¸°
        pyautogui.hotkey('ctrl', 'u')
        time.sleep(random.uniform(self.html_source_delay_time, self.html_source_delay_time + 2))
        pyautogui.hotkey('ctrl', 'a')
        time.sleep(2)
        pyautogui.hotkey('ctrl', 'c')
        time.sleep(2)
        pyautogui.hotkey('ctrl', 'w')
        time.sleep(0.5)

        html_source = pyperclip.paste()
        self.log_signal_func(f"HTML ê¸¸ì´: {len(html_source)}")
        self.log_signal_func(f"ì‹œì‘ë¶€ë¶„: {html_source[:200]}")

        if "ì‚¬ì´íŠ¸ì— ì—°ê²°í•  ìˆ˜ ì—†ìŒ" in html_source:
            self.log_signal_func("âš ï¸ ì‚¬ì´íŠ¸ ì—°ê²° ì˜¤ë¥˜ ê°ì§€ë¨, í¬ë¡¬ ì¬ì‹œì‘ ì‹œë„")
            if not retry:
                self.log_signal_func(f"ë´‡ ê°ì§€ ì‚¬ì´ ì¹´ìš´íŠ¸ : {self.current_cnt - self.ed_cnt}")
                self.log_signal_func(f"ë´‡ ê°ì§€ ì‚¬ì´ ì‹œê°„ : {time.time() - self.ed_tm}")

                self.chrome_reset(name)
                self.ed_cnt = self.current_cnt
                self.ed_tm = time.time()
                return self.get_soup(name, retry=True)
            else:
                self.log_signal_func("âŒ ì¬ì‹œë„ ì‹¤íŒ¨: í¬ë¡¬ ì¬ì‹œì‘ í›„ì—ë„ ë¬¸ì œ ë°œìƒ")
                return None

        soup = BeautifulSoup(html_source, 'html.parser')
        return soup


    # í¬ë¡¬ ë”œë ˆì´ ì¹´ìš´íŠ¸
    def request_chrome_delay(self):
        # ğŸ‘‰ UIì—ê²Œ ì¹´ìš´íŠ¸ë‹¤ìš´ íŒì—… ìš”ì²­
        self.show_countdown_signal_func(self.chrome_delay_time)

        # ğŸ‘‰ ì‹¤ì œ ëŒ€ê¸°ëŠ” workerê°€ ì§ì ‘ ì§„í–‰
        for remaining in range(self.chrome_delay_time, 0, -1):
            # self.log_signal_func(f"â³ ë‚¨ì€ ì‹œê°„: {remaining}ì´ˆ")
            time.sleep(1)


    # í¬ë¡¬ ë¦¬ì…‹
    def chrome_reset(self, name):
        self.log_signal_func(f"ê°ì§€ë´‡ ìš°íšŒ í¬ë¡¬ ê°•ì œì¢…ë£Œ")
        if name == 'main':
            # í¬ë¡¬ ê°•ì œ ì¢…ë£Œ
            os.system("taskkill /f /im chrome.exe")
            self.log_signal_func(f"í¬ë¡¬ ì¢…ë£Œ ëŒ€ê¸°ì¤‘ ì…ë‹ˆë‹¤. {self.chrome_delay_time}ì´ˆ í›„ì— ì—´ë¦½ë‹ˆë‹¤. ë‹¤ë¥¸ ì‘ì—…ì„ í•˜ì§€ë§ˆì„¸ìš”.")
            self.request_chrome_delay()  # ì¹´ìš´íŠ¸ë‹¤ìš´ íŒì—… ìš”ì²­ + sleep
            # í¬ë¡¬ ì‹¤í–‰ (ì‚¬ìš©ì í”„ë¡œí•„ ìœ ì§€)
            chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            subprocess.Popen([chrome_path, self.current_url])
            time.sleep(2)  # ì¿ íŒ¡ ë¡œë”© ëŒ€ê¸°
            self.log_signal_func(f"í¬ë¡¬ ì‹œì‘")
        else:
            # í¬ë¡¬ ê°•ì œ ì¢…ë£Œ
            os.system("taskkill /f /im chrome.exe")
            self.log_signal_func(f"í¬ë¡¬ ì¢…ë£Œ ëŒ€ê¸°ì¤‘ ì…ë‹ˆë‹¤. {self.chrome_delay_time}ì´ˆ í›„ì— ì—´ë¦½ë‹ˆë‹¤. ë‹¤ë¥¸ ì‘ì—…ì„ í•˜ì§€ë§ˆì„¸ìš”.")
            self.request_chrome_delay()  # ì¹´ìš´íŠ¸ë‹¤ìš´ íŒì—… ìš”ì²­ + sleep
            # í¬ë¡¬ ì‹¤í–‰ (ì‚¬ìš©ì í”„ë¡œí•„ ìœ ì§€)
            chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            subprocess.Popen([chrome_path, self.current_detail_url])
            time.sleep(2)  # ì¿ íŒ¡ ë¡œë”© ëŒ€ê¸°
            self.log_signal_func(f"í¬ë¡¬ ì‹œì‘")


    # ì •ì§€    
    def stop(self):
        self.running = False

