import json
import math
import os
import ssl
import threading
import time
from urllib.parse import urlparse, parse_qs, unquote

import pandas as pd
import pyautogui  # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

from src.utils.time_utils import get_current_yyyymmddhhmmss
from src.workers.api_base_worker import BaseApiWorker
from urllib.parse import quote
import re

ssl._create_default_https_context = ssl._create_unverified_context

image_main_directory = 'albamon_images'
company_name = 'ì•Œë°”ëª¬'
site_name = 'albamon'

excel_filename = ''


class ApiAlbaSetLoadWorker(BaseApiWorker):

    # ì´ˆê¸°í™”
    def __init__(self):
        super().__init__()
        self.schExcludeText = ""
        self.schIncludeText = ""
        self.base_detail_url   = "https://www.alba.co.kr/job/Main"
        self.base_main_url   = "https://www.alba.co.kr/"

        self.running = True  # ì‹¤í–‰ ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€
        self.driver = None

        self.com_list = []
        self.main_model = None
        self.product_info_list = []

        self.total_cnt = 0
        self.total_pages = 0
        self.current_page = 0
        self.current_cnt = 0
        self.before_pro_value = 0


    def init(self):
        # í˜„ì¬ ëª¨ë‹ˆí„° í•´ìƒë„ ê°€ì ¸ì˜¤ê¸°
        screen_width, screen_height = pyautogui.size()

        # ì°½ í¬ê¸°ë¥¼ ë„ˆë¹„ ì ˆë°˜, ë†’ì´ ì „ì²´ë¡œ ì„¤ì •
        self.driver.set_window_size(screen_width // 2, screen_height)

        # ì°½ ìœ„ì¹˜ë¥¼ ì™¼ìª½ ìƒë‹¨ì— ë°°ì¹˜
        self.driver.set_window_position(0, 0)

        # ë¡œê·¸ì¸ ì—´ê¸°
        self.driver.get(self.base_main_url)


    # í”„ë¡œê·¸ë¨ ì‹¤í–‰
    def main(self):
        result_list = []
        time.sleep(1)
        try:
            login_button = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".header-link__item.login.member"))
            )
            self.driver.execute_script("arguments[0].click();", login_button)
            self.log_func("âœ… JSë¡œ ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
        except Exception as e:
            self.log_func(f"âŒ JS í´ë¦­ ì‹¤íŒ¨: {e}")

        self.wait_for_user_confirmation()
        self.wait_for_select_confirmation()

        self.log_func("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì— ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.")
        self.log_func(f"ì „ì²´ íšŒì‚¬ìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        self.total_cnt_cal()
        self.log_func(f"ì „ì²´ íšŒì‚¬ìˆ˜ {self.total_cnt} ê°œ")
        self.log_func(f"ì „ì²´ í˜ì´ì§€ìˆ˜ {self.total_pages} ê°œ")

        csv_filename = self.file_driver.get_csv_filename("ì•Œë°”ì²œêµ­")

        columns = ["NO", "ì‚¬ì—…ì²´ëª…", "ì±„ìš©ë‹´ë‹¹ìëª…", "íœ´ëŒ€í° ë²ˆí˜¸","í¬í•¨ í‚¤ì›Œë“œ", "ì œì™¸ í‚¤ì›Œë“œ"]

        df = pd.DataFrame(columns=columns)
        df.to_csv(csv_filename, index=False, encoding="utf-8-sig")

        for page in range(1, self.total_pages + 1):
            self.log_func(f"í˜„ì¬ í˜ì´ì§€ {page}")
            time.sleep(1)
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                self.log_func("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            collection = self.main_request(page)

            for index, recruit_no in enumerate(collection):

                if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                    self.log_func("í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break

                obj = self.get_api_request(recruit_no)


                obj
                self.log_func(f"í˜„ì¬ ë°ì´í„° :  {obj}")
                time.sleep(1)
                result_list.append(obj)

                if (index + 1) % 5 == 0:
                    self.excel_driver.append_to_csv(csv_filename, result_list, columns)

                self.current_cnt = self.current_cnt + 1

                pro_value = (self.current_cnt / self.total_cnt) * 1000000
                self.progress_signal.emit(self.before_pro_value, pro_value)
                self.before_pro_value = pro_value

                self.log_func(f"í˜„ì¬ í˜ì´ì§€ {self.current_cnt}/{self.total_cnt}")

            if result_list:
                self.excel_driver.append_to_csv(csv_filename, result_list, columns)


    def wait_for_user_confirmation(self):
        self.log_func("í¬ë¡¤ë§ ì‚¬ì´íŠ¸ ì¸ì¦ì„ ì‹œë„ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")

        event = threading.Event()  # OK ë²„íŠ¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°í•  ì´ë²¤íŠ¸ ê°ì²´

        # ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì°½ ìš”ì²­
        self.msg_signal.emit("ë¡œê·¸ì¸ í›„  í›„ OKë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”", "info", event)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.log_func("ğŸ“¢ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...")
        event.wait()  # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥´ë©´ í•´ì œë¨

        # ì¿ í‚¤ ì„¤ì •
        cookies = self.driver.get_cookies()
        for cookie in cookies:
            self.sess.cookies.set(cookie['name'], cookie['value'])

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆŒë €ì„ ê²½ìš° ì‹¤í–‰
        self.log_func("âœ… ì‚¬ìš©ìê°€ í™•ì¸ ë²„íŠ¼ì„ ëˆŒë €ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‘ì—… ì§„í–‰ ì¤‘...")

        self.driver.get(self.base_detail_url)

        time.sleep(2)  # ì˜ˆì œìš©

        self.log_func("ğŸš€ ì‘ì—… ì™„ë£Œ!")


    def wait_for_select_confirmation(self):
        """ì‚¬ìš©ìê°€ í™•ì¸(alert) ì°½ì—ì„œ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°"""
        event = threading.Event()  # OK ë²„íŠ¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°í•  ì´ë²¤íŠ¸ ê°ì²´

        # ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì°½ ìš”ì²­
        self.msg_signal.emit("í‚¤ì›Œë“œ(í¬í•¨/ì œì™¸) ì¶”ê°€ í›„ ê²€ìƒ‰ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”(ì•„ë˜ ëª©ë¡ì´ ë‚˜ì˜¤ëŠ”ê±¸ í™•ì¸í•˜ì„¸ìš”)", "info", event)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥¼ ë•Œê¹Œì§€ ëŒ€ê¸°
        self.log_func("ğŸ“¢ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...")
        event.wait()  # ì‚¬ìš©ìê°€ OKë¥¼ ëˆ„ë¥´ë©´ í•´ì œë¨

        current_url = self.driver.current_url
        parsed_url = urlparse(current_url)
        query_params = parse_qs(parsed_url.query)

        # ì‚¬ìš©ìê°€ OKë¥¼ ëˆŒë €ì„ ê²½ìš° ì‹¤í–‰
        self.log_func("âœ… í™•ì¸ ë²„íŠ¼ì„ ëˆŒë €ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‘ì—… ì§„í–‰ ì¤‘...")
        # âœ… ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        # self.hidListView = query_params.get("hidListView", [""])[0]
        # self.hidSortCnt = query_params.get("hidSortCnt", [""])[0]
        # self.hidSortFilter = query_params.get("hidSortFilter", [""])[0]
        # self.page = query_params.get("page", [""])[0]
        # self.hidSearchyn = query_params.get("hidSearchyn", [""])[0]
        self.schIncludeText = unquote(query_params.get("schIncludeText", [""])[0])
        self.schExcludeText = unquote(query_params.get("schExcludeText", [""])[0])

        # âœ… ë¡œê·¸ ì¶œë ¥
        self.log_func(f"ğŸ” í¬í•¨ í‚¤ì›Œë“œ: {self.schIncludeText}")
        self.log_func(f"ğŸ” ì œì™¸ í‚¤ì›Œë“œ: {self.schExcludeText}")

        time.sleep(2)
        self.log_func("ğŸš€ ì‘ì—… ì™„ë£Œ!")




    def main_request(self, page=1, req_type=None):
        """í˜„ì¬ ë¸Œë¼ìš°ì € URLì„ ê¸°ë°˜ìœ¼ë¡œ API ìš”ì²­"""
        url = "https://www.alba.co.kr/job/main"

        params = {
            "page": page,
            "pagesize": "50",
            "hidlistview": "LIST",
            "hidsortcnt": "50",
            "hidsortfilter": "Y",
            "hidsearchyn": "Y",
            "schIncludeText": self.schIncludeText,
            "schExcludeText": self.schExcludeText
        }

        headers = {
            "authority": "www.alba.co.kr",
            "method": "GET",
            "scheme": "https",
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko,en;q=0.9,en-US;q=0.8",
            "sec-ch-ua": '"Chromium";v="136", "Microsoft Edge";v="136", "Not.A/Brand";v="99"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "sec-gpc": "1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0",
        }

        total_cnt = 0
        total_pages = 0
        imid_list = []

        try:
            response = self.sess.get(url, headers=headers, params=params)

            if not response.ok:
                self.log_func(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return total_cnt, total_pages, imid_list

            self.log_func("âœ… ìš”ì²­ ì„±ê³µ")
            soup = BeautifulSoup(response.text, 'html.parser')

            # 1í˜ì´ì§€ì¼ ê²½ìš° ìˆ«ìë„ ì¶”ì¶œ
            strong_tag = soup.find('strong', class_='point-color1')
            if strong_tag:
                total_cnt_text = strong_tag.get_text(strip=True)
                total_cnt = int(re.sub(r'[^\d]', '', total_cnt_text))
                total_pages = math.ceil(total_cnt / 50)

            # í•­ìƒ imid_listë„ ì¶”ì¶œ
            tbody = soup.find('tbody', class_='observe-job')
            if tbody:
                rows = tbody.find_all('tr')
                for tr in rows:
                    imid = tr.get('data-imid')
                    if imid:
                        imid_list.append(imid)
            else:
                self.log_func("âš ï¸ <tbody class='observe-job'>ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.log_func(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ: {e}")
        finally:
            if req_type == 'c':
                return total_cnt, total_pages, imid_list
            else:
                return imid_list


    # í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    def get_api_request(self, recruit_no):
        url = f"https://www.alba.co.kr/job/Detail?adid={recruit_no}&listmenucd=ENTIRE"

        headers = {
            "authority": "www.alba.co.kr",
            "method": "GET",
            "path": "/job/main",
            "scheme": "https",
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko,en;q=0.9,en-US;q=0.8",
            "sec-ch-ua": '"Chromium";v="136", "Microsoft Edge";v="136", "Not.A/Brand";v="99"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "sec-gpc": "1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0",
        }
        company_info = {
            'NO': recruit_no,
            'ì‚¬ì—…ì²´ëª…': '',
            'ì±„ìš©ë‹´ë‹¹ìëª…': '',
            'íœ´ëŒ€í° ë²ˆí˜¸': '',
            'í¬í•¨ í‚¤ì›Œë“œ': self.schIncludeText,
            'ì œì™¸ í‚¤ì›Œë“œ': self.schExcludeText
        }

        try:
            res = self.sess.get(url, headers=headers, timeout=10)

            # ì‘ë‹µ ìƒíƒœ í™•ì¸
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, "html.parser")


                # 1. ì‚¬ì—…ì²´ëª…
                name_tag = soup.find('div', class_='detail-primary__company')
                if name_tag:
                    company_info['ì‚¬ì—…ì²´ëª…'] = name_tag.get_text(strip=True)

                # 2. ë‹´ë‹¹ìëª…, íœ´ëŒ€í°ë²ˆí˜¸ (dl ìˆœì„œ ê¸°ë°˜)
                info_container = soup.find('div', id='InfoCompany')
                if info_container:
                    def_items = info_container.select('.detail-def__item')
                    for item in def_items:
                        term = item.find('dt')
                        data = item.find('dd')

                        if not term or not data:
                            continue

                        term_text = term.get_text(strip=True)

                        if term_text == 'ë‹´ë‹¹ìëª…':
                            company_info['ì±„ìš©ë‹´ë‹¹ìëª…'] = data.get_text(strip=True)

                        elif term_text == 'ì—°ë½ì²˜':
                            # ì—°ë½ì²˜ ì•ˆì— ì—¬ëŸ¬ divê°€ ìˆì„ ê²½ìš° ì²« ë²ˆì§¸ divë§Œ
                            first_div = data.find('div')
                            if first_div and first_div.get_text(strip=True):
                                company_info['íœ´ëŒ€í° ë²ˆí˜¸'] = first_div.get_text(strip=True)
                            else:
                                # div ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ 010- í¬í•¨ë˜ëŠ” ê²ƒë§Œ ì¶”ì¶œ
                                full_text = data.get_text(strip=True)
                                if full_text:
                                    company_info['íœ´ëŒ€í° ë²ˆí˜¸'] = full_text

                # 3. ë³´ì¡° ì²˜ë¦¬: td ë‚´ë¶€ì— '010-' í¬í•¨ëœ ê²½ìš°
                if not company_info.get('íœ´ëŒ€í° ë²ˆí˜¸', '').startswith('010-'):
                    found = False
                    for td in soup.find_all('td'):
                        td_text = td.get_text(strip=True)
                        if td_text.startswith('010-'):
                            company_info['íœ´ëŒ€í° ë²ˆí˜¸'] = td_text
                            found = True
                            break
                        for span in td.find_all('span'):
                            span_text = span.get_text(strip=True)
                            if span_text.startswith('010-'):
                                company_info['íœ´ëŒ€í° ë²ˆí˜¸'] = span_text
                                found = True
                                break
                        if found:
                            break
            else:
                # ìƒíƒœ ì½”ë“œê°€ 200ì´ ì•„ë‹Œ ê²½ìš°
                self.log_func(f"HTTP ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {res.status_code}, ë‚´ìš©: {res.text}")

        except Exception as e:
            print(f'error : {e}')
            # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë˜ëŠ” ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            self.log_func(f"ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        finally:
            return company_info

    # ì „ì²´ ê°¯ìˆ˜ ì¡°íšŒ
    def total_cnt_cal(self):
        try:
            total_cnt, total_pages, imid_list = self.main_request(1, 'c')
            self.total_cnt = total_cnt
            self.total_pages = total_pages
        except Exception as e:
            print(f"Error calculating total count: {e}")
