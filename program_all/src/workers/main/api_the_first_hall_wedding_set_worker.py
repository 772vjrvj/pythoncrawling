import os
import ssl
import time
import re
import pandas as pd
import requests
from PyQt5.QtCore import QThread, pyqtSignal
from bs4 import BeautifulSoup

from src.utils.BeautifulSoup_utils import bs_txt

from src.utils.number_utils import calculate_divmod, divide_and_truncate_per
from src.utils.selenium_utils import SeleniumUtils
from src.utils.str_utils import get_query_params, str_norm
from src.workers.api_base_worker import BaseApiWorker
from src.core.global_state import GlobalState
from src.utils.api_utils import APIClient
from src.utils.excel_utils import ExcelUtils
from src.utils.file_utils import FileUtils
from urllib.parse import urlparse, parse_qs, urljoin



# API
class ApiThefirsthallweddingDetailSetLoadWorker(BaseApiWorker):


    def __init__(self):
        super().__init__()
        self.file_driver = None
        self.excel_driver = None
        self.url_obj_list = []
        self.site_url = "https://www.thewedd.com"
        self.site_review_url = "https://www.thewedd.com/review"
        self.driver = None
        self.running = True  # ì‹¤í–‰ ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€
        self.site_name = "THE FIRST HALL"
        self.csv_filename = ""
        self.product_obj_list = []
        self.total_cnt = 0
        self.page = 0
        self.current_cnt = 0
        self.before_pro_value = 0
        self.api_client = APIClient(use_cache=False)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Connection": "keep-alive"
        }


    # ì´ˆê¸°í™”
    def init(self):
        self.driver_set(True)
        self.driver.get(self.site_url)
        return True

    # ë©”ì¸
    def main(self):
        try:
            self.log_signal.emit("í¬ë¡¤ë§ ì‹œì‘")

            self.set_cookies()

            # csvíŒŒì¼ ë§Œë“¤ê¸°
            self.csv_filename = self.file_driver.get_csv_filename(self.site_name)


            self.excel_driver.init_csv(self.csv_filename, self.columns)

            # url ê°€ì ¸ì˜¤ê¸°
            self.api_url_obj_list()

            # ìƒì„¸ ë°ì´í„°
            self.api_detail_data()

            # CSV -> ì—‘ì…€ ë³€í™˜
            self.excel_driver.convert_csv_to_excel_and_delete(self.csv_filename)

            return True
        except Exception as e:
            self.log_signal_func(f"âŒ ì „ì²´ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return False

    # ë“œë¼ì´ë²„ ì„¸íŒ…
    def driver_set(self, headless):
        self.log_signal_func("ë“œë¼ì´ë²„ ì„¸íŒ… ========================================")

        # ì—‘ì…€ ê°ì²´ ì´ˆê¸°í™”
        self.excel_driver = ExcelUtils(self.log_signal_func)

        # íŒŒì¼ ê°ì²´ ì´ˆê¸°í™”
        self.file_driver = FileUtils(self.log_signal_func)
        # ì…€ë ˆë‹ˆì›€ ì´ˆê¸°í™”
        self.selenium_driver = SeleniumUtils(headless)

        self.driver = self.selenium_driver.start_driver(1200)


    # ì¿ í‚¤ì„¸íŒ…
    def set_cookies(self):
        self.log_signal_func("ğŸ“¢ ì¿ í‚¤ ì„¸íŒ… ì‹œì‘")
        cookies = {cookie['name']: cookie['value'] for cookie in self.driver.get_cookies()}

        for name, value in cookies.items():
            self.api_client.cookie_set(name, value)
        self.log_signal_func("ğŸ“¢ ì¿ í‚¤ ì„¸íŒ… ì™„ë£Œ")
        time.sleep(2)

    def api_url_obj_list(self):
        items, seen = [], set()
        self.page = 1
        while True:
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                break

            url = f"{self.site_review_url}?page={self.page}&category=&cate=&event_type=&desc=&desc2=&list_limit="

            try:
                resp = self.api_client.get(url, headers=self.headers)
                soup = BeautifulSoup(resp, "html.parser")
                table = soup.find("table", class_="story_list_tbl")
                rows = table.select("tbody tr") if table else []

                if not rows:
                    self.log_signal_func(f"ğŸ“¢ PAGE : {self.page} ë°ì´í„° ì—†ì–´ ì¢…ë£Œ")
                    break

                got = 0
                for i, tr in enumerate(rows, start=1):
                    tds = tr.find_all("td")
                    no_text = bs_txt(tds[0]) if tds else ""
                    a = tr.select_one("td.subject a[href]")
                    if not a:
                        continue
                    href = (a.get("href") or "").strip()
                    full = href if href.startswith("http") else urljoin(self.site_url, href)

                    # ë‚ ì§œ: ë³´í†µ ë§ˆì§€ë§‰ tdê°€ ë“±ë¡ì¼
                    reg_dt = bs_txt(tds[-1]) if len(tds) >= 3 else bs_txt(tr.select_one("td.date"))

                    if full in seen:
                        continue
                    seen.add(full)
                    items.append({
                        "No": no_text,
                        "ë“±ë¡ì¼": reg_dt,
                        "url": full,
                    })
                    got += 1

                    self.log_signal_func(f"ğŸ“¢ PAGE:{self.page} / ROW:{i}/{len(rows)} => No={no_text}, ë“±ë¡ì¼={reg_dt}, url={full}")

                self.log_signal_func(f"âœ… PAGE:{self.page} ì™„ë£Œ (+{got}, ëˆ„ì  {len(items)}ê°œ)")
                self.page += 1
                time.sleep(.5)

            except Exception as e:
                self.log_signal_func(f"âŒ PAGE:{self.page} ì—ëŸ¬: {e}")
                # ë‹¤ìŒ í˜ì´ì§€ ì‹œë„ (ì›í•˜ë©´ ì—¬ê¸°ì„œ break ì²˜ë¦¬ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ)
                self.page += 1
                time.sleep(1)

        self.url_obj_list = items



    # ê¸°ì¡´ clean_text ìœ ì§€í•˜ë˜ NBSPë„ ê°™ì´ ì •ëˆë˜ë„ë¡ ì‚´ì§ ë³´ê°•
    def clean_text(self, s):
        if s is None:
            return ""
        s = s.replace("\xa0", " ").replace("\u200b", " ")
        s = re.sub(r"[ \t]+", " ", s)
        s = s.replace("\r\n", "\n").replace("\r", "\n")
        s = re.sub(r"\n{3,}", "\n\n", s)
        return s.strip()


    # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆì—ì„œ ì´ë¯¸ì§€/ë¯¸ë””ì–´ ì œê±°, <a>ëŠ” í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹€
    def _strip_media_and_unwrap_links(self, container):
        if not container:
            return
        # ì´ë¯¸ì§€/ë¯¸ë””ì–´ íƒœê·¸ ì œê±°
        for t in container.find_all(["img", "figure", "picture", "source", "iframe", "video", "svg", "noscript"]):
            t.decompose()
        # ë§í¬ëŠ” í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ê¸° (ë°°ë„ˆ/ê´‘ê³  ë§í¬ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ë¼ì§)
        for a in container.find_all("a"):
            a.unwrap()


    # <p>ë§Œ ì¤‘ì‹¬ìœ¼ë¡œ ë¬¸ë‹¨ì„ ì¬êµ¬ì„±. <p>&nbsp;</p>ëŠ” ë¹ˆ ì¤„ë¡œ.
    def _reconstruct_paragraphs(self, container):
        if not container:
            return ""

        # ìš°ì„  ë¯¸ë””ì–´ ì œê±°/ë§í¬ ì •ë¦¬
        self._strip_media_and_unwrap_links(container)

        paragraphs = container.find_all("p")
        lines = []

        if paragraphs:
            for p in paragraphs:
                txt = p.get_text(" ", strip=True) #<p>Hello<b>World</b></p>  "Hello World"
                txt = str_norm(txt)
                # <p>&nbsp;</p> ê°™ì€ ë¹ˆ ë¬¸ë‹¨ â†’ ë¹ˆ ì¤„
                if txt == "":
                    lines.append("")  # ë¹ˆ ì¤„
                else:
                    lines.append(txt)
        else:
            # <p>ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ íšŒìˆ˜

            # <div class="container">
            # ì•ˆë…•í•˜ì„¸ìš”&nbsp;ì„¸ê³„
            # <p>ì²« ë²ˆì§¸ ë¬¸ë‹¨</p>
            # <p>ë‘ ë²ˆì§¸ ë¬¸ë‹¨</p>
            # ì„¸ ë²ˆì§¸&nbsp;ë¼ì¸
            # </div>

            raw = container.get_text("\n", strip=True) #"ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„\nì²« ë²ˆì§¸ ë¬¸ë‹¨\në‘ ë²ˆì§¸ ë¬¸ë‹¨\nì„¸ ë²ˆì§¸ ë¼ì¸"
            raw = str_norm(raw) #"ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„\nì²« ë²ˆì§¸ ë¬¸ë‹¨\në‘ ë²ˆì§¸ ë¬¸ë‹¨\nì„¸ ë²ˆì§¸ ë¼ì¸"
            lines = raw.split("\n") # # ["ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„", "ì²« ë²ˆì§¸ ë¬¸ë‹¨", "ë‘ ë²ˆì§¸ ë¬¸ë‹¨", "ì„¸ ë²ˆì§¸ ë¼ì¸"]

        # ì—°ì† ë¹ˆ ì¤„ 1ê°œë¡œ ì••ì¶•
        out = []
        prev_blank = False
        # ì§ì „ ì¤„ì´ ë¹ˆ ì¤„ì´ì—ˆëŠ”ì§€" ê¸°ë¡
        # ì¤‘ë³µ ë°©ì§€
        for ln in lines:
            if str_norm(ln) == "":
                if not prev_blank:
                    out.append("")
                prev_blank = True
            else:
                out.append(ln)
                prev_blank = False

        text = "\n".join(out).strip()
        return self.clean_text(text)


    def api_detail_data(self):
        # url_obj_list ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë¡œê·¸
        self.log_signal_func(f"ğŸ“Œ ì´ {len(self.url_obj_list)}ê°œ ë§í¬")
        self.total_cnt = len(self.url_obj_list)
        buffer_list = []

        for i, obj in enumerate(self.url_obj_list, start=1):
            if not self.running:  # ì‹¤í–‰ ìƒíƒœ í™•ì¸
                break
            url = obj.get("url")
            self.current_cnt += 1
            if not url:
                self.log_signal_func(f"[{i}] URL ì—†ìŒ, ìŠ¤í‚µ")
                continue

            try:
                resp = self.api_client.get(url, headers=self.headers)
                soup = BeautifulSoup(resp, "html.parser")

                # íƒ€ì´í‹€ í›„ë³´
                title_el = soup.select_one("table.review_view_tbl thead th")
                title_text = self.clean_text(str_norm(title_el.get_text(" ", strip=True))) if title_el else ""

                # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í›„ë³´: í…Œì´ë¸” êµ¬ì¡° ìš°ì„ , ì—†ìœ¼ë©´ ì¼ë°˜ ì»¨í…Œì´ë„ˆ
                body_container = soup.select_one("table.review_view_tbl tbody")
                body_text = self._reconstruct_paragraphs(body_container) if body_container else ""

                obj["ì œëª©"] = title_text
                obj["ë‚´ìš©"] = body_text

                buffer_list.append(obj)

                if i % 5 == 0:
                    self.excel_driver.append_to_csv(self.csv_filename, buffer_list, self.columns)

                pro_value = (self.current_cnt / self.total_cnt) * 1000000
                self.progress_signal.emit(self.before_pro_value, pro_value)
                self.before_pro_value = pro_value

                self.log_signal_func(f"[{self.current_cnt} / {self.total_cnt}] Data: {obj}")

                time.sleep(.5)

            except Exception as e:
                self.log_signal_func(f"[{i}] âŒ ì—ëŸ¬: {e} / URL={url}")


    # ë§ˆë¬´ë¦¬
    def destroy(self):
        self.progress_signal.emit(self.before_pro_value, 1000000)
        self.log_signal_func("=============== í¬ë¡¤ë§ ì¢…ë£Œì¤‘...")
        time.sleep(5)
        self.log_signal_func("=============== í¬ë¡¤ë§ ì¢…ë£Œ")
        self.progress_end_signal.emit()

    # í”„ë¡œê·¸ë¨ ì¤‘ë‹¨
    def stop(self):
        self.running = False
        if self.driver:
            self.driver.quit()




