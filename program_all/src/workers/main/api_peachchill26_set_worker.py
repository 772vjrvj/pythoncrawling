# -*- coding: utf-8 -*-
import os
import re
import json
import time
import html
import hashlib
from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

from src.workers.api_base_worker import BaseApiWorker
from src.utils.excel_utils import ExcelUtils
from src.utils.file_utils import FileUtils


class ApiPeachhill26SetLoadWorker(BaseApiWorker):
    """
    peachhill.kr /26 ìƒí’ˆ ìˆ˜ì§‘ Worker (requests + BeautifulSoup)
    - Selenium/ì¿ í‚¤ ë¶ˆí•„ìš”
    - page=st_page..ed_page ì„¤ì • ì‹œ ê·¸ ë²”ìœ„ë§Œ
    - ì„¤ì • ì—†ìœ¼ë©´ page=1.. (ë°ì´í„° ì—†ì„ ë•Œê¹Œì§€, 'ì§ì „ í˜ì´ì§€ì™€ ë™ì¼'ì´ë©´ ì¢…ë£Œ)
    - self.columnsëŠ” ìƒìœ„ì—ì„œ ì²´í¬ëœ 'ì—‘ì…€ í—¤ë”(value)' ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ì˜´
      => row ë”•ì…”ë„ˆë¦¬ keyë„ "ì—‘ì…€ í—¤ë” ê·¸ëŒ€ë¡œ" ì‚¬ìš©í•´ì•¼ í•¨
    """

    def __init__(self):
        super().__init__()

        # ===============================
        # ê¸°ë³¸ ì„¤ì •
        # ===============================
        self.site_name = "í”¼ì¹˜í"
        self.base_url = "https://peachhill.kr"
        self.list_path = "/26/"
        self.ajax_path = "/ajax/oms/OMS_get_product.cm"

        self.out_dir = Path("./peachhill26")
        self.detail_root = self.out_dir / "detail_images"

        self.timeout = 20
        self.retries = 3
        self.max_workers = 8
        self.sleep_between_pages = 0.35
        self.sleep_between_items = 0.10

        self.running = True

        # ì§„í–‰ë¥ 
        self.total_cnt = 0
        self.current_cnt = 0
        self.before_pro_value = 0

        # IO ìœ í‹¸
        self.excel_driver = None
        self.file_driver = None
        self.csv_filename = ""

        # ê³µí†µ headers (HTML)
        self.headers_html = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/143.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Connection": "keep-alive",
        }

    # =========================================================
    # BaseApiWorker hook
    # =========================================================
    def init(self):
        self.driver_set()

        self.out_dir.mkdir(parents=True, exist_ok=True)
        self.detail_root.mkdir(parents=True, exist_ok=True)

        # CSV íŒŒì¼ëª… ìƒì„±
        try:
            self.csv_filename = self.file_driver.get_csv_filename(self.site_name)
        except Exception:
            ts = time.strftime("%Y%m%d_%H%M%S")
            self.csv_filename = str(self.out_dir / (self.site_name + "_" + ts + ".csv"))

        if not self.columns:
            raise RuntimeError("self.columns ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ì„¤ì •ì—ì„œ ì»¬ëŸ¼ ì²´í¬ í•„ìš”)")

        # CSV ì´ˆê¸°í™”
        self.excel_driver.init_csv(self.csv_filename, self.columns)

        self.log_signal_func("âœ… peachhill /26 init ì™„ë£Œ")
        return True

    def driver_set(self):
        self.log_signal_func("ë“œë¼ì´ë²„ ì„¸íŒ… ================================")
        self.excel_driver = ExcelUtils(self.log_signal_func)
        self.file_driver = FileUtils(self.log_signal_func)

    def stop(self):
        self.running = False

    def destroy(self):
        # ì§„í–‰ë¥  100% ì°ê³  ì¢…ë£Œ ì‹œê·¸ë„
        try:
            self.progress_signal.emit(self.before_pro_value, 1000000)
        except Exception:
            pass

        self.log_signal_func("=============== peachhill í¬ë¡¤ë§ ì¢…ë£Œì¤‘...")
        time.sleep(0.5)
        self.log_signal_func("=============== peachhill í¬ë¡¤ë§ ì¢…ë£Œ")

        try:
            self.progress_end_signal.emit()
        except Exception:
            pass

    # =========================================================
    # ì„¤ì •: pages
    # =========================================================
    def get_page_range_from_setting(self):
        """
        setting ê°’ st_page, ed_page ìˆìœ¼ë©´ (int, int)
        ì—†ìœ¼ë©´ (None, None)
        """
        try:
            st = (self.get_setting_value(self.setting, "st_page") or "").strip()
            ed = (self.get_setting_value(self.setting, "ed_page") or "").strip()

            if not st or not ed:
                return None, None

            st_page = int(st)
            ed_page = int(ed)

            if st_page < 1:
                st_page = 1
            if ed_page < st_page:
                ed_page = st_page

            return st_page, ed_page
        except Exception:
            return None, None

    # =========================================================
    # Main
    # =========================================================
    def main(self):
        try:
            self.log_signal_func("ğŸš€ peachhill /26 í¬ë¡¤ë§ ì‹œì‘")

            # 0) í˜ì´ì§€ ë²”ìœ„ ê²°ì •
            st_page, ed_page = self.get_page_range_from_setting()
            if st_page and ed_page:
                self.log_signal_func("ğŸ“Œ ì„¤ì • í˜ì´ì§€ ë²”ìœ„: %s ~ %s" % (st_page, ed_page))
            else:
                self.log_signal_func("ğŸ“Œ ì„¤ì • í˜ì´ì§€ ì—†ìŒ -> page=1ë¶€í„° ëê¹Œì§€(ë™ì¼í˜ì´ì§€ ë°˜ë³µ ê°ì§€ë¡œ ì¢…ë£Œ)")

            # 1) ëª©ë¡ ì „ì²´ ìˆ˜ì§‘
            products = self.collect_all_products(st_page, ed_page)

            self.total_cnt = len(products)
            self.current_cnt = 0
            self.before_pro_value = 0

            if self.total_cnt <= 0:
                self.log_signal_func("âš ï¸ ìˆ˜ì§‘ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ")
                return False

            self.log_signal_func("ğŸ“Œ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ. ì „ì²´ ìƒí’ˆìˆ˜: %d" % self.total_cnt)

            # 2) ìƒí’ˆë³„ ì²˜ë¦¬
            idx = 0
            for row in products:
                if not self.running:
                    self.log_signal_func("â›” ì¤‘ì§€ ìš”ì²­ ê°ì§€. ì‘ì—… ì¢…ë£Œ")
                    break

                idx += 1
                self.current_cnt = idx

                now_per = (self.current_cnt / float(self.total_cnt)) * 100.0
                self.log_signal_func("====================================================================================================")
                self.log_signal_func("ì „ì²´ ìƒí’ˆ(%d/%d) [%.2f%%]" % (self.current_cnt, self.total_cnt, now_per))
                self.log_signal_func("í˜„ì¬ ìƒí’ˆì½”ë“œ: %s" % (row.get("ìƒí’ˆì½”ë“œ") or ""))
                self.log_signal_func("----------------------------------------------------------------------------------------------------")

                # (1) ëª©ë¡ ì´ë¯¸ì§€(ìµœëŒ€ 2ì¥)
                self.process_list_images(row)

                # (2) AJAX ìƒì„¸(ìœ íŠœë¸Œ + ìƒì„¸ì´ë¯¸ì§€)
                self.process_ajax_detail(row)

                # (3) ìƒì„¸ HTML ì¸ë„¤ì¼(.shop_goods_img)
                self.process_thumbnails(row)

                # (4) CSV append
                self.excel_driver.append_to_csv(self.csv_filename, [row], self.columns)

                # progress(0~1,000,000)
                pro_value = (self.current_cnt / float(self.total_cnt)) * 1000000
                self.progress_signal.emit(self.before_pro_value, pro_value)
                self.before_pro_value = pro_value

                self.log_signal_func("âœ… ì €ì¥ ì™„ë£Œ")
                self.log_signal_func("====================================================================================================")

                time.sleep(self.sleep_between_items)

            # 3) CSV -> ì—‘ì…€ ë³€í™˜
            try:
                self.excel_driver.convert_csv_to_excel_and_delete(self.csv_filename)
                self.log_signal_func("âœ… CSV -> Excel ë³€í™˜ ì™„ë£Œ")
            except Exception as e:
                self.log_signal_func("âš ï¸ CSV -> Excel ë³€í™˜ ì‹¤íŒ¨(ë¬´ì‹œ ê°€ëŠ¥): " + str(e))

            return True

        except Exception as e:
            self.log_signal_func("âŒ ì˜¤ë¥˜: " + str(e))
            return False

    # =========================================================
    # ìœ í‹¸
    # =========================================================
    def uniq_keep_order(self, seq):
        seen = set()
        out = []
        for x in seq:
            x = (x or "").strip()
            if not x:
                continue
            if x in seen:
                continue
            seen.add(x)
            out.append(x)
        return out

    def guess_ext(self, url):
        try:
            ext = os.path.splitext(urlparse(url).path)[1].lower()
            if ext in [".jpg", ".jpeg", ".png", ".gif", ".webp"]:
                return ext
        except Exception:
            pass
        return ".jpg"

    def build_img_path(self, product_code, subdir_ko, url, prefix, seq):
        h = hashlib.md5(url.encode("utf-8")).hexdigest()[:10]
        filename = "%s_%03d_%s%s" % (prefix, seq, h, self.guess_ext(url))
        return self.detail_root / str(product_code) / subdir_ko / filename

    def download_one(self, url, path):
        path.parent.mkdir(parents=True, exist_ok=True)

        if path.exists() and path.stat().st_size > 0:
            return

        last_err = None
        for i in range(self.retries):
            try:
                r = requests.get(url, headers=self.headers_html, stream=True, timeout=self.timeout)
                r.raise_for_status()

                tmp = path.with_suffix(path.suffix + ".part")
                f = open(tmp, "wb")
                try:
                    for chunk in r.iter_content(65536):
                        if chunk:
                            f.write(chunk)
                finally:
                    f.close()

                tmp.replace(path)
                return

            except Exception as e:
                last_err = e
                time.sleep(0.6 * (i + 1))

        raise RuntimeError("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: %s err=%s" % (url, str(last_err)))

    def http_get_text(self, url, params=None, headers=None):
        last_err = None
        for i in range(self.retries):
            try:
                r = requests.get(
                    url,
                    params=params,
                    headers=headers or self.headers_html,
                    timeout=self.timeout
                )
                r.raise_for_status()
                return r.text
            except Exception as e:
                last_err = e
                time.sleep(0.6 * (i + 1))
        raise RuntimeError("GET ì‹¤íŒ¨: %s err=%s" % (url, str(last_err)))

    # =========================================================
    # 1) ëª©ë¡ ìˆ˜ì§‘
    # =========================================================
    def collect_all_products(self, st_page=None, ed_page=None):
        products = []
        seen_product_codes = set()

        list_url = urljoin(self.base_url, self.list_path)

        def parse_items(html_text):
            soup = BeautifulSoup(html_text, "html.parser")
            return soup.select("div.shop-item._shop_item")

        def fingerprint(item_divs):
            ids = []
            for it in item_divs:
                raw = it.get("data-product-properties") or ""
                raw = html.unescape(raw).strip()
                try:
                    props = json.loads(raw) if raw else {}
                except Exception:
                    props = {}
                pid = str(props.get("idx") or "").strip()
                if pid:
                    ids.append(pid)
            return "|".join(ids)

        # ì„¤ì • ë²”ìœ„ê°€ ìˆìœ¼ë©´ ë²”ìœ„ë§Œ
        if st_page and ed_page:
            page = st_page
            while self.running and page <= ed_page:
                self.log_signal_func("[LIST] page=%d ìš”ì²­(ì„¤ì •)" % page)
                html_text = self.http_get_text(list_url, params={"page": page})
                item_divs = parse_items(html_text)
                self.log_signal_func("[LIST] page=%d items=%d" % (page, len(item_divs)))

                if item_divs:
                    self._append_list_items(item_divs, products, seen_product_codes)

                page += 1
                time.sleep(self.sleep_between_pages)

            return products

        # ì„¤ì • ì—†ìœ¼ë©´ page=1..N, ë™ì¼ í˜ì´ì§€ ë°˜ë³µ ê°ì§€ë¡œ ì¢…ë£Œ
        page = 1
        last_fp = ""
        while self.running:
            self.log_signal_func("[LIST] page=%d ìš”ì²­" % page)
            html_text = self.http_get_text(list_url, params={"page": page})
            item_divs = parse_items(html_text)
            self.log_signal_func("[LIST] page=%d items=%d" % (page, len(item_divs)))

            if not item_divs:
                self.log_signal_func("[LIST] page=%d ë°ì´í„° ì—†ìŒ -> ì¢…ë£Œ" % page)
                break

            fp = fingerprint(item_divs)
            if fp and fp == last_fp:
                self.log_signal_func("[LIST] page=%d ì§ì „ í˜ì´ì§€ì™€ ë™ì¼ ê°ì§€ -> ì¢…ë£Œ" % page)
                break
            last_fp = fp

            self._append_list_items(item_divs, products, seen_product_codes)

            page += 1
            time.sleep(self.sleep_between_pages)

        return products

    def _append_list_items(self, item_divs, products, seen_product_codes):
        for item in item_divs:
            raw = item.get("data-product-properties") or ""
            raw = html.unescape(raw).strip()

            try:
                props = json.loads(raw) if raw else {}
            except Exception:
                props = {}

            product_code = str(props.get("idx") or "").strip()
            if not product_code:
                continue
            if product_code in seen_product_codes:
                continue
            seen_product_codes.add(product_code)

            product_name = props.get("name") or ""
            product_price = props.get("price") or ""

            # detail_url
            a = item.select_one('a[href*="idx="]')
            detail_url = urljoin(self.base_url, a.get("href")) if a and a.get("href") else ""
            if not detail_url:
                detail_url = urljoin(self.base_url, self.list_path + "?idx=" + product_code)

            # ëª©ë¡ ì´ë¯¸ì§€(ìµœëŒ€ 2)
            imgs = []
            for img in item.select("img"):
                src = (img.get("src") or "").strip()
                if src:
                    imgs.append(urljoin(self.base_url, src))
                if len(imgs) >= 2:
                    break
            imgs = self.uniq_keep_order(imgs)

            # âœ… row key = ì—‘ì…€ í—¤ë”(value) ê·¸ëŒ€ë¡œ
            products.append({
                "ìƒí’ˆì½”ë“œ": product_code,
                "ìƒí’ˆëª…": product_name,
                "ìƒí’ˆê°€ê²©": product_price,
                "ìƒí’ˆ ìƒì„¸ ì •ë³´ URL": detail_url,

                "ìƒí’ˆ ëª©ë¡ ì´ë¯¸ì§€ URL": json.dumps(imgs, ensure_ascii=False),
                "ìƒí’ˆ ëª©ë¡ ì´ë¯¸ì§€ëª…": "[]",

                "ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL": "[]",
                "ì¸ë„¤ì¼ ì´ë¯¸ì§€ëª…": "[]",

                "YOUTUBE URL": "",

                "ìƒí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€ URL": "[]",
                "ìƒí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€ëª…": "[]",
            })

    # =========================================================
    # 2) ëª©ë¡ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    # =========================================================
    def process_list_images(self, row):
        product_code = (row.get("ìƒí’ˆì½”ë“œ") or "").strip()
        if not product_code:
            row["ìƒí’ˆ ëª©ë¡ ì´ë¯¸ì§€ëª…"] = "[]"
            return

        try:
            urls = json.loads(row.get("ìƒí’ˆ ëª©ë¡ ì´ë¯¸ì§€ URL") or "[]")
            if not isinstance(urls, list):
                urls = []
        except Exception:
            urls = []

        urls = self.uniq_keep_order(urls)
        if not urls:
            row["ìƒí’ˆ ëª©ë¡ ì´ë¯¸ì§€ëª…"] = "[]"
            return

        names = []
        futures = []

        self.log_signal_func("[ìƒí’ˆëª©ë¡ì´ë¯¸ì§€] ì‹œì‘ ìƒí’ˆì½”ë“œ=%s cnt=%d" % (product_code, len(urls)))

        ex = ThreadPoolExecutor(max_workers=self.max_workers)
        try:
            i = 0
            for u in urls:
                i += 1
                pth = self.build_img_path(product_code, "ìƒí’ˆëª©ë¡ì´ë¯¸ì§€", u, "ëª©ë¡", i)
                names.append(pth.name)
                futures.append(ex.submit(self.download_one, u, pth))

            done = 0
            total = len(futures)
            for f in as_completed(futures):
                _ = f.result()
                done += 1
                if done == total or done % 10 == 0:
                    self.log_signal_func("[ìƒí’ˆëª©ë¡ì´ë¯¸ì§€] ì§„í–‰ ìƒí’ˆì½”ë“œ=%s %d/%d" % (product_code, done, total))
        finally:
            ex.shutdown(wait=True)

        row["ìƒí’ˆ ëª©ë¡ ì´ë¯¸ì§€ëª…"] = json.dumps(names, ensure_ascii=False)
        self.log_signal_func("[ìƒí’ˆëª©ë¡ì´ë¯¸ì§€] ì™„ë£Œ ìƒí’ˆì½”ë“œ=%s" % product_code)

    # =========================================================
    # 3) AJAX ìƒì„¸ (ì¿ í‚¤ ì—†ì´)
    # =========================================================
    def build_ajax_headers_no_cookie(self, product_code):
        product_code = str(product_code or "").strip()
        return {
            "Accept": "*/*",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": self.base_url + "/26/?idx=" + product_code,
            "User-Agent": self.headers_html.get("User-Agent", ""),
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-origin",
        }

    def http_get_json_no_cookie(self, product_code):
        url = urljoin(self.base_url, self.ajax_path)
        params = {"prod_idx": str(product_code).strip()}

        last_err = None
        for i in range(self.retries):
            try:
                r = requests.get(
                    url,
                    params=params,
                    headers=self.build_ajax_headers_no_cookie(product_code),
                    cookies={},  # ì¿ í‚¤ ì°¨ë‹¨
                    timeout=self.timeout,
                    allow_redirects=True
                )
                r.raise_for_status()

                raw = (r.content or b"").lstrip()
                if not raw.startswith(b"{"):
                    head = (r.text or "")[:200]
                    ct = (r.headers.get("content-type") or "")
                    raise RuntimeError("JSON ì•„ë‹˜: status=%s ct=%s body_head=%s" % (r.status_code, ct, head))

                return r.json()

            except Exception as e:
                last_err = e
                time.sleep(0.6 * (i + 1))

        raise RuntimeError("AJAX GET(JSON) ì‹¤íŒ¨: prod_idx=%s err=%s" % (product_code, str(last_err)))

    def parse_ajax_content(self, content_html):
        youtube_url = ""
        detail_urls = []

        soup = BeautifulSoup(content_html or "", "html.parser")

        # youtube iframe
        for iframe in soup.select("iframe[src]"):
            src = (iframe.get("src") or "").strip()
            if "youtube.com" not in src:
                continue
            m = re.search(r"youtube\.com/embed/([^?\"'&/]+)", src)
            if m:
                vid = (m.group(1) or "").strip()
                if vid:
                    youtube_url = "https://www.youtube.com/watch?v=" + vid
                break

        # ìƒì„¸ ì´ë¯¸ì§€
        for img in soup.select("img.fr-dib"):
            src = (img.get("src") or img.get("data-src") or img.get("data-original") or "").strip()
            if src:
                detail_urls.append(urljoin(self.base_url, html.unescape(src).strip()))

        detail_urls = self.uniq_keep_order(detail_urls)
        return youtube_url, detail_urls

    def process_ajax_detail(self, row):
        product_code = (row.get("ìƒí’ˆì½”ë“œ") or "").strip()
        if not product_code:
            row["YOUTUBE URL"] = ""
            row["ìƒí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€ URL"] = "[]"
            row["ìƒí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€ëª…"] = "[]"
            return

        self.log_signal_func("[AJAX] ì‹œì‘ ìƒí’ˆì½”ë“œ=%s" % product_code)

        data = self.http_get_json_no_cookie(product_code)
        content_html = ""
        if isinstance(data, dict):
            content_html = (data.get("data") or {}).get("content") or ""

        youtube_url, detail_urls = self.parse_ajax_content(content_html)

        row["YOUTUBE URL"] = youtube_url
        row["ìƒí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€ URL"] = json.dumps(detail_urls, ensure_ascii=False)

        if not detail_urls:
            row["ìƒí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€ëª…"] = "[]"
            self.log_signal_func("[AJAX] ì™„ë£Œ ìƒí’ˆì½”ë“œ=%s (ìƒì„¸ì´ë¯¸ì§€=0)" % product_code)
            return

        names = []
        futures = []

        self.log_signal_func("[ìƒí’ˆìƒì„¸ì´ë¯¸ì§€] ì‹œì‘ ìƒí’ˆì½”ë“œ=%s cnt=%d" % (product_code, len(detail_urls)))

        ex = ThreadPoolExecutor(max_workers=self.max_workers)
        try:
            i = 0
            for u in detail_urls:
                i += 1
                pth = self.build_img_path(product_code, "ìƒí’ˆìƒì„¸ì´ë¯¸ì§€", u, "ìƒì„¸", i)
                names.append(pth.name)
                futures.append(ex.submit(self.download_one, u, pth))

            done = 0
            total = len(futures)
            for f in as_completed(futures):
                _ = f.result()
                done += 1
                if done == total or done % 10 == 0:
                    self.log_signal_func("[ìƒí’ˆìƒì„¸ì´ë¯¸ì§€] ì§„í–‰ ìƒí’ˆì½”ë“œ=%s %d/%d" % (product_code, done, total))
        finally:
            ex.shutdown(wait=True)

        row["ìƒí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€ëª…"] = json.dumps(names, ensure_ascii=False)
        self.log_signal_func("[AJAX] ì™„ë£Œ ìƒí’ˆì½”ë“œ=%s (ìƒì„¸ì´ë¯¸ì§€=%d)" % (product_code, len(detail_urls)))

    # =========================================================
    # 4) ìƒì„¸ HTML ì¸ë„¤ì¼(.shop_goods_img)
    # =========================================================
    def process_thumbnails(self, row):
        product_code = (row.get("ìƒí’ˆì½”ë“œ") or "").strip()
        detail_url = (row.get("ìƒí’ˆ ìƒì„¸ ì •ë³´ URL") or "").strip()

        if not product_code or not detail_url:
            row["ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL"] = "[]"
            row["ì¸ë„¤ì¼ ì´ë¯¸ì§€ëª…"] = "[]"
            return

        self.log_signal_func("[ì¸ë„¤ì¼] ì‹œì‘ ìƒí’ˆì½”ë“œ=%s" % product_code)

        html_text = self.http_get_text(detail_url)
        soup = BeautifulSoup(html_text, "html.parser")

        urls = []
        for img in soup.select(".shop_goods_img img"):
            src = (img.get("src") or "").strip()
            if src:
                urls.append(urljoin(self.base_url, src))

        urls = self.uniq_keep_order(urls)
        row["ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL"] = json.dumps(urls, ensure_ascii=False)

        if not urls:
            row["ì¸ë„¤ì¼ ì´ë¯¸ì§€ëª…"] = "[]"
            self.log_signal_func("[ì¸ë„¤ì¼] ì™„ë£Œ ìƒí’ˆì½”ë“œ=%s (0)" % product_code)
            return

        names = []
        futures = []

        self.log_signal_func("[ì¸ë„¤ì¼ì´ë¯¸ì§€] ì‹œì‘ ìƒí’ˆì½”ë“œ=%s cnt=%d" % (product_code, len(urls)))

        ex = ThreadPoolExecutor(max_workers=self.max_workers)
        try:
            i = 0
            for u in urls:
                i += 1
                pth = self.build_img_path(product_code, "ì¸ë„¤ì¼ì´ë¯¸ì§€", u, "ì¸ë„¤ì¼", i)
                names.append(pth.name)
                futures.append(ex.submit(self.download_one, u, pth))

            done = 0
            total = len(futures)
            for f in as_completed(futures):
                _ = f.result()
                done += 1
                if done == total or done % 10 == 0:
                    self.log_signal_func("[ì¸ë„¤ì¼ì´ë¯¸ì§€] ì§„í–‰ ìƒí’ˆì½”ë“œ=%s %d/%d" % (product_code, done, total))
        finally:
            ex.shutdown(wait=True)

        row["ì¸ë„¤ì¼ ì´ë¯¸ì§€ëª…"] = json.dumps(names, ensure_ascii=False)
        self.log_signal_func("[ì¸ë„¤ì¼] ì™„ë£Œ ìƒí’ˆì½”ë“œ=%s (cnt=%d)" % (product_code, len(urls)))
