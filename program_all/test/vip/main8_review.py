import pandas as pd
import requests
from bs4 import BeautifulSoup
import html
import time

# 1. ì—‘ì…€ì—ì„œ SHOP_ID ì½ê¸° ë° ì¤‘ë³µ ì œê±°
df = pd.read_excel("vipinfo_all_user_grp.xlsx")
shop_ids = df["SHOP_ID"].dropna().unique()

# 2. ìš”ì²­ í—¤ë” (ì¿ í‚¤ ì œì™¸)
headers = {
    "accept": "application/json, text/javascript, */*; q=0.01",
    "accept-encoding": "gzip, deflate, br, zstd",
    "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "content-type": "application/x-www-form-urlencoded; charset=UTF-8",
    "origin": "https://vipgunma.com",
    "priority": "u=0, i",
    "referer": "https://vipgunma.com/bbs/board.php",
    "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "same-origin",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
    "x-requested-with": "XMLHttpRequest"
}

# 3. ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
review_list = []

# 4. ì „ì²´ SHOP_ID ë°˜ë³µ
for shop_index, sid in enumerate(shop_ids, start=1):
    print(f"\nğŸ“¦ {shop_index}/{len(shop_ids)} | SHOP_ID {sid} ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘...")

    page = 1
    prev_html = None

    while True:
        payload = {
            "request": "requestRev",
            "page": str(page),
            "wr_id": str(sid)
        }

        response = requests.post(
            "https://vipgunma.com/bbs/rev.php",
            headers=headers,
            data=payload
        )

        if response.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: SHOP_ID={sid}, page={page}")
            break

        try:
            json_data = response.json()
        except Exception as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            break

        raw_html = json_data.get("result_data", {}).get("list", "")
        if not raw_html.strip() or raw_html == prev_html:
            break
        prev_html = raw_html

        # HTML ë¬¸ìì—´ íŒŒì‹±
        cleaned_html = html.unescape(raw_html)
        soup = BeautifulSoup(cleaned_html, "html.parser")
        cards = soup.select("div.reviewCard")
        if not cards:
            break

        for review_index, card in enumerate(cards, start=1):
            try:
                user_id = card.get("id", "").replace("c_", "")
                review_nick = card.select_one(".reviewNick").get_text(strip=True)
                review_date = card.select_one(".reviewDate").get_text(strip=True)
                review_content = card.select_one(".reviewCardBody").get_text(separator="\n", strip=True)

                # ë‚ ì§œ ë³´ì •
                if len(review_date) == 5 and "-" in review_date:
                    review_date = f"2025-{review_date}"


                obj = {
                    "shop_id": sid,
                    "user_id": user_id,
                    "review_nick": review_nick,
                    "review_date": review_date,
                    "review_content": review_content
                }

                review_list.append(obj)

                # âœ… ì§„í–‰ ìƒíƒœ ì¶œë ¥
                print(f"ğŸ“ {shop_index}/{len(shop_ids)} | page : {page} | {review_index}/{len(cards)} | SHOP_ID={sid} | ë°ì´í„° : {obj}")


            except Exception as e:
                print(f"âš ï¸ íŒŒì‹± ì˜¤ë¥˜: SHOP_ID={sid}, page={page}, index={review_index} | {e}")
                continue

        page += 1
        time.sleep(0.3)

# 5. ê²°ê³¼ ì—‘ì…€ ì €ì¥
result_df = pd.DataFrame(review_list)
result_df.to_excel("vip_review_result.xlsx", index=False)
print("\nâœ… ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ â†’ vip_review_result.xlsx ì €ì¥ë¨")
