import requests
from bs4 import BeautifulSoup
import re
import time
import pandas as pd
import json
from urllib.parse import urlparse
import shutil
import os

# ê³µí†µ í—¤ë”
common_headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
}

category_urls = {
    "í™ˆì¼€ì–´/ë°©ë¬¸": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=24&ctg=1",
    "ì™ì‹±": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=3",
    "1ì¸ìƒµ": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=2",
    "24ì‹œê°„": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=11",
    "ì‚¬ìš°ë‚˜/ìŠ¤íŒŒ": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=8",
    "ìˆ˜ë©´ê°€ëŠ¥": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=5",
    "ì—¬ì„±í™˜ì˜": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=12",
    "íƒ€ì´ë§ˆì‚¬ì§€": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=14",
    "ê°ì„±ë§ˆì‚¬ì§€": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=19",
    "ìŠˆì–¼ë§ˆì‚¬ì§€": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=18",
    "ë¡œë¯¸ë¡œë¯¸": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=17",
    "ìŠ¤ì›¨ë””ì‹œ": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=16",
    "ë”¥í‹°ìŠˆ": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=26",
    "ìŠ¤í¬ëŸ½": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=27",
    "ë‘ë¦¬ì½”ìŠ¤": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=28",
    "í˜¸í…”ì‹ë§ˆì‚¬ì§€": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=31",
    "ì•„ë¡œë§ˆë§ˆì‚¬ì§€": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=32",
    "ë¦¼í”„ê´€ë¦¬": "https://vipgunma.com/bbs/board.php?bo_table=gm_1&idx=&ctg=3&sfl=wr_7&sfl2=&sca3=&sca2=33"
}

def fetch_wr_id_list_from_html(html, category_name):
    soup = BeautifulSoup(html, "html.parser")
    rows = soup.select(".listRow")
    tab_items = []

    for row in rows:
        a_tag = row.select_one(".subject a[href*='wr_id']")
        span = row.select_one(".subject span")
        if a_tag and span:
            match = re.search(r"wr_id=(\d+)", a_tag['href'])
            if match:
                wr_id = match.group(1)
                title = span.get_text(strip=True)
                tab_items.append({
                    "SHOP_ID": wr_id,
                    "TITLE": title,
                    "MAIN_CATEGORY": category_name
                })

    return tab_items


def fetch_all_wr_id_items():
    print("ğŸ” ì¹´í…Œê³ ë¦¬ë³„ wr_id ìˆ˜ì§‘ ì‹œì‘...")
    result_list = []

    for category, base_url in category_urls.items():
        tab = 1
        prev_count = -1
        final_items = []

        while True:
            if "tab=" in base_url:
                url = re.sub(r'tab=\d+', f'tab={tab}', base_url)
            else:
                url = base_url + f"&tab={tab}"

            print(f'url : {url}')

            try:
                headers = {
                    **common_headers,
                    "Referer": url
                }
                response = requests.get(url, headers=headers)
                if response.status_code != 200:
                    print(f"[{category}][tab={tab}] ìƒíƒœì½”ë“œ ì˜¤ë¥˜: {response.status_code}")
                    break

                html = response.text
                items = fetch_wr_id_list_from_html(html, category)
                current_count = len(items)

                if current_count == 0:
                    print(f"[{category}][tab={tab}] ë°ì´í„° ì—†ìŒ, ì¢…ë£Œ")
                    break

                print(f"[{category}][tab={tab}] ìˆ˜ì§‘ëœ í•­ëª© ìˆ˜: {current_count}")

                # ì´ì „ê³¼ ê°™ìœ¼ë©´ ìµœì¢… ë°ì´í„°ë¡œ íŒë‹¨í•˜ê³  ì¢…ë£Œ
                if current_count == prev_count:
                    final_items = items
                    print(f"[{category}][tab={tab}] ì´ì „ê³¼ í•­ëª© ìˆ˜ ê°™ìŒ â†’ ì¢…ë£Œ")
                    break

                prev_count = current_count
                tab += 1
                time.sleep(0.5)

            except Exception as e:
                print(f"[{category}][tab={tab}] ì—ëŸ¬ ë°œìƒ: {e}")
                break

        result_list.extend(final_items)
        print(f"[{category}] âœ… ìµœì¢… ì €ì¥ í•­ëª© ìˆ˜: {len(final_items)}, ì „ì²´ í•­ëª©ìˆ˜ : {len(result_list)}")

    print(f"\nâœ… ì „ì²´ ìµœì¢… ìˆ˜ì§‘ ê°œìˆ˜: {len(result_list)}\n")
    return result_list

def fetch_review_count(wr_id: str) -> int:
    url = "https://vipgunma.com/bbs/rev.php"
    headers = {
        **common_headers,
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Accept": "application/json, text/javascript, */*; q=0.01",
        "X-Requested-With": "XMLHttpRequest",
        "Origin": "https://vipgunma.com",
        "Referer": f"https://vipgunma.com/bbs/board.php?bo_table=gm_1&wr_id={wr_id}"
    }
    data = {
        "request": "requestRev",
        "page": 1,
        "wr_id": wr_id
    }

    try:
        res = requests.post(url, headers=headers, data=data)
        if res.status_code == 200:
            json_data = res.json()
            if json_data.get("result_code") == "success":
                return json_data.get("result_data", {}).get("total", 0)
    except Exception as e:
        print(f"[{wr_id}] ë¦¬ë·° ìˆ˜ ìš”ì²­ ì‹¤íŒ¨: {e}")

    return 0

def fetch_detail(shop_id: str, program_title: str, category: str) -> dict:
    url = f"https://vipgunma.com/bbs/board.php?bo_table=gm_1&wr_id={shop_id}"
    res = requests.get(url, headers=common_headers)

    if res.status_code != 200:
        print(f"[{shop_id}] ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
        return {}

    item = {
        "SHOP_ID": shop_id,
        "TITLE": program_title,
        "MAIN_CATEGORY": category,
    }

    soup = BeautifulSoup(res.text, "html.parser")

    # ìƒí˜¸ëª…
    store = soup.select_one(".storeName")
    item["SHOP_NAME"] = store.text.strip() if store else ""

    # ì¡°íšŒìˆ˜
    views = soup.select_one(".extra > div > span")
    item["VIEW_COUNT"] = views.text.strip().replace(",", "") if views else ""

    # ë¦¬ë·°ìˆ˜
    item["REVIEW_COUNT"] = fetch_review_count(shop_id)

    # ìƒµíŠ¹ì§•
    shop_type = soup.select_one(".shopType")
    item["FEATURES"] = shop_type.text.strip() if shop_type else ""

    # ìƒì„¸ì •ë³´
    detail_rows = soup.select(".content .detailRow")
    for row in detail_rows:
        head = row.select_one(".detailHead")
        con = row.select_one(".detailCon")
        if not head or not con:
            continue
        key = head.text.strip()
        if key == "ì˜¤ì‹œëŠ”ê¸¸":
            address_divs = con.select("div")
            item["ì§€ë²ˆì£¼ì†Œ"] = address_divs[0].text.strip() if len(address_divs) > 0 else ""
            item["ë„ë¡œëª…ì£¼ì†Œ"] = address_divs[1].text.strip() if len(address_divs) > 1 else ""
        else:
            item[key] = con.text.strip()

        # í”„ë¡œê·¸ë¨ ì •ë³´

    course_info_tag = soup.select_one(".detailCon.programCon > div")
    course_info = course_info_tag.get_text(strip=True).replace('\xa0', ' ') if course_info_tag else ""
    item["COURSE_INFO"] = course_info


    programs = []

    heads = soup.select(".programWrap .programHead")
    for head in heads:
        idx = head.get("idx")
        title = re.sub(r'\ufeff', '', head.get_text(strip=True))

        content = soup.select_one(f".programContents[idx='{idx}']")
        if not content:
            continue

        rows = content.select(".itemRow")
        for row in rows:
            duration_tag = row.select_one(".line1 .infoTxt")
            org_price_tag = row.select(".line1 .diP")[-1] if row.select(".line1 .diP") else None
            dis_price_tag = row.select(".line2 .prc")[-1] if row.select(".line2 .prc") else None
            category_tag = row.select_one(".line2 .subTxt")

            # ê°’ ì¶”ì¶œ
            duration = duration_tag.get_text(strip=True) if duration_tag else ""
            category_text = category_tag.get_text(strip=True) if category_tag else ""

            try:
                original_price = int(org_price_tag.get_text(strip=True).replace(",", "")) if org_price_tag else None
            except ValueError:
                original_price = ''

            try:
                discount_price = int(dis_price_tag.get_text(strip=True).replace(",", "")) if dis_price_tag else None
            except ValueError:
                discount_price = ''

            # JSON ê°ì²´ êµ¬ì„±
            program = {
                "title": title,
                "duration": duration,
                "categories": category_text,
                "original_price": original_price,
                "discount_price": discount_price
            }

            programs.append(program)

    # ë¬¸ìì—´ë¡œ ì €ì¥ (ì—‘ì…€ì—ë„ ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ë¬¸ìì—´ ì²˜ë¦¬)
    item["PROGRAMS"] = json.dumps(programs, ensure_ascii=False)

    hash_tag_spans = soup.select(".hashTag span")
    hash_tags = ", ".join([span.get_text(strip=True) for span in hash_tag_spans]) if hash_tag_spans else ""
    item["HASHTAG"] = hash_tags

    event_tag = soup.select_one(".detailRow.eventRow .detailCon")
    event_text = event_tag.get_text("\n", strip=True).replace('\xa0', ' ') if event_tag else ""
    item["event"] = event_text

    image_data = []

    # sliderGal.slider-for ì•ˆì— .si > img ì¶”ì¶œ
    slider_imgs = soup.select(".sliderGal.slider-for .si img")

    for idx, img in enumerate(slider_imgs, 1):
        src = img.get("src")
        alt = img.get("alt", "")
        if not src:
            continue

        try:
            parsed_url = urlparse(src)
            ext = os.path.splitext(parsed_url.path)[-1] or ".jpg"
        except Exception as e:
            print(f"[{shop_id}] ì´ë¯¸ì§€ URL íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

        save_dir = os.path.join("images", "vip", shop_id)
        os.makedirs(save_dir, exist_ok=True)

        filename = f"{shop_id}_{idx}{ext}"
        filepath = os.path.join(save_dir, filename)

        try:
            response = requests.get(src, timeout=10)
            if response.status_code == 200:
                with open(filepath, "wb") as f:
                    f.write(response.content)
            else:
                print(f"[{shop_id}] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({src})")
        except Exception as e:
            print(f"[{shop_id}] ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            continue

        image_data.append({
            "src": src,
            "alt": alt,
            "filename": filename
        })

    item["images"] = image_data

    return item

def main():
    wr_items = fetch_all_wr_id_items()
    results = []

    for i, shop in enumerate(wr_items, 1):
        shop_id = shop["SHOP_ID"]
        title = shop["TITLE"]
        category = shop["MAIN_CATEGORY"]
        item = fetch_detail(shop_id, title, category)
        if item:
            results.append(item)
            print(f"[{i}/{len(wr_items)}] {shop_id} ìˆ˜ì§‘ ì™„ë£Œ: {item.get('SHOP_NAME', '')}")
        else:
            print(f"[{i}/{len(wr_items)}] {shop_id} ìˆ˜ì§‘ ì‹¤íŒ¨")
        time.sleep(0.5)

    df = pd.DataFrame(results)
    df.to_excel("vipgunma_detail_2025-07-17.xlsx", index=False)
    print("\nâœ… ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ: vipgunma_detail_2025-07-17.xlsx")

if __name__ == "__main__":
    main()
