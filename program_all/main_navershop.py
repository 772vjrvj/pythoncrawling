# -*- coding: utf-8 -*-
import json
import random
import time
from pathlib import Path
from urllib.parse import quote

from playwright.sync_api import sync_playwright, TimeoutError as PwTimeoutError

PROFILE_DIR = Path("./chrome_profile_naver")
PROFILE_DIR.mkdir(parents=True, exist_ok=True)

BASE_URL = "https://msearch.shopping.naver.com/search/all?vertical=search&query="


def build_search_url(keyword: str) -> str:
    return BASE_URL + quote(keyword)


def is_captcha_like(html: str) -> bool:
    h = (html or "").lower()
    return ("wtmcaptcha" in h) or ("captcha" in h) or ("ë³´ì•ˆ í™•ì¸" in html)


def get_next_data(page) -> dict:
    # scriptëŠ” visibleì´ ì•„ë‹ˆë¼ attachedë¡œ ê¸°ë‹¤ë ¤ì•¼ ì•ˆì •ì 
    page.wait_for_selector("#__NEXT_DATA__", state="attached", timeout=60000)
    raw = page.locator("#__NEXT_DATA__").text_content()
    raw = (raw or "").strip()
    if not raw:
        raise RuntimeError("__NEXT_DATA__ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
    return json.loads(raw)


def print_category_names(keyword: str, data: dict) -> None:
    try:
        category = data["props"]["pageProps"]["categoryNames"]
        print(f"\nâœ… [{keyword}] categoryNames")
        for k, v in category.items():
            print(f"{k}: {v}")
    except Exception:
        print(f"\nâŒ [{keyword}] categoryNames ì—†ìŒ")


def run(keywords):
    with sync_playwright() as p:
        ctx = p.chromium.launch_persistent_context(
            user_data_dir=str(PROFILE_DIR),
            channel="chrome",
            headless=False,
            viewport=None,
            args=["--start-maximized"],
            locale="ko-KR",
        )
        page = ctx.new_page()

        # ==========================
        # 1) ì²« ê²€ìƒ‰(ì²« í‚¤ì›Œë“œ)ì—ì„œë§Œ ìº¡ì±  ì²˜ë¦¬
        # ==========================
        first_kw = keywords[0]
        first_url = build_search_url(first_kw)

        print(f"\nğŸš€ (ì²« ê²€ìƒ‰) {first_kw} ì ‘ì†")
        page.goto(first_url, wait_until="domcontentloaded")
        page.wait_for_timeout(1500)

        if is_captcha_like(page.content()):
            print("\nğŸ›¡ï¸ ìº¡ì± /ë³´ì•ˆí™•ì¸ ë°œìƒ(ì²« ê²€ìƒ‰ì—ì„œë§Œ).")
            print("ğŸ‘‰ ë¸Œë¼ìš°ì €ì—ì„œ ìº¡ì±  í•´ê²° + 'í™•ì¸' ë²„íŠ¼ í´ë¦­ê¹Œì§€ ì™„ë£Œí•œ ë’¤ Enter ì¹˜ì„¸ìš”.\n")
            input()

            # ìº¡ì±  í†µê³¼ í›„ ê°™ì€ URLë¡œ ë‹¤ì‹œ ì§„ì… (í™•ì‹¤íˆ)
            page.goto(first_url, wait_until="domcontentloaded")
            page.wait_for_timeout(1500)

        data = get_next_data(page)
        print_category_names(first_kw, data)

        # ë”œë ˆì´
        delay = random.uniform(2.0, 4.0)
        print(f"\nâ³ {delay:.2f}ì´ˆ ëŒ€ê¸°...\n")
        time.sleep(delay)

        # ==========================
        # 2) ì´í›„ í‚¤ì›Œë“œëŠ” ìº¡ì±  ì²´í¬ ì—†ì´ ì­‰ ì§„í–‰
        # ==========================
        for kw in keywords[1:]:
            url = build_search_url(kw)
            print(f"\nğŸš€ {kw} ì ‘ì†")

            # networkidleë¡œ ë°”ê¿”ë„ ë¨. ì—¬ê¸°ì„  domcontentloaded + NEXT_DATA ëŒ€ê¸°ë¡œ ì¶©ë¶„
            page.goto(url, wait_until="domcontentloaded")
            page.wait_for_timeout(1200)

            data = get_next_data(page)
            print_category_names(kw, data)

            delay = random.uniform(2.0, 4.0)
            print(f"\nâ³ {delay:.2f}ì´ˆ ëŒ€ê¸°...\n")
            time.sleep(delay)

        ctx.close()


if __name__ == "__main__":
    keywords = ["ì½”ì¹´ì½œë¼", "í©ì‹œ", "í™˜íƒ€", "ì‚¬ì´ë‹¤", "ì›°ì¹˜ìŠ¤"]
    run(keywords)
